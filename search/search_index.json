{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"<code>torch-biopl</code>: Biologically-plausible networks made easy","text":""},{"location":"#overview","title":"Overview","text":"<ul> <li> <p>We provide our general philosophy and an overview of the functionalities supported in <code>torch-biopl</code> below. This is a growing list, so if you have any suggestions please don't hesitate to get in contact.</p> </li> <li> <p>There are several applications for which you, as a neuroscientist, would want to build a model for. We have done some preliminary work in providing a few example use cases as utilities. These come with instructions on how to create/access relevant datasets (+ accompanying dataloaders) and models. This list is by no means exhaustive but merely a starting point to fuel your imagination.</p> </li> <li> <p>To get started, jump to installation and then take a look at the examples provided.</p> </li> </ul>"},{"location":"#motivation","title":"Motivation","text":"<p>The neuroscience community has varying modeling needs. On one hand, the more functional deep-learning based models are less biologically aligned and do not support testing mechanistic hypotheses. On the other hand, instantiating neural architectures that work at scale (on realistic sensory inputs) requires the practitioner to be overly techinically adept. Our motivation was to bridge these worlds.</p> <p>In this package, we provide a basic set of features that will enable to user to construct biologically-realistic neural networks with minimal effort. At a glance, here are some of the main capabilities supported:</p> <ul> <li> <p>Cell-type specification</p> <p> Synapses can adhere to Dale's law. Specify synapse- vs neuron-specific nonlinearities.  Example</p> </li> <li> <p>Spatially embedded populations</p> <p> Construct hierarchical populations of spatially embedded neurons.  Example</p> </li> <li> <p>Microcircuit archetypes</p> <p> Learnable scaffolds for local \"microcircuit\" patterns of connectivity.  Example</p> </li> <li> <p>Diverse feedback wiring</p> <p> Various specifications for inter-areal feedback projections.  Example</p> </li> <li> <p>Connectome constrained modelling</p> <p> Efficient implementation of sparse-matrix arithmetic for forward and backpropagation.  Example</p> </li> </ul>"},{"location":"install/","title":"Quick start guide","text":""},{"location":"install/#installing-python","title":"Installing Python","text":"<p>This project requires Python 3.9 or higher. Most modern Linux distributions and macOS come with Python 3.9 or higher pre-installed. If you are on a system that does not have Python 3.9 or higher installed, there are a couple of options to install it.</p> <p>We recommend using miniconda, an environment and package manager that allows you to create and manage multiple isolated environments, each with their own Python version and set of installed packages. You can use miniconda even if you already have a Python installation.</p> <p>To install miniconda, follow the instructions here.</p> <p>Alternatively, you can install Python 3.9 or higher directly, following the instructions here.</p>"},{"location":"install/#setting-up-your-environment","title":"Setting up your environment","text":"<p>Once you have Python 3.9 or higher installed, you can create a new environment in which to install the required packages. If you have miniconda installed, follow Using conda below. Otherwise, follow Using venv below (or use any other environment manager you prefer).</p>"},{"location":"install/#using-conda","title":"Using conda","text":"<p>If you have not already done so, initialize conda in your terminal: <pre><code>conda init\n</code></pre></p> <p>Then, create a new environment and activate it: <pre><code>conda create -n bioplnn python=3.12\nconda activate bioplnn\n</code></pre></p> <p>To verify that you are in the <code>bioplnn</code> environment, run: <pre><code>conda env list\n</code></pre></p> <p>To deactivate the environment, run: <pre><code>conda deactivate\n</code></pre></p> <p>For more information on conda, see the conda documentation.</p>"},{"location":"install/#using-venv","title":"Using venv","text":"<pre><code>python -m venv venv\nsource venv/bin/activate\n</code></pre>"},{"location":"install/#requirements","title":"Requirements","text":"<p>This project depends on certain packages that are not available on PyPI. You must install these manually before installing BioPlNN.</p>"},{"location":"install/#pytorch-and-torchvision","title":"PyTorch and Torchvision","text":"<p>Currently, the latest supported version of PyTorch is 2.5.1. To install a specific version of PyTorch (and its corresponding Torchvision version), follow the instructions for your system here.</p> <p>For example, below are the installation commands for PyTorch 2.5.1 and Torchvision 0.20.1 on the following systems:</p> <ul> <li>System with a CUDA 12.4-compatible GPU <pre><code>pip install torch==2.5.1 torchvision==0.20.1 --index-url https://download.pytorch.org/whl/cu124\n</code></pre></li> <li>CPU-only system (not macOS) <pre><code>pip install torch==2.5.1 torchvision==0.20.1 --index-url https://download.pytorch.org/whl/cpu\n</code></pre></li> <li>CPU-only system (macOS) <pre><code>pip install torch==2.5.1 torchvision==0.20.1\n</code></pre></li> </ul>"},{"location":"install/#pytorch-sparse","title":"PyTorch Sparse","text":"<p>You must install PyTorch (see above) before installing PyTorch Sparse.</p> <p>To install PyTorch Sparse for your specific system and PyTorch version, follow the instructions here.</p> <p>For example, to install PyTorch Sparse for PyTorch 2.5.1 and a system with a CUDA 12.4-compatible GPU, you would run:</p> <pre><code>pip install torch-sparse torch-scatter -f https://data.pyg.org/whl/torch-2.5.1+cu124.html\n</code></pre> <p>And for a CPU-only system (any OS), you would run:</p> <pre><code>pip install torch-sparse torch-scatter -f https://data.pyg.org/whl/torch-2.5.1+cpu.html\n</code></pre>"},{"location":"install/#installation","title":"Installation","text":"<p>Make sure you have installed the requirements as described above. Then, you can install the package using one of the following methods:</p>"},{"location":"install/#from-pypi-recommended-not-yet-available","title":"From PyPI (recommended) (not yet available)","text":"<pre><code>pip install bioplnn\n</code></pre>"},{"location":"install/#from-source","title":"From source","text":"<ol> <li>Clone the BioPlNN repository:</li> </ol> <pre><code>git clone https://github.com/FieteLab/torch-bioplnn-dev.git\n</code></pre> <ol> <li>Navigate to the cloned directory:</li> </ol> <pre><code>cd bioplnn\n</code></pre> <ol> <li>Install the package:</li> </ol> <p><pre><code>pip install -e .\n</code></pre> where <code>-e</code> installs the package in editable mode.</p>"},{"location":"install/#usage","title":"Usage","text":""},{"location":"install/#using-the-cli","title":"Using the CLI","text":"<p>Provided in the <code>examples</code> directory is <code>trainer.py</code>, a sample script for training the models on classification tasks.</p> <p>The model, data, and training parameters are configured using Hydra configs, which are stored in the <code>config</code> directory. See Hydra's docs for more information on the directory structure and syntax.</p> <p>Suppose we want to use the <code>e1l.yaml</code> model config in <code>config/model</code> and the <code>mnist.yaml</code> data config in <code>config/data</code>. To specify these from the command line, run <pre><code>python examples/trainer.py model=e1l data=mnist\n</code></pre> This relies on the <code>config/config.yaml</code> file, which contains the following: <pre><code>defaults:\n  - model: null\n  - data: null\n  ...\n</code></pre> This means that the <code>model</code> and <code>data</code> keys must be overridden in the command line, as shown above. If you want to set these to the default values, you can edit the <code>config/config.yaml</code> file as follows: <pre><code>defaults:\n  - model: e1l\n  - data: mnist\n  ...\n</code></pre></p>"},{"location":"install/#using-the-api","title":"Using the API","text":"<p>For details on using <code>torch-biopl</code> via the API please refer to the tutorials and API documentation.</p>"},{"location":"examples/advance_configs/","title":"Exploring advanced configurations","text":""},{"location":"examples/advance_configs/#overview","title":"Overview","text":"<p>In addition to the essential capabilities exposed in the first tutorial, we can exercise fine-grained control in how we wire up biologically-plausible networks. These are some of the functionality we will explore in this section:</p> <ul> <li>Synapse vs neuron nonlinearities</li> <li>Microcircuit archetypes</li> <li>Parameter sharing capabilities</li> <li>Hierarchically constructed neural areas</li> <li>Inter-areal feedback connectivity</li> </ul> <pre><code>import torch\nimport numpy as np\nfrom bioplnn.models import SpatiallyEmbeddedRNN, SpatiallyEmbeddedAreaConfig\n</code></pre>"},{"location":"examples/advance_configs/#synapse-vs-neuron-nonlinearities","title":"Synapse vs neuron nonlinearities","text":"<p>What do we mean by this? In an attempt to distinguish synaptic transfer functions from post-aggregation neuronal transfer functions, we give users the ability to specify pre- and post-integration nonlinearities.</p> <p>Let us consider the same example model from the previous tutorial: A simple one-area network with two neural classes with the following <code>inter_neuron_type_connectivity</code>: \\(\\begin{bmatrix}1&amp;1&amp;0\\cr1&amp;1&amp;1\\cr1&amp;1&amp;0\\end{bmatrix}\\). Following the same convention as the connectivity matrix, you can specify the transfer function for each of those synapse groups by setting the <code>inter_neuron_type_nonlinearity</code> parameter. Similarly, <code>neuron_type_nonlinearity</code> can be used to control the post-aggregation transfer function for each neuron type.</p> <p>If you were to construe a scenario where synapses have unbounded transfer functions while the neuron as whole is bounded from above (and for the sake of argument: bounded differently for the E and I subpopulations), then you'd do something like this:</p> <pre><code># writing a custom activation function that works similarly to a ReLU but is bounded from above!\nfrom torch.nn.modules.activation import Hardtanh\nclass ModRelu(Hardtanh):\n    def __init__(self, _ub: float, _lb: float = 0., inplace: bool = False):\n        super().__init__(_lb, _ub, inplace)\n\n    def extra_repr(self) -&gt; str:\n        inplace_str = 'inplace=True' if self.inplace else ''\n        return inplace_str\nupper_bounded_relu = ModRelu(_ub = 5.)\n</code></pre> <pre><code>area_configs = [\n    SpatiallyEmbeddedAreaConfig(\n                num_neuron_types = 2,\n                num_neuron_subtypes = np.array([16, 16]),\n                neuron_type_class = np.array(['excitatory', 'inhibitory']),\n                neuron_type_nonlinearity = ['sigmoid', upper_bounded_relu],\n                inter_neuron_type_connectivity = np.array([[1, 1, 0], [1, 1, 1], [1, 1, 0]]),\n                inter_neuron_type_nonlinearity = np.array([['relu', 'relu', ''], ['relu', 'relu', 'relu'], ['relu', 'relu', '']]),\n                in_size = [28, 28],\n                in_channels =  1,\n                out_channels = 32,\n    )\n]\nmodel = SpatiallyEmbeddedRNN(num_areas=1, area_configs=area_configs, batch_first=False)\n</code></pre>"},{"location":"examples/advance_configs/#microcircuit-archetypes","title":"Microcircuit archetypes","text":"<p>It must be evident that <code>inter_neuron_type_connectivity</code> is a powerful option that can be used to dictate a wide variety of microcircuit motifs. We provide some examples below for inspiration.</p>"},{"location":"examples/advance_configs/#feedback-inhibition","title":"Feedback inhibition","text":"<p>Parvalbumin-positive inhibitory cells in Layer \u2154 of the cortex are known to interact with Pyramidal cells through some form of divisive inhibition Jonke et al. (2017). To instantiate this microcircuit, you'd set <code>inter_neuron_type_connectivity</code> \\(= \\begin{bmatrix}1&amp;0&amp;0\\cr0&amp;1&amp;1\\cr1&amp;1&amp;0\\end{bmatrix}\\) (conventions same as in the original example).</p>"},{"location":"examples/advance_configs/#feedforward-inhibition","title":"Feedforward inhibition","text":"<p>Feedforward inhibition is another essential mechanism within the brain, to regulate neuronal firing and prevent runaway excitation (Panthi and Leitch (2019), Large et al. (2016)). To implement the microcircuit presented in these (and related) papers, you can set <code>inter_neuron_type_connectivity</code> \\(= \\begin{bmatrix}1&amp;1&amp;0\\cr1&amp;0&amp;1\\cr1&amp;1&amp;1\\end{bmatrix}\\)</p>"},{"location":"examples/advance_configs/#pyr-pv-sst-vip-motif","title":"Pyr-PV-SST-VIP motif","text":"<p>Interneuron subtypes play a critical role in several aspects of cortical function (Guo and Kumar (2023), Condylis et al. (2022), etc.). Of particular interest is a motif that involves one excitatory and three inhibitory interneuron populations (PV, SST, VIP). Please refer to these papers for pictorial depictions of the microcircuits. To realise this in <code>torch-biopl</code>, we would do the following:</p> <pre><code>area_configs = [\n    SpatiallyEmbeddedAreaConfig(\n                num_neuron_types = 4,\n                num_neuron_subtypes = np.array([16, 8, 8, 8]),\n                neuron_type_class = np.array(['excitatory', 'inhibitory', 'inhibitory', 'inhibitory']),\n                inter_neuron_type_connectivity = np.array([[1, 0, 0, 0, 0], [1, 1, 1, 1, 1], [1, 1, 0, 0, 0], [1, 1, 0, 1, 0], [0, 0, 1, 0, 0]]),\n                in_size = [28, 28],\n                in_channels =  1,\n                out_channels = 32,\n    )\n]\nmodel = SpatiallyEmbeddedRNN(num_areas=1, area_configs=area_configs, batch_first=False)\n</code></pre> <p>We remark that this is not an exhaustive list, but merely a window into endless possibilities :)</p>"},{"location":"examples/advance_configs/#parameter-sharing-capabilities-for-tau_mem","title":"Parameter sharing capabilities for $ \\tau_{mem} $","text":"<p>We provide the user the option to tie neural time constants: - Across space, but unique for each cell subtype (<code>tau_mode</code> = 'subtype') - Across cell subtype, but unique for each spatial location (<code>tau_mode</code> = 'spatial') - Each neuron learns its own time constant (<code>tau_mode</code> = 'subtype_spatial') - Across types (<code>tau_mode</code> = 'type')</p> <p>To go hand in hand with this, we also allow the user to provide an initialization for these time constants. This can be done via <code>tau_init_fn</code>. As with the nonlinearities, users can either provide torch initializers or custom functions to accomplish this.</p>"},{"location":"examples/advance_configs/#hierarchically-constructed-neural-areas","title":"Hierarchically constructed neural areas","text":"<p>Intuitive and expressive. For reasons more than one, you may want to wire up brain areas that are connected to each other via long-range synapses. This is quite easy to accomplish in <code>torch-biopl</code>. Note that each area can be configured independently!</p> <pre><code>area_configs = [\n    SpatiallyEmbeddedAreaConfig(\n                num_neuron_types = 2,\n                num_neuron_subtypes = np.array([16, 16]),\n                neuron_type_class = np.array(['excitatory', 'inhibitory']),\n                inter_neuron_type_connectivity = np.array([[1, 1, 0], [1, 1, 1], [1, 1, 0]]),\n                in_size = [28, 28],\n                in_channels =  1,\n                out_channels = 32,\n    ),\n    SpatiallyEmbeddedAreaConfig(\n                num_neuron_types = 2,\n                num_neuron_subtypes = np.array([32, 32]),\n                neuron_type_class = np.array(['excitatory', 'inhibitory']),\n                inter_neuron_type_connectivity = np.array([[1, 1, 0], [1, 1, 1], [1, 1, 0]]),\n                in_size = [14, 14],\n                in_channels =  32,\n                out_channels = 32,\n    )\n]\n\nmodel = SpatiallyEmbeddedRNN(num_areas=2, area_configs=area_configs, batch_first=False)\n</code></pre>"},{"location":"examples/advance_configs/#inter-areal-feedback-connectivity","title":"Inter-areal feedback connectivity","text":"<p>Finally, when you have multiple interacting areas, you'd want to ability to feedback information from downstream areas back up to early areas. <code>torch-biopl</code> provides an easy way to configure the flow of information. In simple terms, users can provide an adjacency matrix where rows are presynaptic areas and columns are postsynaptic areas.</p> <pre><code>conn = SpatiallyEmbeddedAreaConfig.inter_neuron_type_connectivity_template_df(use_feedback=True, num_neuron_types=2)\n# this prints out the format of the connectivity adjacency matrix that you can follow\nprint(conn)\n</code></pre> <pre><code>          neuron_0  neuron_1  output\ninput        False     False   False\nfeedback     False     False   False\nneuron_0     False     False   False\nneuron_1     False     False   False\n</code></pre> <pre><code>area_configs_feedback_model = [\n    SpatiallyEmbeddedAreaConfig(\n                num_neuron_types = 2,\n                num_neuron_subtypes = np.array([16, 16]),\n                neuron_type_class = np.array(['excitatory', 'inhibitory']),\n                inter_neuron_type_connectivity = np.array([[1, 1, 0], [1, 0, 0], [1, 1, 1], [1, 1, 0]]),\n                feedback_channels = 16,\n                in_size = [28, 28],\n                in_channels =  1,\n                out_channels = 32,\n    ),\n    SpatiallyEmbeddedAreaConfig(\n                num_neuron_types = 2,\n                num_neuron_subtypes = np.array([32, 32]),\n                neuron_type_class = np.array(['excitatory', 'inhibitory']),\n                inter_neuron_type_connectivity = np.array([[1, 1, 0], [1, 1, 1], [1, 1, 0]]),\n                in_size = [14, 14],\n                in_channels =  32,\n                out_channels = 32,\n    )\n]\n\nmodel_wFeedback = SpatiallyEmbeddedRNN(\n                            num_areas = 2,\n                            area_configs = area_configs_feedback_model,\n                            batch_first = False,\n                            inter_area_feedback_connectivity = np.array([[0, 0],[1, 0]])\n                    )\n</code></pre>"},{"location":"examples/basic_spatial_example/","title":"Basic API usage example","text":""},{"location":"examples/basic_spatial_example/#overview","title":"Overview","text":"<p>The goal of this tutorial is to wire up a simple model that is comprised of a spatially embedded brain area, specify a simple learning objective, and optimize model parameters.</p> <p>Before we begin, here are a few notes pertaining to the nomenclature we have adopted.</p> <ul> <li>A neuron class is defined by its synaptic affiliation. In <code>torch-biopl</code> you can configure types to be <code>Excitatory</code>/<code>Inhibitory</code> (where synapses have a postive/negative sign), or <code>Hybrid</code> which defaults to standard machine learning-style synapses that are unconstrained.</li> <li>Within each neuron class, you can instantiate neuron types. We employ the definition of neuron types with an eye to be able to specify inter-type local connectivity rules.</li> <li>Within each neuron type, are subtypes. Neurons within a subtype can (but don't have to) share properties like time constants, nonlinearities, and biases.</li> <li>Each neural area can be configured independently by specifying its classes, types, subtypes, and inter-type connectivity rules.</li> <li>Areas can be stitched together to form larger networks.</li> <li>Learnable parameters in <code>torch-biopl</code> are usually the synaptic weights, neural time constants, and biases.</li> </ul> <pre><code>import torch\nimport torchvision.transforms as T\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import MNIST\nfrom tqdm import tqdm\nimport numpy as np\n\nfrom bioplnn.models import SpatiallyEmbeddedClassifier\n</code></pre> <pre><code># Torch setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.set_float32_matmul_precision(\"high\")\n</code></pre> <p>Let us wire up a simple one-area network with two neural classes. Let one of them be an <code>Excitatory</code> cell class and one be <code>Inhibitory</code> each with <code>16</code> subtypes. Now, we have our neural populations. All that's left to do is to specify the inter-type connectivity.</p> <p>In <code>torch-biopl</code> we adopt the following convention:</p> <ul> <li>Inter-celltype connectivity (within a given area) is specified through an adjacency matrix.</li> <li>In addition to the neuron types within an area, we also have to account for projections into and out of the area. Keeping this in mind, we use a schema where rows in the adjacency matrix represent the pre-synaptic neuron type and columns represent the post-synaptic neuron type.</li> <li>The first row always denotes projections into the area, and the last column always denotes feedforward projections out of the area.</li> </ul> <p>For example, if our neuron_type_1 is E and neuron_type_2 is I, then <code>inter_neuron_type_connectivity</code>= \\(\\begin{bmatrix} 1 &amp; 1 &amp; 0 \\cr 1 &amp; 1 &amp; 1 \\cr 1 &amp; 1 &amp; 0 \\end{bmatrix}\\) represents a standard recurrent inhibitory circuit motif (ala Wong et al. (2006)), where both the E and I populations receive input, and only the E population projects downstream.</p> <p>Since we plan to train on grayscale images in this example, <code>in_channels</code> = 1</p> <pre><code># Model setup\nmodel = SpatiallyEmbeddedClassifier(\n    rnn_kwargs={\n        \"num_areas\": 1,\n        \"area_kwargs\": [\n            {\n                \"num_neuron_types\": 2,\n                \"num_neuron_subtypes\": np.array([16, 16]),\n                \"neuron_type_class\": np.array(['excitatory', 'inhibitory']),\n                \"inter_neuron_type_connectivity\": np.array([[1, 1, 0], [1, 1, 1], [1, 1, 0]]),\n                \"in_size\": [28, 28],\n                \"in_channels\": 1,\n                \"out_channels\": 32,\n            },\n        ],\n    },\n    num_classes = 10,\n    fc_dim = 256,\n    dropout = 0.5,\n).to(device)\n</code></pre> <pre><code># Define the optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# Define the loss function\ncriterion = nn.CrossEntropyLoss()\n</code></pre> <pre><code># Dataloader setup\ntransform = T.Compose([T.ToTensor(), T.Normalize((0.1307,), (0.3081,))])\ntrain_data = MNIST(root=\"data\", train=True, transform=transform)\ntrain_loader = DataLoader(\n    train_data, batch_size=256, num_workers=8, shuffle=True\n)\n</code></pre> <pre><code>/scratch2/weka/mcdermott/lakshmin/conda_envs/test_bioplnn_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n</code></pre> <pre><code># Define the training loop\nmodel.train()\nn_epochs = 10\nlog_frequency = 100\n\nrunning_loss, running_correct, running_total = 0, 0, 0\nfor epoch in range(n_epochs):\n    for i, (x, labels) in enumerate(tqdm(train_loader)):\n        x = x.to(device)\n        labels = labels.to(device)\n        torch._inductor.cudagraph_mark_step_begin()\n        logits = model(x, num_steps=5)\n        loss = criterion(logits, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Calculate running accuracy and loss\n        _, predicted = torch.max(logits, 1)\n        running_total += labels.size(0)\n        running_correct += (predicted == labels).sum().item()\n        running_loss += loss.item()\n\n        running_acc = running_correct / running_total\n        if (i + 1)%log_frequency == 0:\n            print(\n                f\"Training | Epoch: {epoch} | \" +\n                f\"Loss: {running_loss:.4f} | \" +\n                f\"Acc: {running_acc:.2%}\"\n            )\n            running_loss, running_correct, running_total = 0, 0, 0\n</code></pre> <pre><code> 43%|\u2588\u2588\u2588\u2588\u258e     | 101/235 [00:10&lt;00:06, 21.68it/s]\n\nTraining | Epoch: 0 | Loss: 230.5550 | Acc: 11.30%\n\n\n 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 202/235 [00:16&lt;00:01, 18.90it/s]\n\nTraining | Epoch: 0 | Loss: 220.1151 | Acc: 20.44%\n\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 235/235 [00:17&lt;00:00, 13.13it/s]\n 43%|\u2588\u2588\u2588\u2588\u258e     | 102/235 [00:10&lt;00:06, 21.96it/s]\n\nTraining | Epoch: 1 | Loss: 257.5806 | Acc: 26.85%\n\n\n 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 201/235 [00:15&lt;00:01, 20.88it/s]\n\nTraining | Epoch: 1 | Loss: 180.8878 | Acc: 28.72%\n\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 235/235 [00:16&lt;00:00, 13.97it/s]\n 43%|\u2588\u2588\u2588\u2588\u258e     | 102/235 [00:09&lt;00:06, 22.11it/s]\n\nTraining | Epoch: 2 | Loss: 237.9631 | Acc: 30.28%\n\n\n 87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 204/235 [00:14&lt;00:01, 20.53it/s]\n\nTraining | Epoch: 2 | Loss: 171.1650 | Acc: 32.17%\n\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 235/235 [00:15&lt;00:00, 14.71it/s]\n 44%|\u2588\u2588\u2588\u2588\u258d     | 104/235 [00:09&lt;00:06, 21.30it/s]\n\nTraining | Epoch: 3 | Loss: 222.0076 | Acc: 35.64%\n\n\n 87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 204/235 [00:14&lt;00:01, 21.45it/s]\n\nTraining | Epoch: 3 | Loss: 157.9773 | Acc: 38.38%\n\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 235/235 [00:16&lt;00:00, 14.59it/s]\n 44%|\u2588\u2588\u2588\u2588\u258d     | 104/235 [00:09&lt;00:06, 20.57it/s]\n\nTraining | Epoch: 4 | Loss: 205.7286 | Acc: 40.15%\n\n\n 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 202/235 [00:14&lt;00:01, 20.17it/s]\n\nTraining | Epoch: 4 | Loss: 148.0692 | Acc: 42.49%\n\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 235/235 [00:15&lt;00:00, 15.02it/s]\n 45%|\u2588\u2588\u2588\u2588\u258d     | 105/235 [00:10&lt;00:05, 22.12it/s]\n\nTraining | Epoch: 5 | Loss: 194.4819 | Acc: 43.67%\n\n\n 87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 204/235 [00:14&lt;00:01, 20.49it/s]\n\nTraining | Epoch: 5 | Loss: 139.6240 | Acc: 45.91%\n\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 235/235 [00:16&lt;00:00, 14.60it/s]\n 44%|\u2588\u2588\u2588\u2588\u258d     | 103/235 [00:10&lt;00:06, 20.21it/s]\n\nTraining | Epoch: 6 | Loss: 186.2790 | Acc: 46.58%\n\n\n 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 202/235 [00:14&lt;00:01, 19.63it/s]\n\nTraining | Epoch: 6 | Loss: 134.6118 | Acc: 48.28%\n\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 235/235 [00:16&lt;00:00, 14.54it/s]\n 44%|\u2588\u2588\u2588\u2588\u258d     | 103/235 [00:09&lt;00:06, 20.21it/s]\n\nTraining | Epoch: 7 | Loss: 180.0221 | Acc: 48.97%\n\n\n 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 201/235 [00:14&lt;00:01, 21.88it/s]\n\nTraining | Epoch: 7 | Loss: 131.4528 | Acc: 49.70%\n\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 235/235 [00:16&lt;00:00, 14.62it/s]\n 44%|\u2588\u2588\u2588\u2588\u258d     | 103/235 [00:09&lt;00:06, 21.15it/s]\n\nTraining | Epoch: 8 | Loss: 173.1868 | Acc: 51.24%\n\n\n 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 203/235 [00:14&lt;00:01, 21.70it/s]\n\nTraining | Epoch: 8 | Loss: 130.3644 | Acc: 49.56%\n\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 235/235 [00:16&lt;00:00, 14.67it/s]\n 45%|\u2588\u2588\u2588\u2588\u258d     | 105/235 [00:09&lt;00:05, 22.82it/s]\n\nTraining | Epoch: 9 | Loss: 171.3067 | Acc: 51.74%\n\n\n 87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 204/235 [00:14&lt;00:01, 21.01it/s]\n\nTraining | Epoch: 9 | Loss: 126.4864 | Acc: 51.59%\n\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 235/235 [00:15&lt;00:00, 15.27it/s]\n</code></pre>"},{"location":"examples/connectome_backward/","title":"Computing gradients and backpropagating through connectome-initialized models","text":"<p>While the previous tutorial showed you how to initialize a neural network with connectome-determined weights, it didn't provide you with information on how to \"tune\" model parameters in a data-driven manner. Turns out, computing (and backpropagating) gradients in networks with both sparse and dense tensors is non-trivial. In this tutorial, we spin up a small example on how you can accomplish this in <code>torch-biopl</code>.</p> <p>Goals:</p> <ul> <li>Continue from our previous example.</li> <li>Implement a wrapper on top of the <code>ConnectomeODERNN</code> to support a readout layer.</li> <li>Setup a simple training loop.</li> </ul> <p>Note: For demonstration purposes, we'll use flattened MNIST images as inputs into the connectome. This is however simplistic and we do allow for arbitrarily complex input mappings. To learn how to do that please refer to the API.</p> <pre><code>import os\nimport torch\nimport torchvision.transforms as T\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import MNIST\nfrom tqdm import tqdm\n\nfrom bioplnn.models import ConnectomeODEClassifier\n</code></pre> <pre><code>device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.set_float32_matmul_precision(\"high\")\nprint('Using device: {}'.format(device))\n</code></pre> <pre><code>Using device: cuda\n</code></pre> <pre><code># Download the connectome and read it in as a torch tensor. We have pre-processed this as a sparse tensor for the purposes of this example.\n!gdown \"https://drive.google.com/uc?id=18448HYpYrm60boziHG73bxN4CK5jG-1g\"\nconnectome = torch.load('turaga-dros-visual-connectome.pt', weights_only=True)\n\nfrom bioplnn.utils.torch import create_identity_ih_connectivity\n# since we are feeding in MNIST images\ninput_size = 28 * 28\nnum_neurons = connectome.shape[0]\n\ninput_projection_matrix = create_identity_ih_connectivity(\n                                input_size=input_size,\n                                num_neurons=num_neurons,\n                                input_indices=torch.randint(high=num_neurons, size=(input_size,))\n                            )\n\n# for now, lets just read outputs from all neurons\noutput_projection_matrix = None\n</code></pre> <pre><code>Downloading...\nFrom (original): https://drive.google.com/uc?id=18448HYpYrm60boziHG73bxN4CK5jG-1g\nFrom (redirected): https://drive.google.com/uc?id=18448HYpYrm60boziHG73bxN4CK5jG-1g&amp;confirm=t&amp;uuid=1c1721af-76e8-4f9c-9e5f-bf15e9e4c66f\nTo: /net/vast-storage/scratch/vast/mcdermott/lakshmin/hackathon-test/bioplnn/examples/turaga-dros-visual-connectome.pt\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 111M/111M [00:01&lt;00:00, 108MB/s]\n</code></pre>"},{"location":"examples/connectome_backward/#setting-up-the-classifier-wrapper","title":"Setting up the classifier wrapper","text":"<p>Here, we have written a simple utility that adds an output projecting layer from the connectome to the desired logit space. Again, this is simply an example. Please feel free to add sophistication to this as you please.</p> <pre><code>model = ConnectomeODEClassifier(\n    rnn_kwargs={\n        \"input_size\": input_size,\n        \"hidden_size\": num_neurons,\n        \"connectivity_hh\": torch.abs(connectome),\n        \"connectivity_ih\": input_projection_matrix,\n        \"output_neurons\": output_projection_matrix,\n        \"nonlinearity\": \"Sigmoid\",\n        \"batch_first\": False,\n        \"compile_solver_kwargs\": {\n            \"mode\": \"max-autotune\",\n            \"dynamic\": False,\n            \"fullgraph\": True,\n        },\n    },\n    num_classes = 10,\n    fc_dim = 256,\n    dropout = 0.5,\n).to(device)\nprint(model)\n\n# Define the optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# Define the loss function\ncriterion = nn.CrossEntropyLoss()\n</code></pre> <pre><code>ConnectomeODEClassifier(\n  (rnn): ConnectomeODERNN(\n    (nonlinearity): Sigmoid()\n    (hh): SparseLinear()\n    (ih): SparseLinear()\n    (solver): OptimizedModule(\n      (_orig_mod): AutoDiffAdjoint(step_method=Dopri5(\n        (term): ODETerm()\n      ), step_size_controller=IntegralController(\n        (term): ODETerm()\n      ), max_steps=None, backprop_through_step_size_control=True)\n    )\n  )\n  (out_layer): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=47521, out_features=256, bias=True)\n    (2): ReLU()\n    (3): Dropout(p=0.5, inplace=False)\n    (4): Linear(in_features=256, out_features=10, bias=True)\n  )\n)\n</code></pre> <pre><code># Dataloader setup\ntransform = T.Compose([T.ToTensor(), T.Normalize((0.1307,), (0.3081,))])\ntrain_data = MNIST(root=\"data\", train=True, transform=transform, download=True)\ntrain_loader = DataLoader(\n    train_data, batch_size=128, num_workers=0, shuffle=True\n)\n</code></pre> <pre><code>n_epochs = 1\n# print training statistics every five batches\nlog_frequency = 50\nmodel = model.train()\n</code></pre>"},{"location":"examples/connectome_backward/#note-things-to-look-out-for","title":"Note: Things to look out for","text":"<ul> <li>Depending on the GPU you have access to and the number of CPUs you will have to adjust <code>batch_size</code> and <code>num_workers</code> in the DataLoader, as well as <code>num_steps</code> in the model forward pass. For reference, on a A100 GPU and a single CPU <code>batch_size = 256</code>, <code>num_workers = 0</code>, and <code>num_steps=5</code> is a reasonable estimate.</li> <li>In this example, we have used the torch compiler with the <code>max-autotune</code> flag. This means the first few steps in the first epoch WILL BE EXTREMELY SLOW. But, this will dramatically improve as the training goes on. Fret not, early on!</li> </ul> <pre><code>running_loss, running_correct, running_total = 0, 0, 0\n\nfor epoch in range(n_epochs):\n    for i, (x, labels) in enumerate(train_loader):\n        x = x.to(device)\n        labels = labels.to(device)\n        torch._inductor.cudagraph_mark_step_begin()\n        logits = model(x, num_steps=2)\n        loss = criterion(logits, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Calculate running accuracy and loss\n        _, predicted = torch.max(logits, 1)\n        running_total += labels.size(0)\n        running_correct += (predicted == labels).sum().item()\n        running_loss += loss.item()\n\n        running_acc = running_correct / running_total\n        if (i + 1)%log_frequency == 0:\n            print(\n                f\"Training | Epoch: {epoch} | \" +\n                f\"Loss: {running_loss:.4f} | \" +\n                f\"Acc: {running_acc:.2%}\"\n            )\n            running_loss, running_correct, running_total = 0, 0, 0\n</code></pre> <pre><code>Training | Epoch: 0 | Loss: 180.6905 | Acc: 10.23%\nTraining | Epoch: 0 | Loss: 115.1636 | Acc: 9.80%\nTraining | Epoch: 0 | Loss: 115.1504 | Acc: 9.77%\nTraining | Epoch: 0 | Loss: 115.1351 | Acc: 9.72%\nTraining | Epoch: 0 | Loss: 115.1452 | Acc: 9.11%\nTraining | Epoch: 0 | Loss: 115.0842 | Acc: 10.80%\nTraining | Epoch: 0 | Loss: 115.1163 | Acc: 11.39%\nTraining | Epoch: 0 | Loss: 115.0861 | Acc: 10.94%\nTraining | Epoch: 0 | Loss: 115.0515 | Acc: 11.91%\n</code></pre>"},{"location":"examples/connectome_forward/","title":"Running forward dynamics in a connectome-constrained model","text":"<p>The goal of this tutorial is to walk you through the following steps:</p> <ul> <li>Initializing a sparse neural network model based on a connectome. In this example, we will use the optic lobe of Drosophila as done in Lappalainen et al. (2024).</li> <li>Specifying input and output projections.</li> <li>Driving activity in the network with visual inputs.</li> </ul> <p>For demonstration purposes, we'll use flattened MNIST images as inputs into the connectome. This is however simplistic and we do allow for arbitrarily complex input mappings. To learn how to do that please refer to the API.</p> <pre><code>import torch, os\nimport torchvision.transforms as T\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import MNIST\nfrom tqdm import tqdm\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom bioplnn.models import ConnectomeODERNN\n</code></pre> <p>Check the device to make sure you are on a GPU. If you aren't its not a big deal. Its just going to take much longer!</p> <pre><code>device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.set_float32_matmul_precision(\"high\")\nprint('Using device: {}'.format(device))\n</code></pre> <pre><code>Using device: cuda\n</code></pre> <p>Download the connectome and read it in as a torch tensor. We have pre-processed this as a sparse tensor for the purposes of this example.</p> <pre><code>!gdown \"https://drive.google.com/uc?id=18448HYpYrm60boziHG73bxN4CK5jG-1g\"\nconnectome = torch.load('turaga-dros-visual-connectome.pt', weights_only=True)\n</code></pre> <pre><code>Downloading...\nFrom (original): https://drive.google.com/uc?id=18448HYpYrm60boziHG73bxN4CK5jG-1g\nFrom (redirected): https://drive.google.com/uc?id=18448HYpYrm60boziHG73bxN4CK5jG-1g&amp;confirm=t&amp;uuid=35c6f337-799b-41ed-a155-7a469ccf8124\nTo: /net/vast-storage/scratch/vast/mcdermott/lakshmin/hackathon-test/bioplnn/examples/turaga-dros-visual-connectome.pt\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 111M/111M [00:00&lt;00:00, 112MB/s]\n</code></pre> <pre><code>print('Connectome dimensions: {}x{}'.format(connectome.shape[0], connectome.shape[1]))\nprint('Number of synapses: {}'.format(connectome._nnz()))\n\nspatial_extent = 100\nvmin, vmax = connectome.values().min(), connectome.values().max()\nfig = plt.figure(figsize=(6, 6))\nax = fig.add_subplot(111)\nax.imshow(torch.abs(connectome.to_dense()[:spatial_extent, :spatial_extent]), cmap='inferno')\nax.set_xlabel('Postsynaptic', fontsize=18)\nax.set_ylabel('Presynaptic', fontsize=18)\nax.set_xticks([0, spatial_extent-1])\nax.set_yticks([0, spatial_extent-1])\nplt.show()\n</code></pre> <pre><code>Connectome dimensions: 47521x47521\nNumber of synapses: 4623254\n</code></pre> <p></p>"},{"location":"examples/connectome_forward/#creating-input-and-output-projection-matrices","title":"Creating input and output projection matrices","text":"<p>To drive the network with external inputs, you'd want to specify the subset of neurons in the model that receive input or project to downstream areas. We have an utility to create these sparse projection matrices. For the purposes of this example, we shall pick a random subset of input/output neurons. In a world where each neuron receives external input, you can also initialize this projection as an arbitrary dense matrix.</p> <pre><code>from bioplnn.utils.torch import create_identity_ih_connectivity\n# since we are feeding in MNIST images\ninput_size = 28 * 28\nnum_neurons = connectome.shape[0]\n\ninput_projection_matrix = create_identity_ih_connectivity(\n                                input_size=input_size,\n                                num_neurons=num_neurons,\n                                input_indices=torch.randint(high=num_neurons, size=(input_size,))\n                            )\n\n# for now, lets just read outputs from all neurons\noutput_projection_matrix = None\n</code></pre>"},{"location":"examples/connectome_forward/#setting-up-the-connectome-constrained-model","title":"Setting up the connectome-constrained model","text":"<pre><code>connectome_rnn_kwargs = {\n        \"input_size\": input_size,\n        \"hidden_size\": num_neurons,\n        \"connectivity_hh\": connectome,\n        \"connectivity_ih\": input_projection_matrix,\n        \"output_neurons\": output_projection_matrix,\n        \"nonlinearity\": \"Sigmoid\",\n        \"batch_first\": False,\n        \"compile_solver_kwargs\": {\n            \"mode\": \"max-autotune\",\n            \"dynamic\": False,\n            \"fullgraph\": True,\n        }\n}\nmodel = ConnectomeODERNN(**connectome_rnn_kwargs).to(device)\nprint(model)\n</code></pre> <pre><code>ConnectomeODERNN(\n  (nonlinearity): Sigmoid()\n  (hh): SparseLinear()\n  (ih): SparseLinear()\n  (layernorm): Identity()\n  (solver): OptimizedModule(\n    (_orig_mod): AutoDiffAdjoint(step_method=Dopri5(\n      (term): ODETerm()\n    ), step_size_controller=IntegralController(\n      (term): ODETerm()\n    ), max_steps=None, backprop_through_step_size_control=True)\n  )\n)\n</code></pre> <pre><code># get some data for us to pipe into the model\ntransform = T.Compose([T.ToTensor(), T.Normalize((0.1307,), (0.3081,))])\ntrain_data = MNIST(root=\"data\", train=True, transform=transform, download=True)\ntrain_loader = DataLoader(\n    train_data, batch_size=8, num_workers=0, shuffle=True\n)\n</code></pre> <pre><code># getting one batch of the input\nx, label = next(iter(train_loader))\nprint(f\"x shape: {x.shape}, label_shape: {label.shape}\")\nx = x.flatten(1)\nx = x.to(device)\nprint(f\"x flattened shape: {x.shape}\")\n</code></pre> <pre><code>x shape: torch.Size([8, 1, 28, 28]), label_shape: torch.Size([8])\nx flattened shape: torch.Size([8, 784])\n</code></pre> <pre><code>model.eval()\n_, neural_activities, timesteps = model(x, start_time=0, end_time=1.0, num_steps=20)\nprint(f\"Neural activity shape: {neural_activities.shape}\")\n</code></pre> <pre><code>/scratch2/weka/mcdermott/lakshmin/conda_envs/test_bioplnn_env/lib/python3.12/site-packages/torch/_inductor/cudagraph_trees.py:2345: UserWarning: Unable to hit fast path of CUDAGraphs because of pending, uninvoked backwards. Consider running with torch.no_grad() or using torch.compiler.cudagraph_mark_step_begin() before each model invocation\n  warnings.warn(\n\n\nNeural activity shape: torch.Size([20, 8, 47521])\n</code></pre> <pre><code>fig = plt.figure(figsize=(6,6))\nax = fig.add_subplot(111)\nax.plot(\n    timesteps[:, 0].detach().cpu().numpy(),\n    neural_activities[:, 0, torch.randint(0, 47521, (25,))].detach().cpu().numpy()\n)\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.set_xlabel('Time (in a.u.)', fontsize=18)\nax.set_ylabel('Activity', fontsize=18)\n</code></pre> <pre><code>Text(0, 0.5, 'Activity')\n</code></pre>"},{"location":"examples/connectome_forward_neuron_types/","title":"Connectome-constrained models with neural types","text":"<p>In this tutorial, we extend on the previous example, but now explicitly specify cell-type information for each neuron in our connectome. Cell-types by construction can share parameters such as synapse sign, time constants and nonlinearities.</p> <pre><code>import torch\nimport os\nimport torchvision.transforms as T\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import MNIST\nfrom tqdm import tqdm\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom bioplnn.models import ConnectomeODERNN\n</code></pre> <p>Check the device to make sure you are on a GPU. If you aren't its not a big deal. Its just going to take much longer!</p> <pre><code>device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.set_float32_matmul_precision(\"high\")\nprint(\"Using device: {}\".format(device))\n</code></pre> <pre><code>Using device: cuda\n</code></pre> <p>Download the connectome and read it in as a torch tensor. We have pre-processed this as a sparse tensor for the purposes of this example. In addition to the connectome from before, we also have typing information preprocessed for you. Feel free to check its contents out.</p> <pre><code>save_dir = \"connectivity/turaga\"\nos.makedirs(save_dir, exist_ok=True)\nsave_path = f\"{save_dir}/turaga-dros-visual-connectome.pt\"\n!gdown \"https://drive.google.com/uc?id=18448HYpYrm60boziHG73bxN4CK5jG-1g\" -O \"{save_path}\"\nconnectome = torch.load(save_path, weights_only=True)\n\nsave_path_index_1 = f\"{save_dir}/example_indices_ntype1.pt\"\n!gdown \"https://drive.google.com/uc?id=19ePGRpMznn2l1Mp8Gu0rTk-PP-EDlyEC\" -O \"{save_path_index_1}\"\nex_indices_neuron_type1 = torch.load(save_path_index_1, weights_only=True)\n\nsave_path_index_2 = f\"{save_dir}/example_indices_ntype2.pt\"\n!gdown \"https://drive.google.com/uc?id=1eb0H-WTQWg1DzFZ201-ihIFqgZ8u3N0Y\" -O \"{save_path_index_2}\"\nex_indices_neuron_type2 = torch.load(save_path_index_2, weights_only=True)\n</code></pre> <pre><code>Downloading...\nFrom (original): https://drive.google.com/uc?id=18448HYpYrm60boziHG73bxN4CK5jG-1g\nFrom (redirected): https://drive.google.com/uc?id=18448HYpYrm60boziHG73bxN4CK5jG-1g&amp;confirm=t&amp;uuid=27f9b878-4dc9-4201-8ae8-83319e4db65d\nTo: /net/vast-storage/scratch/vast/mcdermott/lakshmin/hackathon-test/bioplnn/examples/connectivity/turaga/turaga-dros-visual-connectome.pt\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 111M/111M [00:01&lt;00:00, 89.2MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=19ePGRpMznn2l1Mp8Gu0rTk-PP-EDlyEC\nTo: /net/vast-storage/scratch/vast/mcdermott/lakshmin/hackathon-test/bioplnn/examples/connectivity/turaga/example_indices_ntype1.pt\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 381k/381k [00:00&lt;00:00, 17.9MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1eb0H-WTQWg1DzFZ201-ihIFqgZ8u3N0Y\nTo: /net/vast-storage/scratch/vast/mcdermott/lakshmin/hackathon-test/bioplnn/examples/connectivity/turaga/example_indices_ntype2.pt\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 381k/381k [00:00&lt;00:00, 12.6MB/s]\n</code></pre> <pre><code>print(\n    \"Connectome dimensions: {}x{}\".format(\n        connectome.shape[0], connectome.shape[1]\n    )\n)\nprint(\"Number of synapses: {}\".format(connectome._nnz()))\n\nspatial_extent = 100\nvmin, vmax = connectome.values().min(), connectome.values().max()\nfig = plt.figure(figsize=(6, 6))\nax = fig.add_subplot(111)\nax.imshow(\n    torch.abs(connectome.to_dense()[:spatial_extent, :spatial_extent]),\n    cmap=\"inferno\",\n)\nax.set_xlabel(\"Postsynaptic\", fontsize=18)\nax.set_ylabel(\"Presynaptic\", fontsize=18)\nax.set_xticks([0, spatial_extent - 1])\nax.set_yticks([0, spatial_extent - 1])\nplt.show()\n</code></pre> <pre><code>Connectome dimensions: 47521x47521\nNumber of synapses: 4623254\n</code></pre> <p></p>"},{"location":"examples/connectome_forward_neuron_types/#creating-input-and-output-projection-matrices","title":"Creating input and output projection matrices","text":"<p>To drive the network with external inputs, you'd want to specify the subset of neurons in the model that receive input or project to downstream areas. We have an utility to create these sparse projection matrices. For the purposes of this example, we shall pick a random subset of input/output neurons. In a world where each neuron receives external input, you can also initialize this projection as an arbitrary dense matrix.</p> <pre><code>from bioplnn.utils.torch import create_identity_ih_connectivity\n\n# since we are feeding in MNIST images\ninput_size = 28 * 28\nnum_neurons = connectome.shape[0]\n\ninput_projection_matrix = create_identity_ih_connectivity(\n    input_size=input_size,\n    num_neurons=num_neurons,\n    input_indices=torch.randint(high=num_neurons, size=(input_size,)),\n)\n\n# for now, lets just read outputs from all neurons\noutput_projection_matrix = None\n</code></pre>"},{"location":"examples/connectome_forward_neuron_types/#setting-up-the-connectome-constrained-model","title":"Setting up the connectome-constrained model","text":"<pre><code>connectome_rnn_kwargs = {\n    \"input_size\": input_size,\n    \"hidden_size\": num_neurons,\n    \"connectivity_hh\": connectome,\n    \"connectivity_ih\": input_projection_matrix,\n    \"output_neurons\": output_projection_matrix,\n    \"num_neuron_types\": 2,\n\n    # this is new!\n    \"neuron_type_class\": [\"excitatory\", \"inhibitory\"],\n    \"neuron_type_indices\": [\n        ex_indices_neuron_type1,\n        ex_indices_neuron_type2,\n    ],\n\n    # note how the nonlinearity is global -- it can be per-celltype if needed\n    # also note how the taus are initialized per neuron type.\n    \"neuron_type_nonlinearity\": \"Sigmoid\",\n    \"neuron_type_tau_init\": [1.25, 2.0],\n\n    # flag to determine if these are tunable via gradients. same holds true for synaptic gains.\n    \"train_tau\": True,\n    \"batch_first\": False,\n    \"compile_solver_kwargs\": {\n        \"mode\": \"max-autotune\",\n        \"dynamic\": False,\n        \"fullgraph\": True,\n    },\n}\nmodel = ConnectomeODERNN(**connectome_rnn_kwargs).to(device)\nprint(model)\n</code></pre> <pre><code>ConnectomeODERNN(\n  (nonlinearity): Sigmoid()\n  (hh): SparseLinear()\n  (ih): SparseLinear()\n  (solver): OptimizedModule(\n    (_orig_mod): AutoDiffAdjoint(step_method=Dopri5(\n      (term): ODETerm()\n    ), step_size_controller=IntegralController(\n      (term): ODETerm()\n    ), max_steps=None, backprop_through_step_size_control=True)\n  )\n  (neuron_type_indices): ParameterList(\n      (0): Parameter containing: [torch.int64 of size 23760 (cuda:0)]\n      (1): Parameter containing: [torch.int64 of size 23761 (cuda:0)]\n  )\n  (neuron_type_nonlinearity): ModuleList(\n    (0-1): 2 x Sigmoid()\n  )\n)\n</code></pre> <pre><code># get some data for us to pipe into the model\ntransform = T.Compose([T.ToTensor(), T.Normalize((0.1307,), (0.3081,))])\ntrain_data = MNIST(root=\"data\", train=True, transform=transform, download=True)\ntrain_loader = DataLoader(\n    train_data, batch_size=8, num_workers=0, shuffle=True\n)\n</code></pre> <pre><code># getting one batch of the input\nx, label = next(iter(train_loader))\nprint(f\"x shape: {x.shape}, label_shape: {label.shape}\")\nx = x.flatten(1)\nx = x.to(device)\nprint(f\"x flattened shape: {x.shape}\")\n</code></pre> <pre><code>x shape: torch.Size([8, 1, 28, 28]), label_shape: torch.Size([8])\nx flattened shape: torch.Size([8, 784])\n</code></pre> <pre><code>model.eval()\n_, neural_activities, timesteps = model(\n    x, start_time=0, end_time=1.0, num_steps=20\n)\nprint(f\"Neural activity shape: {neural_activities.shape}\")\n</code></pre> <pre><code>Neural activity shape: torch.Size([20, 8, 47521])\n\n\n/scratch2/weka/mcdermott/lakshmin/conda_envs/test_bioplnn_env/lib/python3.12/site-packages/torch/_inductor/cudagraph_trees.py:2345: UserWarning: Unable to hit fast path of CUDAGraphs because of pending, uninvoked backwards. Consider running with torch.no_grad() or using torch.compiler.cudagraph_mark_step_begin() before each model invocation\n  warnings.warn(\n</code></pre> <pre><code>fig = plt.figure(figsize=(6, 6))\nax = fig.add_subplot(111)\nax.plot(\n    timesteps[:, 0].detach().cpu().numpy(),\n    neural_activities[:, 0, torch.randint(0, 47521, (25,))]\n    .detach()\n    .cpu()\n    .numpy(),\n)\nax.spines[\"top\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)\nax.set_xlabel(\"Time (in a.u.)\", fontsize=18)\nax.set_ylabel(\"Activity\", fontsize=18)\n</code></pre> <pre><code>Text(0, 0.5, 'Activity')\n</code></pre>"},{"location":"reference/","title":"API Reference:","text":"<ul> <li> <p>Spatially embedded networks Model constructors and helpers to wire up cortical architectures with varying levels of biological specification.</p> </li> <li> <p>Connectome-constrained models Model constructors and helpers to wire up cortical architectures with varying levels of biological specification.</p> </li> <li> <p>Sparse utilities Model constructors and helpers to wire up cortical architectures with varying levels of biological specification.</p> </li> <li> <p>Misc utilities Model constructors and helpers to wire up cortical architectures with varying levels of biological specification.</p> </li> </ul>"},{"location":"reference/connectome/","title":"Connectome-constrained model utilities","text":""},{"location":"reference/connectome/#src.bioplnn.models.connectome","title":"<code>connectome</code>","text":""},{"location":"reference/connectome/#src.bioplnn.models.connectome.ConnectomeODERNN","title":"<code>ConnectomeODERNN</code>","text":"<p>               Bases: <code>_ConnectomeRNNMixIn</code>, <code>SparseODERNN</code></p> Source code in <code>src/bioplnn/models/connectome.py</code> <pre><code>class ConnectomeODERNN(_ConnectomeRNNMixIn, SparseODERNN):\n    f\"\"\"Continuous-time ConnectomeRNN using ODE solver integration.\n\n    A continuous-time version of the ConnectomeRNN that simulates neural\n    dynamics using an ODE solver and computes the gradient with respect to the\n    parameters for efficient training.\n\n    Key features:\n        - ODE solver integration: Built on top of `SparseODERNN`, allowing for\n          the efficient computation of the gradients of the hidden states with\n          respect to the parameters. In the particular, the `torchode.AutoDiffAdjoint`\n          solver allows for O(1) computational complexity for the backward pass with\n          respect to the number of simulated timesteps.\n        {key_features_docstring}\n\n    Attributes:\n        {attributes_docstring}\n\n    Example:\n        &gt;&gt;&gt; connectome = create_sparse_topographic_connectome((10, 10), 0.1, 10, True)\n        &gt;&gt;&gt; rnn = ConnectomeRNN(10, 10, connectome)\n        &gt;&gt;&gt; odenn = ConnectomeODERNN(10, 10, connectome)\n    \"\"\"\n\n    def update_fn(\n        self, t: torch.Tensor, h: torch.Tensor, args: Mapping[str, Any]\n    ) -&gt; torch.Tensor:\n        \"\"\"ODE function for neural dynamics with neuron type differentiation.\n\n        Args:\n            t: Current time point.\n            h: Current hidden state.\n            args: Additional arguments containing:\n                x: Input sequence tensor\n                start_time: Integration start time\n                end_time: Integration end time\n\n        Returns:\n            Rate of change of hidden state (dh/dt)\n        \"\"\"\n        h = h.t()\n\n        x = args[\"x\"]\n        start_time = args[\"start_time\"]\n        end_time = args[\"end_time\"]\n        batch_size = x.shape[-1]\n\n        # Get index corresponding to time t\n        idx = self._index_from_time(t, x, start_time, end_time)\n\n        # Get input at time t\n        batch_indices = torch.arange(batch_size, device=idx.device)\n        x_t = x[idx, :, batch_indices].t()\n\n        # Apply sign mask to input\n        h_signed = h * self.neuron_sign_mask\n\n        # Compute new hidden state\n        h_new = self.ih(x_t) + self.hh(h_signed)\n\n        if self.neuron_nonlinearity_mode == \"one\":\n            h_new = self.neuron_nonlinearity(h_new)\n        else:\n            for i in range(self.num_neuron_types):\n                h_new[self.neuron_type_indices[i]] = self.neuron_nonlinearity[  # type: ignore\n                    i\n                ](h_new[self.neuron_type_indices[i]])\n\n        # Rectify hidden state to ensure it's non-negative\n        # Note: The user is still expected to provide a nonlinearity that\n        #   ensures non-negativity, e.g. sigmoid.\n        h_new = F.relu(h_new)\n\n        # Compute rate of change of hidden state\n        if self.neuron_tau_mode == \"per_neuron\":\n            dhdt = (h_new - h) / self.tau\n        else:\n            dhdt = (h_new - h) / self.tau[self.neuron_type_mask, :]\n\n        return dhdt.t()\n\n    def forward(\n        self,\n        x,\n        num_evals: int = 2,\n        start_time: float = 0.0,\n        end_time: float = 1.0,\n        neuron_state0: Optional[torch.Tensor] = None,\n        neuron_state_init_fn: Optional[Union[str, TensorInitFnType]] = None,\n    ):\n        \"\"\"Forward pass of the ConnectomeODERNN layer.\n\n        Wraps the `SparseODERNN.forward` method to change nomenclature.\n\n        See `SparseODERNN.forward` for more details.\n        \"\"\"\n        self._clamp_tau()\n\n        return super().forward(\n            x=x,\n            num_evals=num_evals,\n            start_time=start_time,\n            end_time=end_time,\n            h0=neuron_state0,\n            hidden_init_fn=neuron_state_init_fn,\n        )\n</code></pre>"},{"location":"reference/connectome/#src.bioplnn.models.connectome.ConnectomeODERNN.forward","title":"<code>forward(x, num_evals=2, start_time=0.0, end_time=1.0, neuron_state0=None, neuron_state_init_fn=None)</code>","text":"<p>Forward pass of the ConnectomeODERNN layer.</p> <p>Wraps the <code>SparseODERNN.forward</code> method to change nomenclature.</p> <p>See <code>SparseODERNN.forward</code> for more details.</p> Source code in <code>src/bioplnn/models/connectome.py</code> <pre><code>def forward(\n    self,\n    x,\n    num_evals: int = 2,\n    start_time: float = 0.0,\n    end_time: float = 1.0,\n    neuron_state0: Optional[torch.Tensor] = None,\n    neuron_state_init_fn: Optional[Union[str, TensorInitFnType]] = None,\n):\n    \"\"\"Forward pass of the ConnectomeODERNN layer.\n\n    Wraps the `SparseODERNN.forward` method to change nomenclature.\n\n    See `SparseODERNN.forward` for more details.\n    \"\"\"\n    self._clamp_tau()\n\n    return super().forward(\n        x=x,\n        num_evals=num_evals,\n        start_time=start_time,\n        end_time=end_time,\n        h0=neuron_state0,\n        hidden_init_fn=neuron_state_init_fn,\n    )\n</code></pre>"},{"location":"reference/connectome/#src.bioplnn.models.connectome.ConnectomeODERNN.update_fn","title":"<code>update_fn(t, h, args)</code>","text":"<p>ODE function for neural dynamics with neuron type differentiation.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Tensor</code> <p>Current time point.</p> required <code>h</code> <code>Tensor</code> <p>Current hidden state.</p> required <code>args</code> <code>Mapping[str, Any]</code> <p>Additional arguments containing: x: Input sequence tensor start_time: Integration start time end_time: Integration end time</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Rate of change of hidden state (dh/dt)</p> Source code in <code>src/bioplnn/models/connectome.py</code> <pre><code>def update_fn(\n    self, t: torch.Tensor, h: torch.Tensor, args: Mapping[str, Any]\n) -&gt; torch.Tensor:\n    \"\"\"ODE function for neural dynamics with neuron type differentiation.\n\n    Args:\n        t: Current time point.\n        h: Current hidden state.\n        args: Additional arguments containing:\n            x: Input sequence tensor\n            start_time: Integration start time\n            end_time: Integration end time\n\n    Returns:\n        Rate of change of hidden state (dh/dt)\n    \"\"\"\n    h = h.t()\n\n    x = args[\"x\"]\n    start_time = args[\"start_time\"]\n    end_time = args[\"end_time\"]\n    batch_size = x.shape[-1]\n\n    # Get index corresponding to time t\n    idx = self._index_from_time(t, x, start_time, end_time)\n\n    # Get input at time t\n    batch_indices = torch.arange(batch_size, device=idx.device)\n    x_t = x[idx, :, batch_indices].t()\n\n    # Apply sign mask to input\n    h_signed = h * self.neuron_sign_mask\n\n    # Compute new hidden state\n    h_new = self.ih(x_t) + self.hh(h_signed)\n\n    if self.neuron_nonlinearity_mode == \"one\":\n        h_new = self.neuron_nonlinearity(h_new)\n    else:\n        for i in range(self.num_neuron_types):\n            h_new[self.neuron_type_indices[i]] = self.neuron_nonlinearity[  # type: ignore\n                i\n            ](h_new[self.neuron_type_indices[i]])\n\n    # Rectify hidden state to ensure it's non-negative\n    # Note: The user is still expected to provide a nonlinearity that\n    #   ensures non-negativity, e.g. sigmoid.\n    h_new = F.relu(h_new)\n\n    # Compute rate of change of hidden state\n    if self.neuron_tau_mode == \"per_neuron\":\n        dhdt = (h_new - h) / self.tau\n    else:\n        dhdt = (h_new - h) / self.tau[self.neuron_type_mask, :]\n\n    return dhdt.t()\n</code></pre>"},{"location":"reference/connectome/#src.bioplnn.models.connectome.ConnectomeRNN","title":"<code>ConnectomeRNN</code>","text":"<p>               Bases: <code>_ConnectomeRNNMixIn</code>, <code>SparseRNN</code></p> Source code in <code>src/bioplnn/models/connectome.py</code> <pre><code>class ConnectomeRNN(_ConnectomeRNNMixIn, SparseRNN):\n    f\"\"\"Connectome Recurrent Neural Network.\n\n    An RNN that leverages the `SparseRNN` class to efficiently simulate a\n    sparsely-connected network of neurons with biologically-inspired dynamics.\n\n    Key features:\n        {key_features_docstring}\n\n    Attributes:\n        {attributes_docstring}\n    \"\"\"\n\n    def update_fn(self, x_t: torch.Tensor, h: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"update hidden state for one timestep.\n\n        Args:\n            x_t (torch.Tensor): Input at current timestep.\n            h (torch.Tensor): Hidden state at previous timestep.\n\n        Returns:\n            torch.Tensor: Updated hidden state.\n        \"\"\"\n\n        # Apply sign mask to input\n        h_signed = h * self.neuron_sign_mask\n\n        # Compute new hidden state\n        h_new = self.ih(x_t) + self.hh(h_signed)\n\n        if self.neuron_nonlinearity_mode == \"one\":\n            h_new = self.neuron_nonlinearity(h_new)\n        else:\n            for i in range(self.num_neuron_types):\n                h_new[self.neuron_type_indices[i]] = self.neuron_nonlinearity[  # type: ignore\n                    i\n                ](h_new[self.neuron_type_indices[i]])\n\n        # Rectify hidden state to ensure it's non-negative\n        # Note: The user is still expected to provide a nonlinearity that\n        #   ensures non-negativity, e.g. sigmoid.\n        h_new = F.relu(h_new)\n\n        # Compute rate of change of hidden state\n        if self.neuron_tau_mode == \"per_neuron\":\n            tau_inv = 1 / self.tau\n            return tau_inv * h_new + (1 - tau_inv) * h\n        else:\n            tau_inv = 1 / self.tau[self.neuron_type_mask, :]\n            return tau_inv * h_new + (1 - tau_inv) * h\n\n    def forward(\n        self,\n        x,\n        num_steps: Optional[int] = None,\n        neuron_state0: Optional[torch.Tensor] = None,\n        neuron_state_init_fn: Optional[Union[str, TensorInitFnType]] = None,\n    ):\n        \"\"\"Forward pass of the ConnectomeODERNN layer.\n\n        Wraps the `SparseODERNN.forward` method to change nomenclature.\n\n        See `SparseODERNN.forward` for more details.\n        \"\"\"\n        self._clamp_tau()\n\n        return super().forward(\n            x=x,\n            num_steps=num_steps,\n            h0=neuron_state0,\n            hidden_init_fn=neuron_state_init_fn,\n        )\n</code></pre>"},{"location":"reference/connectome/#src.bioplnn.models.connectome.ConnectomeRNN.forward","title":"<code>forward(x, num_steps=None, neuron_state0=None, neuron_state_init_fn=None)</code>","text":"<p>Forward pass of the ConnectomeODERNN layer.</p> <p>Wraps the <code>SparseODERNN.forward</code> method to change nomenclature.</p> <p>See <code>SparseODERNN.forward</code> for more details.</p> Source code in <code>src/bioplnn/models/connectome.py</code> <pre><code>def forward(\n    self,\n    x,\n    num_steps: Optional[int] = None,\n    neuron_state0: Optional[torch.Tensor] = None,\n    neuron_state_init_fn: Optional[Union[str, TensorInitFnType]] = None,\n):\n    \"\"\"Forward pass of the ConnectomeODERNN layer.\n\n    Wraps the `SparseODERNN.forward` method to change nomenclature.\n\n    See `SparseODERNN.forward` for more details.\n    \"\"\"\n    self._clamp_tau()\n\n    return super().forward(\n        x=x,\n        num_steps=num_steps,\n        h0=neuron_state0,\n        hidden_init_fn=neuron_state_init_fn,\n    )\n</code></pre>"},{"location":"reference/connectome/#src.bioplnn.models.connectome.ConnectomeRNN.update_fn","title":"<code>update_fn(x_t, h)</code>","text":"<p>update hidden state for one timestep.</p> <p>Parameters:</p> Name Type Description Default <code>x_t</code> <code>Tensor</code> <p>Input at current timestep.</p> required <code>h</code> <code>Tensor</code> <p>Hidden state at previous timestep.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Updated hidden state.</p> Source code in <code>src/bioplnn/models/connectome.py</code> <pre><code>def update_fn(self, x_t: torch.Tensor, h: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"update hidden state for one timestep.\n\n    Args:\n        x_t (torch.Tensor): Input at current timestep.\n        h (torch.Tensor): Hidden state at previous timestep.\n\n    Returns:\n        torch.Tensor: Updated hidden state.\n    \"\"\"\n\n    # Apply sign mask to input\n    h_signed = h * self.neuron_sign_mask\n\n    # Compute new hidden state\n    h_new = self.ih(x_t) + self.hh(h_signed)\n\n    if self.neuron_nonlinearity_mode == \"one\":\n        h_new = self.neuron_nonlinearity(h_new)\n    else:\n        for i in range(self.num_neuron_types):\n            h_new[self.neuron_type_indices[i]] = self.neuron_nonlinearity[  # type: ignore\n                i\n            ](h_new[self.neuron_type_indices[i]])\n\n    # Rectify hidden state to ensure it's non-negative\n    # Note: The user is still expected to provide a nonlinearity that\n    #   ensures non-negativity, e.g. sigmoid.\n    h_new = F.relu(h_new)\n\n    # Compute rate of change of hidden state\n    if self.neuron_tau_mode == \"per_neuron\":\n        tau_inv = 1 / self.tau\n        return tau_inv * h_new + (1 - tau_inv) * h\n    else:\n        tau_inv = 1 / self.tau[self.neuron_type_mask, :]\n        return tau_inv * h_new + (1 - tau_inv) * h\n</code></pre>"},{"location":"reference/eirnn/","title":"Spatially embedded cortical networks","text":""},{"location":"reference/eirnn/#src.bioplnn.models.spatially_embedded","title":"<code>spatially_embedded</code>","text":""},{"location":"reference/eirnn/#src.bioplnn.models.spatially_embedded.Conv2dRectify","title":"<code>Conv2dRectify</code>","text":"<p>               Bases: <code>Conv2d</code></p> <p>Applies a 2d convolution with nonnegative weights and biases.</p> Source code in <code>src/bioplnn/models/spatially_embedded.py</code> <pre><code>class Conv2dRectify(nn.Conv2d):\n    \"\"\"Applies a 2d convolution with nonnegative weights and biases.\"\"\"\n\n    def forward(self, *args, **kwargs):\n        \"\"\"Forward pass of the layer.\n\n        Args:\n            *args: Positional arguments passed to `nn.Conv2d`.\n            **kwargs: Keyword arguments passed to `nn.Conv2d`.\n\n        Returns:\n            The convolution output.\n        \"\"\"\n        self.weight.data.clamp_(min=0.0)\n        if self.bias is not None:\n            self.bias.data.clamp_(min=0.0)\n        return super().forward(*args, **kwargs)\n</code></pre>"},{"location":"reference/eirnn/#src.bioplnn.models.spatially_embedded.Conv2dRectify.forward","title":"<code>forward(*args, **kwargs)</code>","text":"<p>Forward pass of the layer.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Positional arguments passed to <code>nn.Conv2d</code>.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments passed to <code>nn.Conv2d</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <p>The convolution output.</p> Source code in <code>src/bioplnn/models/spatially_embedded.py</code> <pre><code>def forward(self, *args, **kwargs):\n    \"\"\"Forward pass of the layer.\n\n    Args:\n        *args: Positional arguments passed to `nn.Conv2d`.\n        **kwargs: Keyword arguments passed to `nn.Conv2d`.\n\n    Returns:\n        The convolution output.\n    \"\"\"\n    self.weight.data.clamp_(min=0.0)\n    if self.bias is not None:\n        self.bias.data.clamp_(min=0.0)\n    return super().forward(*args, **kwargs)\n</code></pre>"},{"location":"reference/eirnn/#src.bioplnn.models.spatially_embedded.SpatiallyEmbeddedArea","title":"<code>SpatiallyEmbeddedArea</code>","text":"<p>               Bases: <code>Module</code></p> <p>A biologically-plausible, spatially embedded neural area.</p> <p>This module imposes a series of biologically-inspired constraints on artificial neural networks. At its core, it is a collection of 2D (hence spatially embedded) convolutional layers organized into a 'circuit motif'. Here, 'circuit motif' refers to the connectivity pattern between the input, feedback, neuron types, and output within a distinct neural area. For example, if we have two neuron types, an excitatory and an inhibitory, then the circuit motif determines which neuron types receive input, which neuron types receive feedback, which neuron types are connected to which other neuron types, and which neuron types project to the output of the area.</p>"},{"location":"reference/eirnn/#src.bioplnn.models.spatially_embedded.SpatiallyEmbeddedArea--key-features","title":"Key features:","text":"<ul> <li>Configurable neuron types (excitatory/inhibitory/hybrid)</li> <li>Configurable spatial extents of lateral connections (same/half)</li> <li>Convolutional connectivity between neuron populations</li> <li>Recurrent dynamics with learnable time constants</li> <li>Optional feedback connections</li> <li>Customizable activation functions</li> </ul> <p>Attributes:</p> Name Type Description <code>in_size</code> <code>tuple[int, int]</code> <p>Spatial size of the input data (height, width).</p> <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> <code>out_channels</code> <code>int</code> <p>Number of output channels.</p> <code>feedback_channels</code> <code>int</code> <p>Number of feedback channels (0 if none).</p> <code>use_feedback</code> <code>bool</code> <p>Whether this area receives feedback from another area.</p> <code>in_class</code> <code>str</code> <p>Class of input signal (\"excitatory\", \"inhibitory\", or \"hybrid\").</p> <code>feedback_class</code> <code>str</code> <p>Class of feedback signal (\"excitatory\", \"inhibitory\", or \"hybrid\").</p> <code>out_nonlinearity</code> <code>Module</code> <p>Nonlinearity applied to the output.</p> <code>num_neuron_types</code> <code>int</code> <p>Number of neuron types in the area.</p> <code>num_neuron_subtypes</code> <code>list[int]</code> <p>Number of subtypes for each neuron type.</p> <code>neuron_type_class</code> <code>list[str]</code> <p>Class of each neuron type (\"excitatory\", \"inhibitory\", or \"hybrid\").</p> <code>neuron_type_density</code> <code>list[str]</code> <p>Spatial density of each neuron type (\"same\" or \"half\").</p> <code>neuron_type_nonlinearity</code> <code>ModuleList</code> <p>Nonlinearity for each neuron type's activity.</p> <code>neuron_type_size</code> <code>list[tuple[int, int]]</code> <p>Spatial size of each neuron type.</p> <code>num_rows_connectivity</code> <code>int</code> <p>Number of rows in the connectivity matrix.</p> <code>num_cols_connectivity</code> <code>int</code> <p>Number of columns in the connectivity matrix.</p> <code>inter_neuron_type_connectivity</code> <code>ndarray</code> <p>Connectivity matrix for the circuit motif.</p> <code>inter_neuron_type_spatial_extents</code> <code>ndarray</code> <p>Spatial extent for each circuit motif connection.</p> <code>inter_neuron_type_num_subtype_groups</code> <code>ndarray</code> <p>Number of subtype groups for each circuit motif connection.</p> <code>inter_neuron_type_nonlinearity</code> <code>ndarray</code> <p>Nonlinearity for each circuit motif connection.</p> <code>inter_neuron_type_bias</code> <code>ndarray</code> <p>Whether to add a bias term for each circuit motif connection.</p> <code>tau_mode</code> <code>list[str]</code> <p>Mode determining which parts of neuron activity share a time constant.</p> <code>tau_init_fn</code> <code>list[Union[str, TensorInitFnType]]</code> <p>Initialization for the membrane time constants.</p> <code>tau</code> <code>ParameterList</code> <p>Learnable time constants for each neuron type.</p> <code>default_neuron_state_init_fn</code> <code>Union[str, TensorInitFnType]</code> <p>Default initialization for neuron states.</p> <code>default_feedback_state_init_fn</code> <code>Union[str, TensorInitFnType]</code> <p>Default initialization for feedback states.</p> <code>default_output_state_init_fn</code> <code>Union[str, TensorInitFnType]</code> <p>Default initialization for output states.</p> <code>convs</code> <code>ModuleDict</code> <p>Convolutional layers representing connections between neuron types.</p> <code>out_convs</code> <code>ModuleDict</code> <p>Convolutional layers connecting to the output.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = SpatiallyEmbeddedAreaConfig(\n...     in_size=(32, 32),\n...     in_channels=3,\n...     out_channels=16,\n...     num_neuron_types=2,\n...     num_neuron_subtypes=16,\n...     neuron_type_class=[\"excitatory\", \"inhibitory\"],\n...     neuron_type_density=[\"same\", \"half\"],\n...     neuron_type_nonlinearity=,\n...     inter_neuron_type_connectivity=[[1, 0, 0], [1, 1, 1], [1, 0, 0]],\n...     inter_neuron_type_spatial_extents=(3, 3),\n... )\n&gt;&gt;&gt; area = SpatiallyEmbeddedArea(config)\n&gt;&gt;&gt; print(area.summary())\n</code></pre> Source code in <code>src/bioplnn/models/spatially_embedded.py</code> <pre><code>class SpatiallyEmbeddedArea(nn.Module):\n    \"\"\"A biologically-plausible, spatially embedded neural area.\n\n    This module imposes a series of biologically-inspired constraints on\n    artificial neural networks. At its core, it is a collection of 2D (hence\n    spatially embedded) convolutional layers organized into a 'circuit\n    motif'. Here, 'circuit motif' refers to the connectivity pattern between\n    the input, feedback, neuron types, and output within a distinct neural\n    area. For example, if we have two neuron types, an excitatory and an\n    inhibitory, then the circuit motif determines which neuron types receive\n    input, which neuron types receive feedback, which neuron types are connected\n    to which other neuron types, and which neuron types project to the output of\n    the area.\n\n    ## Key features:\n\n    - Configurable neuron types (excitatory/inhibitory/hybrid)\n    - Configurable spatial extents of lateral connections (same/half)\n    - Convolutional connectivity between neuron populations\n    - Recurrent dynamics with learnable time constants\n    - Optional feedback connections\n    - Customizable activation functions\n\n    Attributes:\n        in_size (tuple[int, int]): Spatial size of the input data\n            (height, width).\n        in_channels (int): Number of input channels.\n        out_channels (int): Number of output channels.\n        feedback_channels (int): Number of feedback channels (0 if none).\n        use_feedback (bool): Whether this area receives feedback from another\n            area.\n        in_class (str): Class of input signal (\"excitatory\", \"inhibitory\", or\n            \"hybrid\").\n        feedback_class (str): Class of feedback signal (\"excitatory\", \"inhibitory\",\n            or \"hybrid\").\n        out_nonlinearity (nn.Module): Nonlinearity applied to the output.\n\n        num_neuron_types (int): Number of neuron types in the area.\n        num_neuron_subtypes (list[int]): Number of subtypes for each neuron type.\n        neuron_type_class (list[str]): Class of each neuron type (\"excitatory\",\n            \"inhibitory\", or \"hybrid\").\n        neuron_type_density (list[str]): Spatial density of each neuron type\n            (\"same\" or \"half\").\n        neuron_type_nonlinearity (nn.ModuleList): Nonlinearity for each neuron\n            type's activity.\n        neuron_type_size (list[tuple[int, int]]): Spatial size of each neuron type.\n\n        num_rows_connectivity (int): Number of rows in the connectivity matrix.\n        num_cols_connectivity (int): Number of columns in the connectivity matrix.\n        inter_neuron_type_connectivity (np.ndarray): Connectivity matrix for the\n            circuit motif.\n        inter_neuron_type_spatial_extents (np.ndarray): Spatial extent for each\n            circuit motif connection.\n        inter_neuron_type_num_subtype_groups (np.ndarray): Number of subtype groups\n            for each circuit motif connection.\n        inter_neuron_type_nonlinearity (np.ndarray): Nonlinearity for each circuit\n            motif connection.\n        inter_neuron_type_bias (np.ndarray): Whether to add a bias term for each\n            circuit motif connection.\n\n        tau_mode (list[str]): Mode determining which parts of neuron activity share\n            a time constant.\n        tau_init_fn (list[Union[str, TensorInitFnType]]): Initialization for the\n            membrane time constants.\n        tau (nn.ParameterList): Learnable time constants for each neuron type.\n\n        default_neuron_state_init_fn (Union[str, TensorInitFnType]): Default\n            initialization for neuron states.\n        default_feedback_state_init_fn (Union[str, TensorInitFnType]): Default\n            initialization for feedback states.\n        default_output_state_init_fn (Union[str, TensorInitFnType]): Default\n            initialization for output states.\n\n        convs (nn.ModuleDict): Convolutional layers representing connections between\n            neuron types.\n        out_convs (nn.ModuleDict): Convolutional layers connecting to the output.\n\n    Examples:\n        &gt;&gt;&gt; config = SpatiallyEmbeddedAreaConfig(\n        ...     in_size=(32, 32),\n        ...     in_channels=3,\n        ...     out_channels=16,\n        ...     num_neuron_types=2,\n        ...     num_neuron_subtypes=16,\n        ...     neuron_type_class=[\"excitatory\", \"inhibitory\"],\n        ...     neuron_type_density=[\"same\", \"half\"],\n        ...     neuron_type_nonlinearity=,\n        ...     inter_neuron_type_connectivity=[[1, 0, 0], [1, 1, 1], [1, 0, 0]],\n        ...     inter_neuron_type_spatial_extents=(3, 3),\n        ... )\n        &gt;&gt;&gt; area = SpatiallyEmbeddedArea(config)\n        &gt;&gt;&gt; print(area.summary())\n    \"\"\"\n\n    def __init__(\n        self, config: Optional[SpatiallyEmbeddedAreaConfig] = None, **kwargs\n    ):\n        \"\"\"Initialize the SpatiallyEmbeddedArea.\n\n        Args:\n            config: Configuration object that specifies the area architecture and parameters.\n                See SpatiallyEmbeddedAreaConfig for details. If None, parameters must be\n                provided as keyword arguments.\n            **kwargs: Keyword arguments to instantiate the configuration if\n                `config` is not provided. Cannot provide both `config` and\n                keyword arguments.\n\n        Raises:\n            ValueError: If an invalid configuration is provided.\n        \"\"\"\n\n        super().__init__()\n\n        if config is None:\n            config = SpatiallyEmbeddedAreaConfig(**kwargs)\n        elif kwargs:\n            raise ValueError(\n                \"Cannot provide both config and keyword arguments. Please provide \"\n                \"only one of the two.\"\n            )\n\n        #####################################################################\n        # Input, output, and feedback parameters\n        #####################################################################\n\n        self.in_size = config.in_size\n        self.in_channels = config.in_channels\n        self.out_channels = config.out_channels\n        self.feedback_channels = (\n            config.feedback_channels\n            if config.feedback_channels is not None\n            else 0\n        )\n        self.use_feedback = self.feedback_channels &gt; 0\n\n        self.in_class = config.in_class\n        check_possible_values(\n            \"in_class\",\n            (self.in_class,),\n            (\"excitatory\", \"inhibitory\", \"hybrid\"),\n        )\n        self.feedback_class = config.feedback_class\n        check_possible_values(\n            \"feedback_class\",\n            (self.feedback_class,),\n            (\"excitatory\", \"inhibitory\", \"hybrid\"),\n        )\n\n        self.out_nonlinearity = get_activation(config.out_nonlinearity)\n\n        #####################################################################\n        # Neuron type parameters\n        #####################################################################\n\n        self.num_neuron_types = config.num_neuron_types\n\n        # Format neuron type\n        self.num_neuron_subtypes = expand_list(\n            config.num_neuron_subtypes, self.num_neuron_types\n        )\n        self.neuron_type_class = expand_list(\n            config.neuron_type_class, self.num_neuron_types\n        )\n        check_possible_values(\n            \"neuron_type_class\",\n            self.neuron_type_class,\n            (\"excitatory\", \"inhibitory\", \"hybrid\"),\n        )\n        self.neuron_type_density = expand_list(\n            config.neuron_type_density, self.num_neuron_types\n        )\n        # TODO: Add support for quarter\n        check_possible_values(\n            \"neuron_type_density\",\n            self.neuron_type_density,\n            (\"same\", \"half\"),\n        )\n        neuron_type_nonlinearity = expand_list(\n            config.neuron_type_nonlinearity, self.num_neuron_types\n        )\n        self.neuron_type_nonlinearity = nn.ModuleList(\n            [\n                get_activation(nonlinearity)\n                for nonlinearity in neuron_type_nonlinearity\n            ]\n        )\n\n        # Save number of \"types\" for the input to and output from the area\n        self.num_rows_connectivity = (\n            1 + int(self.use_feedback) + self.num_neuron_types\n        )  # input + feedback + neurons\n        self.num_cols_connectivity = (\n            self.num_neuron_types + 1\n        )  # neurons + output\n\n        # Calculate half spatial size\n        self.half_size = (\n            ceil(self.in_size[0] / 2),\n            ceil(self.in_size[1] / 2),\n        )\n\n        self.neuron_type_size = [\n            self.in_size\n            if self.neuron_type_density[i] == \"same\"\n            else self.half_size\n            for i in range(self.num_neuron_types)\n        ]\n\n        #####################################################################\n        # Circuit motif connectivity\n        #####################################################################\n\n        # Format circuit connectivity\n        self.inter_neuron_type_connectivity = np.array(\n            config.inter_neuron_type_connectivity\n        )\n        if self.inter_neuron_type_connectivity.shape != (\n            self.num_rows_connectivity,\n            self.num_cols_connectivity,\n        ):\n            raise ValueError(\n                \"The shape of inter_neuron_type_connectivity must match the number of \"\n                \"rows and columns in the connectivity matrix.\"\n            )\n\n        # Format connectivity variables to match circuit connectivity\n        self.inter_neuron_type_spatial_extents = expand_array_2d(\n            config.inter_neuron_type_spatial_extents,\n            self.inter_neuron_type_connectivity.shape[0],\n            self.inter_neuron_type_connectivity.shape[1],\n            depth=1,\n        )\n        self.inter_neuron_type_num_subtype_groups = expand_array_2d(\n            config.inter_neuron_type_num_subtype_groups,\n            self.inter_neuron_type_connectivity.shape[0],\n            self.inter_neuron_type_connectivity.shape[1],\n        )\n        self.inter_neuron_type_nonlinearity = expand_array_2d(\n            config.inter_neuron_type_nonlinearity,\n            self.inter_neuron_type_connectivity.shape[0],\n            self.inter_neuron_type_connectivity.shape[1],\n        )\n        self.inter_neuron_type_bias = expand_array_2d(\n            config.inter_neuron_type_bias,\n            self.inter_neuron_type_connectivity.shape[0],\n            self.inter_neuron_type_connectivity.shape[1],\n        )\n\n        #####################################################################\n        # Circuit motif convolutions\n        # Here, we represent the circuit connectivity between neuron classes\n        # as an array of convolutions (implemented as a dictionary for\n        # efficiency). The convolution self.convs[f\"{i}-&gt;{j}\"] corresponds\n        # to the connection from neuron class i to neuron class j.\n        #####################################################################\n\n        self.convs = nn.ModuleDict()\n        self.out_convs = nn.ModuleDict()\n        for i, row in enumerate(self.inter_neuron_type_connectivity):\n            # Handle input neuron channel and spatial mode based on neuron type\n            conv_in_type = self._source_from_row_idx(i)\n            if conv_in_type == \"input\":\n                conv_in_channels = self.in_channels\n                conv_in_density = \"same\"\n            elif conv_in_type == \"feedback\":\n                conv_in_channels = self.feedback_channels\n                conv_in_density = \"same\"\n            else:\n                assert conv_in_type == \"cell\"\n                conv_in_channels = self.num_neuron_subtypes[\n                    i - 1 - int(self.use_feedback)\n                ]\n                conv_in_density = self.neuron_type_density[\n                    i - 1 - int(self.use_feedback)\n                ]\n\n            conv_in_class = self._class_from_row_idx(i)\n            if conv_in_class in (\"excitatory\", \"inhibitory\"):\n                Conv2d = Conv2dRectify\n            else:\n                assert conv_in_class == \"hybrid\"\n                Conv2d = nn.Conv2d\n\n            # Handle output neurons\n            # TODO: Optimize using smart grouping and convolution sharing\n            to_indices = np.nonzero(row)[0]\n            for j in to_indices:\n                if self.inter_neuron_type_connectivity[i, j]:\n                    # Handle output neuron channel and spatial mode based on neuron type\n                    conv_out_type = self._destination_from_col_idx(j)\n                    if conv_out_type == \"cell\":\n                        conv_out_channels: int = self.num_neuron_subtypes[j]  # type: ignore\n                        conv_out_density: str = self.neuron_type_density[j]  # type: ignore\n                    else:\n                        assert conv_out_type == \"output\"\n                        if conv_in_type in (\"input\", \"feedback\"):\n                            warnings.warn(\n                                \"Input or feedback is connected to output. \"\n                                \"This is typically undesired as the signal \"\n                                \"will bypass the neuron types and go directly \"\n                                \"to the output. Consider removing this \"\n                                \"connection in the inter_neuron_type_connectivity \"\n                                \"matrix.\"\n                            )\n                        conv_out_channels = self.out_channels\n                        conv_out_density = \"same\"\n\n                    # Handle stride upsampling if necessary\n                    conv = nn.Sequential()\n                    conv_stride = 1\n                    if (\n                        conv_in_density == \"half\"\n                        and conv_out_density == \"same\"\n                    ):\n                        conv_stride = 2\n                    elif (\n                        conv_in_density == \"same\"\n                        and conv_out_density == \"half\"\n                    ):\n                        conv.append(\n                            nn.Upsample(size=self.in_size, mode=\"bilinear\")\n                        )\n\n                    # Handle upsampling if necessary\n                    conv.append(\n                        Conv2d(\n                            in_channels=conv_in_channels,\n                            out_channels=conv_out_channels,\n                            kernel_size=self.inter_neuron_type_spatial_extents[\n                                i, j\n                            ],\n                            stride=conv_stride,\n                            padding=(\n                                self.inter_neuron_type_spatial_extents[i, j][0]\n                                // 2,\n                                self.inter_neuron_type_spatial_extents[i, j][1]\n                                // 2,\n                            ),\n                            groups=self.inter_neuron_type_num_subtype_groups[\n                                i, j\n                            ],\n                            bias=self.inter_neuron_type_bias[i, j],\n                        )\n                    )\n                    conv.append(\n                        get_activation(\n                            self.inter_neuron_type_nonlinearity[i, j]\n                        )\n                    )\n                    if conv_out_type == \"output\":\n                        self.out_convs[f\"{i}-&gt;out\"] = conv\n                    else:\n                        self.convs[f\"{i}-&gt;{j}\"] = conv\n\n        #####################################################################\n        # Post convolution operations\n        #####################################################################\n\n        # Initialize membrane time constants\n        self.tau_mode = expand_list(config.tau_mode, self.num_neuron_types)\n        check_possible_values(\n            \"tau_mode\",\n            self.tau_mode,\n            (\"subtype\", \"spatial\", \"subtype_spatial\", \"type\"),\n        )\n        self.tau_init_fn = expand_list(\n            config.tau_init_fn, self.num_neuron_types\n        )\n\n        self.tau = nn.ParameterList()\n        for i in range(self.num_neuron_types):\n            if self.tau_mode[i] == \"spatial\":\n                tau_channels = 1\n                tau_size = self.neuron_type_size[i]\n            elif self.tau_mode[i] == \"subtype\":\n                tau_channels = self.num_neuron_subtypes[i]\n                tau_size = (1, 1)\n            elif self.tau_mode[i] == \"subtype_spatial\":\n                tau_channels = self.num_neuron_subtypes[i]\n                tau_size = self.neuron_type_size[i]\n            else:\n                assert self.tau_mode[i] == \"type\"\n                tau_channels = 1\n                tau_size = (1, 1)\n\n            tau = init_tensor(\n                self.tau_init_fn[i],\n                1,\n                tau_channels,\n                *tau_size,\n            )\n            noise = torch.rand_like(tau) * 1e-6\n\n            self.tau.append(\n                nn.Parameter(\n                    tau + noise,\n                    requires_grad=True,\n                )\n            )\n\n        #####################################################################\n        # Tensor initialization\n        #####################################################################\n\n        self.default_neuron_state_init_fn = config.default_neuron_state_init_fn\n        self.default_feedback_state_init_fn = (\n            config.default_feedback_state_init_fn\n        )\n        self.default_output_state_init_fn = config.default_output_state_init_fn\n\n    def _source_from_row_idx(self, idx: int) -&gt; Optional[str]:\n        \"\"\"Converts a row index to the corresponding source.\n\n        Args:\n            idx: Row index in the circuit connectivity matrix.\n\n        Returns:\n            The source associated with the index. Can be \"input\",\n            \"feedback\", or \"cell\".\n        \"\"\"\n        if idx == 0:\n            return \"input\"\n        elif self.use_feedback and idx == 1:\n            return \"feedback\"\n        else:\n            return \"cell\"\n\n    def _class_from_row_idx(self, idx: int) -&gt; Optional[str]:\n        \"\"\"Converts a row index to the corresponding class.\n\n        Args:\n            idx: Row index in the circuit connectivity matrix.\n\n        Returns:\n            The class associated with the index. Can be \"excitatory\",\n            \"inhibitory\", or \"hybrid\".\n        \"\"\"\n        source = self._source_from_row_idx(idx)\n        if source == \"input\":\n            return self.in_class\n        elif source == \"feedback\":\n            return self.feedback_class\n        else:\n            return self.neuron_type_class[idx - 1 - int(self.use_feedback)]  # type: ignore\n\n    def _destination_from_col_idx(self, idx: int) -&gt; Optional[str]:\n        \"\"\"Converts a column index to the corresponding destination.\n\n        Args:\n            idx: Column index in the circuit connectivity matrix.\n\n        Returns:\n            The destination associated with the index. Can be \"cell\" or\n            \"output\".\n        \"\"\"\n        if idx &lt; self.num_cols_connectivity - 1:\n            return \"cell\"\n        else:\n            return \"output\"\n\n    def _clamp_tau(self) -&gt; None:\n        for tau in self.tau:\n            tau.data = torch.clamp(tau, min=1.0)\n\n    def init_neuron_state(\n        self,\n        batch_size: int,\n        init_fn: Optional[Union[str, TensorInitFnType]] = None,\n        device: Optional[Union[torch.device, str]] = None,\n    ) -&gt; list[torch.Tensor]:\n        \"\"\"initializers the neuron hidden states.\n\n        Args:\n            batch_size: Batch size.\n            init_fn: Initialization mode. Must be 'zeros', 'ones', 'randn', 'rand',\n                a function, or None. If None, the default initialization mode will be used.\n                If a function, it must take a variable number of positional arguments\n                corresponding to the shape of the tensor to initialize, as well\n                as a `device` keyword argument that sends the device to allocate\n                the tensor on.\n            device: Device to allocate the hidden states on.\n\n        Returns:\n            A list containing the initialized neuron hidden states.\n        \"\"\"\n\n        init_fn_corrected = (\n            init_fn\n            if init_fn is not None\n            else self.default_neuron_state_init_fn\n        )\n\n        return [\n            init_tensor(\n                init_fn_corrected,\n                batch_size,\n                self.num_neuron_subtypes[i],\n                *self.in_size,\n                device=device,\n            )\n            for i in range(self.num_neuron_types)\n        ]\n\n    def init_output_state(\n        self,\n        batch_size: int,\n        init_fn: Optional[Union[str, TensorInitFnType]] = None,\n        device: Optional[Union[torch.device, str]] = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"Initializes the output.\n\n        Args:\n            batch_size: Batch size.\n            init_fn: Initialization function. Must be 'zeros', 'ones', 'randn', 'rand',\n                a function, or None. If None, the default initialization mode will be used.\n            device: Device to allocate the hidden states on.\n\n        Returns:\n            The initialized output.\n        \"\"\"\n\n        init_fn_corrected = (\n            init_fn\n            if init_fn is not None\n            else self.default_output_state_init_fn\n        )\n\n        return init_tensor(\n            init_fn_corrected,\n            batch_size,\n            self.out_channels,\n            *self.in_size,\n            device=device,\n        )\n\n    def init_feedback_state(\n        self,\n        batch_size: int,\n        init_fn: Optional[Union[str, TensorInitFnType]] = None,\n        device: Optional[Union[torch.device, str]] = None,\n    ) -&gt; Union[torch.Tensor, None]:\n        \"\"\"Initializes the feedback input.\n\n        Args:\n            batch_size: Batch size.\n            init_fn: Initialization function. Must be 'zeros', 'ones', 'randn', 'rand',\n                a function, or None. If None, the default initialization mode will be used.\n            device: Device to allocate the hidden states on.\n\n        Returns:\n            The initialized feedback input if `use_feedback` is True, otherwise None.\n        \"\"\"\n\n        if not self.use_feedback:\n            return None\n\n        init_fn_corrected = (\n            init_fn\n            if init_fn is not None\n            else self.default_feedback_state_init_fn\n        )\n\n        return init_tensor(\n            init_fn_corrected,\n            batch_size,\n            self.feedback_channels,\n            *self.in_size,\n            device=device,\n        )\n\n    def neuron_description_df(self) -&gt; pd.DataFrame:\n        \"\"\"Creates a DataFrame representing the neuron types.\n\n        Returns:\n            DataFrame with columns for neuron type, spatial mode, and number of channels.\n        \"\"\"\n\n        df_columns = defaultdict(list)\n        for i in range(self.num_neuron_types):\n            df_columns[\"type\"].append(self.neuron_type_class[i])\n            df_columns[\"spatial_mode\"].append(self.neuron_type_density[i])\n            df_columns[\"channels\"].append(self.num_neuron_subtypes[i])\n\n        df = pd.DataFrame(df_columns)\n\n        return df\n\n    def conv_connectivity_df(self) -&gt; pd.DataFrame:\n        \"\"\"Creates a DataFrame representing connectivity between neural populations.\n\n        Returns:\n            DataFrame with rows representing source populations (\"from\") and\n            columns representing target populations (\"to\").\n        \"\"\"\n        row_labels = (\n            [\"input\"]\n            + ([\"feedback\"] if self.use_feedback else [])\n            + [f\"neuron_{i}\" for i in range(self.num_neuron_types)]\n        )\n        column_labels = [\n            f\"neuron_{i}\" for i in range(self.num_neuron_types)\n        ] + [\"output\"]\n\n        assert len(row_labels) == self.num_rows_connectivity\n        assert len(column_labels) == self.num_cols_connectivity\n\n        array = np.empty((len(row_labels), len(column_labels)), dtype=object)\n        for i in range(self.num_rows_connectivity):\n            for j in range(self.num_cols_connectivity):\n                if self.inter_neuron_type_connectivity[i, j]:\n                    content = []\n                    content.append(\n                        \"b\" if self.inter_neuron_type_bias[i, j] else \"_\"\n                    )\n                    content.append(\n                        self.inter_neuron_type_nonlinearity[i, j][:2]\n                        if self.inter_neuron_type_nonlinearity[i, j]\n                        else \"_\"\n                    )\n                    array[i, j] = \",\".join(content)\n                else:\n                    array[i, j] = \"\"\n\n        df = pd.DataFrame(\n            array.tolist(), index=row_labels, columns=column_labels\n        )\n        df.index.name = \"from\"\n        df.columns.name = \"to\"\n\n        return df\n\n    def summary(self) -&gt; str:\n        \"\"\"Returns a string representation of the SpatiallyEmbeddedArea.\n\n        Returns:\n            String representation of the SpatiallyEmbeddedArea.\n        \"\"\"\n        repr_str = \"SpatiallyEmbeddedArea:\\n\"\n        repr_str += \"=\" * 80 + \"\\n\"\n        repr_str += \"Connectivity:\\n\"\n        repr_str += self.conv_connectivity_df().to_string()\n        repr_str += \"-\" * 80 + \"\\n\"\n        repr_str += \"Neuron Description:\\n\"\n        repr_str += self.neuron_description_df().to_string()\n        repr_str += \"=\" * 80 + \"\\n\"\n        return repr_str\n\n    def forward(\n        self,\n        input: torch.Tensor,\n        neuron_state: Union[torch.Tensor, list[torch.Tensor]],\n        feedback_state: Optional[torch.Tensor] = None,\n    ) -&gt; tuple[torch.Tensor, Union[torch.Tensor, list[torch.Tensor]]]:\n        \"\"\"Forward pass of the SpatiallyEmbeddedArea.\n\n        Args:\n            input: Input tensor of shape (batch_size, in_channels, in_size[0], in_size[1]).\n            h_neuron: List of neuron hidden states of shape (batch_size, neuron_channels[i],\n                in_size[0], in_size[1]) for each neuron type i.\n            feedback_state: Feedback input of shape (batch_size, feedback_channels,\n                in_size[0], in_size[1]).\n\n        Returns:\n            A tuple containing the output and new neuron hidden state.\n        \"\"\"\n\n        # Expand h_neuron to match the number of neuron channels\n        if isinstance(neuron_state, torch.Tensor):\n            if self.num_neuron_types != 1:\n                raise ValueError(\n                    \"neuron_state must be a list of tensors if num_neuron_types is not 1.\"\n                )\n            neuron_state = [neuron_state]\n\n        # Check if feedback is provided if necessary\n        if self.use_feedback == (feedback_state is None):\n            raise ValueError(\n                \"use_feedback must be True if and only if feedback_state is provided.\"\n            )\n\n        # Compute convolutions for each connection in the circuit\n        circuit_ins = (\n            [input]\n            + ([feedback_state] if self.use_feedback else [])\n            + neuron_state\n        )\n        circuit_outs = [[] for _ in range(self.num_neuron_types)]\n        for key, conv in self.convs.items():\n            i, j = key.split(\"-&gt;\")\n            i, j = int(i), int(j)\n            if self._class_from_row_idx(i) == \"inhibitory\":\n                sign = -1\n            else:\n                sign = 1\n\n            circuit_outs[j].append(sign * conv(circuit_ins[i]))\n\n        # Update neuron states\n        self._clamp_tau()\n        neuron_state_new = []\n        for i in range(self.num_neuron_types):\n            # Aggregate all circuit outputs to this neuron type\n            state_new = torch.stack(circuit_outs[i], dim=0).sum(dim=0)\n            state_new = self.neuron_type_nonlinearity[i](state_new)\n\n            # Euler update\n            state_new = (\n                state_new / self.tau[i]\n                + (1 - 1 / self.tau[i]) * neuron_state[i]\n            )\n            neuron_state_new.append(state_new)\n\n        # Compute output\n        out = []\n        for key, conv in self.out_convs.items():\n            i, j = key.split(\"-&gt;\")\n            i = int(i)\n\n            if self._class_from_row_idx(i) == \"inhibitory\":\n                sign = -1\n            else:\n                sign = 1\n\n            source = self._source_from_row_idx(i)\n            if source == \"cell\":\n                i = i - 1 - int(self.use_feedback)\n                out.append(sign * conv(neuron_state_new[i]))\n            else:\n                warnings.warn(\n                    f\"Connection from {source} to output is not \"\n                    \"recommended. Consider changing inter_neuron_type_connectivity \"\n                    \"to remove this connection.\"\n                )\n                out.append(sign * conv(circuit_ins[i]))\n\n        out = torch.stack(out, dim=0).sum(dim=0)\n\n        return out, neuron_state_new\n</code></pre>"},{"location":"reference/eirnn/#src.bioplnn.models.spatially_embedded.SpatiallyEmbeddedArea.__init__","title":"<code>__init__(config=None, **kwargs)</code>","text":"<p>Initialize the SpatiallyEmbeddedArea.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[SpatiallyEmbeddedAreaConfig]</code> <p>Configuration object that specifies the area architecture and parameters. See SpatiallyEmbeddedAreaConfig for details. If None, parameters must be provided as keyword arguments.</p> <code>None</code> <code>**kwargs</code> <p>Keyword arguments to instantiate the configuration if <code>config</code> is not provided. Cannot provide both <code>config</code> and keyword arguments.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an invalid configuration is provided.</p> Source code in <code>src/bioplnn/models/spatially_embedded.py</code> <pre><code>def __init__(\n    self, config: Optional[SpatiallyEmbeddedAreaConfig] = None, **kwargs\n):\n    \"\"\"Initialize the SpatiallyEmbeddedArea.\n\n    Args:\n        config: Configuration object that specifies the area architecture and parameters.\n            See SpatiallyEmbeddedAreaConfig for details. If None, parameters must be\n            provided as keyword arguments.\n        **kwargs: Keyword arguments to instantiate the configuration if\n            `config` is not provided. Cannot provide both `config` and\n            keyword arguments.\n\n    Raises:\n        ValueError: If an invalid configuration is provided.\n    \"\"\"\n\n    super().__init__()\n\n    if config is None:\n        config = SpatiallyEmbeddedAreaConfig(**kwargs)\n    elif kwargs:\n        raise ValueError(\n            \"Cannot provide both config and keyword arguments. Please provide \"\n            \"only one of the two.\"\n        )\n\n    #####################################################################\n    # Input, output, and feedback parameters\n    #####################################################################\n\n    self.in_size = config.in_size\n    self.in_channels = config.in_channels\n    self.out_channels = config.out_channels\n    self.feedback_channels = (\n        config.feedback_channels\n        if config.feedback_channels is not None\n        else 0\n    )\n    self.use_feedback = self.feedback_channels &gt; 0\n\n    self.in_class = config.in_class\n    check_possible_values(\n        \"in_class\",\n        (self.in_class,),\n        (\"excitatory\", \"inhibitory\", \"hybrid\"),\n    )\n    self.feedback_class = config.feedback_class\n    check_possible_values(\n        \"feedback_class\",\n        (self.feedback_class,),\n        (\"excitatory\", \"inhibitory\", \"hybrid\"),\n    )\n\n    self.out_nonlinearity = get_activation(config.out_nonlinearity)\n\n    #####################################################################\n    # Neuron type parameters\n    #####################################################################\n\n    self.num_neuron_types = config.num_neuron_types\n\n    # Format neuron type\n    self.num_neuron_subtypes = expand_list(\n        config.num_neuron_subtypes, self.num_neuron_types\n    )\n    self.neuron_type_class = expand_list(\n        config.neuron_type_class, self.num_neuron_types\n    )\n    check_possible_values(\n        \"neuron_type_class\",\n        self.neuron_type_class,\n        (\"excitatory\", \"inhibitory\", \"hybrid\"),\n    )\n    self.neuron_type_density = expand_list(\n        config.neuron_type_density, self.num_neuron_types\n    )\n    # TODO: Add support for quarter\n    check_possible_values(\n        \"neuron_type_density\",\n        self.neuron_type_density,\n        (\"same\", \"half\"),\n    )\n    neuron_type_nonlinearity = expand_list(\n        config.neuron_type_nonlinearity, self.num_neuron_types\n    )\n    self.neuron_type_nonlinearity = nn.ModuleList(\n        [\n            get_activation(nonlinearity)\n            for nonlinearity in neuron_type_nonlinearity\n        ]\n    )\n\n    # Save number of \"types\" for the input to and output from the area\n    self.num_rows_connectivity = (\n        1 + int(self.use_feedback) + self.num_neuron_types\n    )  # input + feedback + neurons\n    self.num_cols_connectivity = (\n        self.num_neuron_types + 1\n    )  # neurons + output\n\n    # Calculate half spatial size\n    self.half_size = (\n        ceil(self.in_size[0] / 2),\n        ceil(self.in_size[1] / 2),\n    )\n\n    self.neuron_type_size = [\n        self.in_size\n        if self.neuron_type_density[i] == \"same\"\n        else self.half_size\n        for i in range(self.num_neuron_types)\n    ]\n\n    #####################################################################\n    # Circuit motif connectivity\n    #####################################################################\n\n    # Format circuit connectivity\n    self.inter_neuron_type_connectivity = np.array(\n        config.inter_neuron_type_connectivity\n    )\n    if self.inter_neuron_type_connectivity.shape != (\n        self.num_rows_connectivity,\n        self.num_cols_connectivity,\n    ):\n        raise ValueError(\n            \"The shape of inter_neuron_type_connectivity must match the number of \"\n            \"rows and columns in the connectivity matrix.\"\n        )\n\n    # Format connectivity variables to match circuit connectivity\n    self.inter_neuron_type_spatial_extents = expand_array_2d(\n        config.inter_neuron_type_spatial_extents,\n        self.inter_neuron_type_connectivity.shape[0],\n        self.inter_neuron_type_connectivity.shape[1],\n        depth=1,\n    )\n    self.inter_neuron_type_num_subtype_groups = expand_array_2d(\n        config.inter_neuron_type_num_subtype_groups,\n        self.inter_neuron_type_connectivity.shape[0],\n        self.inter_neuron_type_connectivity.shape[1],\n    )\n    self.inter_neuron_type_nonlinearity = expand_array_2d(\n        config.inter_neuron_type_nonlinearity,\n        self.inter_neuron_type_connectivity.shape[0],\n        self.inter_neuron_type_connectivity.shape[1],\n    )\n    self.inter_neuron_type_bias = expand_array_2d(\n        config.inter_neuron_type_bias,\n        self.inter_neuron_type_connectivity.shape[0],\n        self.inter_neuron_type_connectivity.shape[1],\n    )\n\n    #####################################################################\n    # Circuit motif convolutions\n    # Here, we represent the circuit connectivity between neuron classes\n    # as an array of convolutions (implemented as a dictionary for\n    # efficiency). The convolution self.convs[f\"{i}-&gt;{j}\"] corresponds\n    # to the connection from neuron class i to neuron class j.\n    #####################################################################\n\n    self.convs = nn.ModuleDict()\n    self.out_convs = nn.ModuleDict()\n    for i, row in enumerate(self.inter_neuron_type_connectivity):\n        # Handle input neuron channel and spatial mode based on neuron type\n        conv_in_type = self._source_from_row_idx(i)\n        if conv_in_type == \"input\":\n            conv_in_channels = self.in_channels\n            conv_in_density = \"same\"\n        elif conv_in_type == \"feedback\":\n            conv_in_channels = self.feedback_channels\n            conv_in_density = \"same\"\n        else:\n            assert conv_in_type == \"cell\"\n            conv_in_channels = self.num_neuron_subtypes[\n                i - 1 - int(self.use_feedback)\n            ]\n            conv_in_density = self.neuron_type_density[\n                i - 1 - int(self.use_feedback)\n            ]\n\n        conv_in_class = self._class_from_row_idx(i)\n        if conv_in_class in (\"excitatory\", \"inhibitory\"):\n            Conv2d = Conv2dRectify\n        else:\n            assert conv_in_class == \"hybrid\"\n            Conv2d = nn.Conv2d\n\n        # Handle output neurons\n        # TODO: Optimize using smart grouping and convolution sharing\n        to_indices = np.nonzero(row)[0]\n        for j in to_indices:\n            if self.inter_neuron_type_connectivity[i, j]:\n                # Handle output neuron channel and spatial mode based on neuron type\n                conv_out_type = self._destination_from_col_idx(j)\n                if conv_out_type == \"cell\":\n                    conv_out_channels: int = self.num_neuron_subtypes[j]  # type: ignore\n                    conv_out_density: str = self.neuron_type_density[j]  # type: ignore\n                else:\n                    assert conv_out_type == \"output\"\n                    if conv_in_type in (\"input\", \"feedback\"):\n                        warnings.warn(\n                            \"Input or feedback is connected to output. \"\n                            \"This is typically undesired as the signal \"\n                            \"will bypass the neuron types and go directly \"\n                            \"to the output. Consider removing this \"\n                            \"connection in the inter_neuron_type_connectivity \"\n                            \"matrix.\"\n                        )\n                    conv_out_channels = self.out_channels\n                    conv_out_density = \"same\"\n\n                # Handle stride upsampling if necessary\n                conv = nn.Sequential()\n                conv_stride = 1\n                if (\n                    conv_in_density == \"half\"\n                    and conv_out_density == \"same\"\n                ):\n                    conv_stride = 2\n                elif (\n                    conv_in_density == \"same\"\n                    and conv_out_density == \"half\"\n                ):\n                    conv.append(\n                        nn.Upsample(size=self.in_size, mode=\"bilinear\")\n                    )\n\n                # Handle upsampling if necessary\n                conv.append(\n                    Conv2d(\n                        in_channels=conv_in_channels,\n                        out_channels=conv_out_channels,\n                        kernel_size=self.inter_neuron_type_spatial_extents[\n                            i, j\n                        ],\n                        stride=conv_stride,\n                        padding=(\n                            self.inter_neuron_type_spatial_extents[i, j][0]\n                            // 2,\n                            self.inter_neuron_type_spatial_extents[i, j][1]\n                            // 2,\n                        ),\n                        groups=self.inter_neuron_type_num_subtype_groups[\n                            i, j\n                        ],\n                        bias=self.inter_neuron_type_bias[i, j],\n                    )\n                )\n                conv.append(\n                    get_activation(\n                        self.inter_neuron_type_nonlinearity[i, j]\n                    )\n                )\n                if conv_out_type == \"output\":\n                    self.out_convs[f\"{i}-&gt;out\"] = conv\n                else:\n                    self.convs[f\"{i}-&gt;{j}\"] = conv\n\n    #####################################################################\n    # Post convolution operations\n    #####################################################################\n\n    # Initialize membrane time constants\n    self.tau_mode = expand_list(config.tau_mode, self.num_neuron_types)\n    check_possible_values(\n        \"tau_mode\",\n        self.tau_mode,\n        (\"subtype\", \"spatial\", \"subtype_spatial\", \"type\"),\n    )\n    self.tau_init_fn = expand_list(\n        config.tau_init_fn, self.num_neuron_types\n    )\n\n    self.tau = nn.ParameterList()\n    for i in range(self.num_neuron_types):\n        if self.tau_mode[i] == \"spatial\":\n            tau_channels = 1\n            tau_size = self.neuron_type_size[i]\n        elif self.tau_mode[i] == \"subtype\":\n            tau_channels = self.num_neuron_subtypes[i]\n            tau_size = (1, 1)\n        elif self.tau_mode[i] == \"subtype_spatial\":\n            tau_channels = self.num_neuron_subtypes[i]\n            tau_size = self.neuron_type_size[i]\n        else:\n            assert self.tau_mode[i] == \"type\"\n            tau_channels = 1\n            tau_size = (1, 1)\n\n        tau = init_tensor(\n            self.tau_init_fn[i],\n            1,\n            tau_channels,\n            *tau_size,\n        )\n        noise = torch.rand_like(tau) * 1e-6\n\n        self.tau.append(\n            nn.Parameter(\n                tau + noise,\n                requires_grad=True,\n            )\n        )\n\n    #####################################################################\n    # Tensor initialization\n    #####################################################################\n\n    self.default_neuron_state_init_fn = config.default_neuron_state_init_fn\n    self.default_feedback_state_init_fn = (\n        config.default_feedback_state_init_fn\n    )\n    self.default_output_state_init_fn = config.default_output_state_init_fn\n</code></pre>"},{"location":"reference/eirnn/#src.bioplnn.models.spatially_embedded.SpatiallyEmbeddedArea.conv_connectivity_df","title":"<code>conv_connectivity_df()</code>","text":"<p>Creates a DataFrame representing connectivity between neural populations.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with rows representing source populations (\"from\") and</p> <code>DataFrame</code> <p>columns representing target populations (\"to\").</p> Source code in <code>src/bioplnn/models/spatially_embedded.py</code> <pre><code>def conv_connectivity_df(self) -&gt; pd.DataFrame:\n    \"\"\"Creates a DataFrame representing connectivity between neural populations.\n\n    Returns:\n        DataFrame with rows representing source populations (\"from\") and\n        columns representing target populations (\"to\").\n    \"\"\"\n    row_labels = (\n        [\"input\"]\n        + ([\"feedback\"] if self.use_feedback else [])\n        + [f\"neuron_{i}\" for i in range(self.num_neuron_types)]\n    )\n    column_labels = [\n        f\"neuron_{i}\" for i in range(self.num_neuron_types)\n    ] + [\"output\"]\n\n    assert len(row_labels) == self.num_rows_connectivity\n    assert len(column_labels) == self.num_cols_connectivity\n\n    array = np.empty((len(row_labels), len(column_labels)), dtype=object)\n    for i in range(self.num_rows_connectivity):\n        for j in range(self.num_cols_connectivity):\n            if self.inter_neuron_type_connectivity[i, j]:\n                content = []\n                content.append(\n                    \"b\" if self.inter_neuron_type_bias[i, j] else \"_\"\n                )\n                content.append(\n                    self.inter_neuron_type_nonlinearity[i, j][:2]\n                    if self.inter_neuron_type_nonlinearity[i, j]\n                    else \"_\"\n                )\n                array[i, j] = \",\".join(content)\n            else:\n                array[i, j] = \"\"\n\n    df = pd.DataFrame(\n        array.tolist(), index=row_labels, columns=column_labels\n    )\n    df.index.name = \"from\"\n    df.columns.name = \"to\"\n\n    return df\n</code></pre>"},{"location":"reference/eirnn/#src.bioplnn.models.spatially_embedded.SpatiallyEmbeddedArea.forward","title":"<code>forward(input, neuron_state, feedback_state=None)</code>","text":"<p>Forward pass of the SpatiallyEmbeddedArea.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, in_channels, in_size[0], in_size[1]).</p> required <code>h_neuron</code> <p>List of neuron hidden states of shape (batch_size, neuron_channels[i], in_size[0], in_size[1]) for each neuron type i.</p> required <code>feedback_state</code> <code>Optional[Tensor]</code> <p>Feedback input of shape (batch_size, feedback_channels, in_size[0], in_size[1]).</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Tensor, Union[Tensor, list[Tensor]]]</code> <p>A tuple containing the output and new neuron hidden state.</p> Source code in <code>src/bioplnn/models/spatially_embedded.py</code> <pre><code>def forward(\n    self,\n    input: torch.Tensor,\n    neuron_state: Union[torch.Tensor, list[torch.Tensor]],\n    feedback_state: Optional[torch.Tensor] = None,\n) -&gt; tuple[torch.Tensor, Union[torch.Tensor, list[torch.Tensor]]]:\n    \"\"\"Forward pass of the SpatiallyEmbeddedArea.\n\n    Args:\n        input: Input tensor of shape (batch_size, in_channels, in_size[0], in_size[1]).\n        h_neuron: List of neuron hidden states of shape (batch_size, neuron_channels[i],\n            in_size[0], in_size[1]) for each neuron type i.\n        feedback_state: Feedback input of shape (batch_size, feedback_channels,\n            in_size[0], in_size[1]).\n\n    Returns:\n        A tuple containing the output and new neuron hidden state.\n    \"\"\"\n\n    # Expand h_neuron to match the number of neuron channels\n    if isinstance(neuron_state, torch.Tensor):\n        if self.num_neuron_types != 1:\n            raise ValueError(\n                \"neuron_state must be a list of tensors if num_neuron_types is not 1.\"\n            )\n        neuron_state = [neuron_state]\n\n    # Check if feedback is provided if necessary\n    if self.use_feedback == (feedback_state is None):\n        raise ValueError(\n            \"use_feedback must be True if and only if feedback_state is provided.\"\n        )\n\n    # Compute convolutions for each connection in the circuit\n    circuit_ins = (\n        [input]\n        + ([feedback_state] if self.use_feedback else [])\n        + neuron_state\n    )\n    circuit_outs = [[] for _ in range(self.num_neuron_types)]\n    for key, conv in self.convs.items():\n        i, j = key.split(\"-&gt;\")\n        i, j = int(i), int(j)\n        if self._class_from_row_idx(i) == \"inhibitory\":\n            sign = -1\n        else:\n            sign = 1\n\n        circuit_outs[j].append(sign * conv(circuit_ins[i]))\n\n    # Update neuron states\n    self._clamp_tau()\n    neuron_state_new = []\n    for i in range(self.num_neuron_types):\n        # Aggregate all circuit outputs to this neuron type\n        state_new = torch.stack(circuit_outs[i], dim=0).sum(dim=0)\n        state_new = self.neuron_type_nonlinearity[i](state_new)\n\n        # Euler update\n        state_new = (\n            state_new / self.tau[i]\n            + (1 - 1 / self.tau[i]) * neuron_state[i]\n        )\n        neuron_state_new.append(state_new)\n\n    # Compute output\n    out = []\n    for key, conv in self.out_convs.items():\n        i, j = key.split(\"-&gt;\")\n        i = int(i)\n\n        if self._class_from_row_idx(i) == \"inhibitory\":\n            sign = -1\n        else:\n            sign = 1\n\n        source = self._source_from_row_idx(i)\n        if source == \"cell\":\n            i = i - 1 - int(self.use_feedback)\n            out.append(sign * conv(neuron_state_new[i]))\n        else:\n            warnings.warn(\n                f\"Connection from {source} to output is not \"\n                \"recommended. Consider changing inter_neuron_type_connectivity \"\n                \"to remove this connection.\"\n            )\n            out.append(sign * conv(circuit_ins[i]))\n\n    out = torch.stack(out, dim=0).sum(dim=0)\n\n    return out, neuron_state_new\n</code></pre>"},{"location":"reference/eirnn/#src.bioplnn.models.spatially_embedded.SpatiallyEmbeddedArea.init_feedback_state","title":"<code>init_feedback_state(batch_size, init_fn=None, device=None)</code>","text":"<p>Initializes the feedback input.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size.</p> required <code>init_fn</code> <code>Optional[Union[str, TensorInitFnType]]</code> <p>Initialization function. Must be 'zeros', 'ones', 'randn', 'rand', a function, or None. If None, the default initialization mode will be used.</p> <code>None</code> <code>device</code> <code>Optional[Union[device, str]]</code> <p>Device to allocate the hidden states on.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Tensor, None]</code> <p>The initialized feedback input if <code>use_feedback</code> is True, otherwise None.</p> Source code in <code>src/bioplnn/models/spatially_embedded.py</code> <pre><code>def init_feedback_state(\n    self,\n    batch_size: int,\n    init_fn: Optional[Union[str, TensorInitFnType]] = None,\n    device: Optional[Union[torch.device, str]] = None,\n) -&gt; Union[torch.Tensor, None]:\n    \"\"\"Initializes the feedback input.\n\n    Args:\n        batch_size: Batch size.\n        init_fn: Initialization function. Must be 'zeros', 'ones', 'randn', 'rand',\n            a function, or None. If None, the default initialization mode will be used.\n        device: Device to allocate the hidden states on.\n\n    Returns:\n        The initialized feedback input if `use_feedback` is True, otherwise None.\n    \"\"\"\n\n    if not self.use_feedback:\n        return None\n\n    init_fn_corrected = (\n        init_fn\n        if init_fn is not None\n        else self.default_feedback_state_init_fn\n    )\n\n    return init_tensor(\n        init_fn_corrected,\n        batch_size,\n        self.feedback_channels,\n        *self.in_size,\n        device=device,\n    )\n</code></pre>"},{"location":"reference/eirnn/#src.bioplnn.models.spatially_embedded.SpatiallyEmbeddedArea.init_neuron_state","title":"<code>init_neuron_state(batch_size, init_fn=None, device=None)</code>","text":"<p>initializers the neuron hidden states.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size.</p> required <code>init_fn</code> <code>Optional[Union[str, TensorInitFnType]]</code> <p>Initialization mode. Must be 'zeros', 'ones', 'randn', 'rand', a function, or None. If None, the default initialization mode will be used. If a function, it must take a variable number of positional arguments corresponding to the shape of the tensor to initialize, as well as a <code>device</code> keyword argument that sends the device to allocate the tensor on.</p> <code>None</code> <code>device</code> <code>Optional[Union[device, str]]</code> <p>Device to allocate the hidden states on.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Tensor]</code> <p>A list containing the initialized neuron hidden states.</p> Source code in <code>src/bioplnn/models/spatially_embedded.py</code> <pre><code>def init_neuron_state(\n    self,\n    batch_size: int,\n    init_fn: Optional[Union[str, TensorInitFnType]] = None,\n    device: Optional[Union[torch.device, str]] = None,\n) -&gt; list[torch.Tensor]:\n    \"\"\"initializers the neuron hidden states.\n\n    Args:\n        batch_size: Batch size.\n        init_fn: Initialization mode. Must be 'zeros', 'ones', 'randn', 'rand',\n            a function, or None. If None, the default initialization mode will be used.\n            If a function, it must take a variable number of positional arguments\n            corresponding to the shape of the tensor to initialize, as well\n            as a `device` keyword argument that sends the device to allocate\n            the tensor on.\n        device: Device to allocate the hidden states on.\n\n    Returns:\n        A list containing the initialized neuron hidden states.\n    \"\"\"\n\n    init_fn_corrected = (\n        init_fn\n        if init_fn is not None\n        else self.default_neuron_state_init_fn\n    )\n\n    return [\n        init_tensor(\n            init_fn_corrected,\n            batch_size,\n            self.num_neuron_subtypes[i],\n            *self.in_size,\n            device=device,\n        )\n        for i in range(self.num_neuron_types)\n    ]\n</code></pre>"},{"location":"reference/eirnn/#src.bioplnn.models.spatially_embedded.SpatiallyEmbeddedArea.init_output_state","title":"<code>init_output_state(batch_size, init_fn=None, device=None)</code>","text":"<p>Initializes the output.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size.</p> required <code>init_fn</code> <code>Optional[Union[str, TensorInitFnType]]</code> <p>Initialization function. Must be 'zeros', 'ones', 'randn', 'rand', a function, or None. If None, the default initialization mode will be used.</p> <code>None</code> <code>device</code> <code>Optional[Union[device, str]]</code> <p>Device to allocate the hidden states on.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The initialized output.</p> Source code in <code>src/bioplnn/models/spatially_embedded.py</code> <pre><code>def init_output_state(\n    self,\n    batch_size: int,\n    init_fn: Optional[Union[str, TensorInitFnType]] = None,\n    device: Optional[Union[torch.device, str]] = None,\n) -&gt; torch.Tensor:\n    \"\"\"Initializes the output.\n\n    Args:\n        batch_size: Batch size.\n        init_fn: Initialization function. Must be 'zeros', 'ones', 'randn', 'rand',\n            a function, or None. If None, the default initialization mode will be used.\n        device: Device to allocate the hidden states on.\n\n    Returns:\n        The initialized output.\n    \"\"\"\n\n    init_fn_corrected = (\n        init_fn\n        if init_fn is not None\n        else self.default_output_state_init_fn\n    )\n\n    return init_tensor(\n        init_fn_corrected,\n        batch_size,\n        self.out_channels,\n        *self.in_size,\n        device=device,\n    )\n</code></pre>"},{"location":"reference/eirnn/#src.bioplnn.models.spatially_embedded.SpatiallyEmbeddedArea.neuron_description_df","title":"<code>neuron_description_df()</code>","text":"<p>Creates a DataFrame representing the neuron types.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with columns for neuron type, spatial mode, and number of channels.</p> Source code in <code>src/bioplnn/models/spatially_embedded.py</code> <pre><code>def neuron_description_df(self) -&gt; pd.DataFrame:\n    \"\"\"Creates a DataFrame representing the neuron types.\n\n    Returns:\n        DataFrame with columns for neuron type, spatial mode, and number of channels.\n    \"\"\"\n\n    df_columns = defaultdict(list)\n    for i in range(self.num_neuron_types):\n        df_columns[\"type\"].append(self.neuron_type_class[i])\n        df_columns[\"spatial_mode\"].append(self.neuron_type_density[i])\n        df_columns[\"channels\"].append(self.num_neuron_subtypes[i])\n\n    df = pd.DataFrame(df_columns)\n\n    return df\n</code></pre>"},{"location":"reference/eirnn/#src.bioplnn.models.spatially_embedded.SpatiallyEmbeddedArea.summary","title":"<code>summary()</code>","text":"<p>Returns a string representation of the SpatiallyEmbeddedArea.</p> <p>Returns:</p> Type Description <code>str</code> <p>String representation of the SpatiallyEmbeddedArea.</p> Source code in <code>src/bioplnn/models/spatially_embedded.py</code> <pre><code>def summary(self) -&gt; str:\n    \"\"\"Returns a string representation of the SpatiallyEmbeddedArea.\n\n    Returns:\n        String representation of the SpatiallyEmbeddedArea.\n    \"\"\"\n    repr_str = \"SpatiallyEmbeddedArea:\\n\"\n    repr_str += \"=\" * 80 + \"\\n\"\n    repr_str += \"Connectivity:\\n\"\n    repr_str += self.conv_connectivity_df().to_string()\n    repr_str += \"-\" * 80 + \"\\n\"\n    repr_str += \"Neuron Description:\\n\"\n    repr_str += self.neuron_description_df().to_string()\n    repr_str += \"=\" * 80 + \"\\n\"\n    return repr_str\n</code></pre>"},{"location":"reference/eirnn/#src.bioplnn.models.spatially_embedded.SpatiallyEmbeddedAreaConfig","title":"<code>SpatiallyEmbeddedAreaConfig</code>  <code>dataclass</code>","text":"<p>Configuration for <code>SpatiallyEmbeddedArea</code>.</p> <p>This class defines the configuration for a spatially embedded area. It specifies the size of the input data, the number of input and output channels, the connectivity matrix for the circuit motif, and the parameters for the neuron types.</p> <p>The default configuration corresponds to a spatially embedded area with one excitatory neuron type which is stimulated by the input and by itself (lateral connections).</p> <p>Any of the parameters annotated with <code>ScalarOrListLike</code> can be either a single value that applies to all neuron types, or a list of values that apply to each neuron type.</p> <p>Any of the parameters annotated with <code>ScalarOrArray2dType</code> can be either a single value that applies to all connections in the circuit motif, or a 2D array that applies to each connection in the circuit motif.</p> <p>Attributes:</p> Name Type Description <code>in_size</code> <code>tuple[int, int]</code> <p>Spatial size of the input data (height, width). This size determines the spatial sizes of the feedback signal, the neuronal states, and the output.</p> <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> <code>out_channels</code> <code>int</code> <p>Number of output channels.</p> <code>feedback_channels</code> <code>Optional[int]</code> <p>Number of feedback channels. If provided, this area must receive feedback from another area.</p> <code>in_class</code> <code>str</code> <p>Class of input signal. Can be \"excitatory\", \"inhibitory\", or \"hybrid\".</p> <code>feedback_class</code> <code>str</code> <p>Class of feedback signal. Can be \"excitatory\", \"inhibitory\", or \"hybrid\".</p> <code>num_neuron_types</code> <code>int</code> <p>Number of neuron types.</p> <code>num_neuron_subtypes</code> <code>ScalarOrListLike[int]</code> <p>Number of subtypes for each neuron type.</p> <code>neuron_type_class</code> <code>ScalarOrListLike[str]</code> <p>Class of neuron type. Can be \"excitatory\", \"inhibitory\", or \"hybrid\".</p> <code>neuron_type_density</code> <code>ScalarOrListLike[str]</code> <p>Spatial density of each neuron type. Can be \"same\" or \"half\".</p> <code>neuron_type_nonlinearity</code> <code>ScalarOrListLike[Optional[Union[str, Module]]]</code> <p>Nonlinearity to apply to each neuron type's activity after adding the impact of all connected inputs/neuron types in the circuit motif.</p> <code>inter_neuron_type_connectivity</code> <code>Array2dType[Union[int, bool]]</code> <p>Connectivity matrix for the circuit motif. The shape should be (1 + int(use_feedback) + num_neuron_types, num_neuron_types + 1). Here, rows represent source types and columns represent destination types. A True entry in the matrix indicates a connection from the source to the destination. The first row corresponds to the input, the second row corresponds to the feedback (if feedback_channels &gt; 0), and the remaining rows correspond to the neuron types. The first num_neuron_types columns correspond to the neuron types and the last column corresponds to the output. To get a template of the connectivity matrix for your configuration with appropriate row and column labels, use the <code>inter_neuron_type_connectivity_template_df</code> method of this class.</p> <code>inter_neuron_type_spatial_extents</code> <code>ScalarOrArray2dType[tuple[int, int]]</code> <p>Spatial extent for each circuit motif connection. Same shape as <code>inter_neuron_type_connectivity</code>.</p> <code>inter_neuron_type_num_subtype_groups</code> <code>ScalarOrArray2dType[int]</code> <p>Number of subtype groups for each circuit motif connection. Same shape as <code>inter_neuron_type_connectivity</code>.</p> <code>inter_neuron_type_nonlinearity</code> <code>ScalarOrArray2dType[Optional[Union[str, Module]]]</code> <p>Nonlinearity for each circuit motif connection. Same shape as <code>inter_neuron_type_connectivity</code>.</p> <code>inter_neuron_type_bias</code> <code>ScalarOrArray2dType[bool]</code> <p>Whether to add a bias term for each circuit motif connection. Same shape as <code>inter_neuron_type_connectivity</code>.</p> <code>tau_mode</code> <code>ScalarOrListLike[str]</code> <p>Mode determining which parts of neuron activity share a time constant. Can be \"type\" (one tau for each neuron type), \"subtype\" (one tau per neuron subtype), \"spatial\" (one tau per spatial location), or \"subtype_spatial\" (one tau per neuron subtype and spatial location)</p> <code>tau_init_fn</code> <code>ScalarOrListLike[Union[str, TensorInitFnType]]</code> <p>Initialization mode for the membrane time constants.</p> <code>out_nonlinearity</code> <code>Optional[Union[str, Module]]</code> <p>Nonlinearity to apply to the output.</p> <code>default_neuron_state_init_fn</code> <code>Union[str, TensorInitFnType]</code> <p>Initialization mode for the hidden state.</p> <code>default_feedback_state_init_fn</code> <code>Union[str, TensorInitFnType]</code> <p>Initialization mode for the feedback state.</p> <code>default_output_state_init_fn</code> <code>Union[str, TensorInitFnType]</code> <p>Initialization mode for the output state.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; connectivity_df = SpatiallyEmbeddedAreaConfig.inter_neuron_type_connectivity_template_df(\n...     use_feedback=False,\n...     num_neuron_types=2,\n... )\n&gt;&gt;&gt; print(connectivity_df)\n                destination\nsource          neuron_0  neuron_1  output\ninput           False     False     False\nneuron_0        False     False     False\nneuron_1        False     False     False\n&gt;&gt;&gt; connectivity_df.loc[\"input\", \"neuron_0\"] = True\n&gt;&gt;&gt; connectivity_df.loc[\"neuron_0\", \"neuron_0\"] = True\n&gt;&gt;&gt; connectivity_df.loc[\"neuron_0\", \"neuron_1\"] = True\n&gt;&gt;&gt; connectivity_df.loc[\"neuron_1\", \"neuron_0\"] = True\n&gt;&gt;&gt; connectivity_df.loc[\"neuron_0\", \"output\"] = True\n&gt;&gt;&gt; config = SpatiallyEmbeddedAreaConfig(\n...     in_size=(32, 32),\n...     in_channels=3,\n...     out_channels=16,\n...     inter_neuron_type_connectivity=connectivity_df.to_numpy(),\n... )\n</code></pre> Source code in <code>src/bioplnn/models/spatially_embedded.py</code> <pre><code>@dataclass\nclass SpatiallyEmbeddedAreaConfig:\n    \"\"\"Configuration for `SpatiallyEmbeddedArea`.\n\n    This class defines the configuration for a spatially embedded area. It\n    specifies the size of the input data, the number of input and output\n    channels, the connectivity matrix for the circuit motif, and the parameters\n    for the neuron types.\n\n    The default configuration corresponds to a spatially embedded area with\n    one excitatory neuron type which is stimulated by the input and by itself\n    (lateral connections).\n\n    Any of the parameters annotated with `ScalarOrListLike` can be either a\n    single value that applies to all neuron types, or a list of values that\n    apply to each neuron type.\n\n    Any of the parameters annotated with `ScalarOrArray2dType` can be either a\n    single value that applies to all connections in the circuit motif, or a\n    2D array that applies to each connection in the circuit motif.\n\n    Attributes:\n        in_size: Spatial size of the input data (height, width).\n            This size determines the spatial sizes of the feedback signal, the\n            neuronal states, and the output.\n        in_channels: Number of input channels.\n        out_channels: Number of output channels.\n        feedback_channels: Number of feedback channels. If provided, this area\n            must receive feedback from another area.\n        in_class: Class of input signal. Can be\n            \"excitatory\", \"inhibitory\", or \"hybrid\".\n        feedback_class: Class of feedback signal. Can be\n            \"excitatory\", \"inhibitory\", or \"hybrid\".\n        num_neuron_types: Number of neuron types.\n        num_neuron_subtypes: Number of subtypes for each neuron type.\n        neuron_type_class: Class of neuron type. Can be\n            \"excitatory\", \"inhibitory\", or \"hybrid\".\n        neuron_type_density: Spatial density of each neuron type. Can be \"same\"\n            or \"half\".\n        neuron_type_nonlinearity: Nonlinearity to apply to each neuron type's\n            activity after adding the impact of all connected inputs/neuron\n            types in the circuit motif.\n        inter_neuron_type_connectivity: Connectivity matrix for the circuit motif.\n            The shape should be\n            (1 + int(use_feedback) + num_neuron_types, num_neuron_types + 1). Here,\n            rows represent source types and columns represent destination types.\n            A True entry in the matrix indicates a connection from the source to\n            the destination.\n            The first row corresponds to the input, the second row corresponds to\n            the feedback (if feedback_channels &gt; 0), and the remaining rows\n            correspond to the neuron types.\n            The first num_neuron_types columns correspond to the neuron types\n            and the last column corresponds to the output. To get a template of\n            the connectivity matrix for your configuration with appropriate row\n            and column labels, use the `inter_neuron_type_connectivity_template_df`\n            method of this class.\n        inter_neuron_type_spatial_extents: Spatial extent for each circuit\n            motif connection. Same shape as `inter_neuron_type_connectivity`.\n        inter_neuron_type_num_subtype_groups: Number of subtype groups for each\n            circuit motif connection. Same shape as `inter_neuron_type_connectivity`.\n        inter_neuron_type_nonlinearity: Nonlinearity for each circuit motif\n            connection. Same shape as `inter_neuron_type_connectivity`.\n        inter_neuron_type_bias: Whether to add a bias term for each circuit\n            motif connection. Same shape as `inter_neuron_type_connectivity`.\n        tau_mode: Mode determining which parts of\n            neuron activity share a time constant. Can be \"type\" (one tau for each neuron type),\n            \"subtype\" (one tau per neuron subtype), \"spatial\" (one tau per spatial location), or\n            \"subtype_spatial\" (one tau per neuron subtype and spatial location)\n        tau_init_fn: Initialization mode for the membrane time constants.\n        out_nonlinearity: Nonlinearity to apply to the output.\n        default_neuron_state_init_fn: Initialization mode for the hidden state.\n        default_feedback_state_init_fn: Initialization mode for the feedback state.\n        default_output_state_init_fn: Initialization mode for the output state.\n\n    Examples:\n        &gt;&gt;&gt; connectivity_df = SpatiallyEmbeddedAreaConfig.inter_neuron_type_connectivity_template_df(\n        ...     use_feedback=False,\n        ...     num_neuron_types=2,\n        ... )\n        &gt;&gt;&gt; print(connectivity_df)\n                        destination\n        source          neuron_0  neuron_1  output\n        input           False     False     False\n        neuron_0        False     False     False\n        neuron_1        False     False     False\n        &gt;&gt;&gt; connectivity_df.loc[\"input\", \"neuron_0\"] = True\n        &gt;&gt;&gt; connectivity_df.loc[\"neuron_0\", \"neuron_0\"] = True\n        &gt;&gt;&gt; connectivity_df.loc[\"neuron_0\", \"neuron_1\"] = True\n        &gt;&gt;&gt; connectivity_df.loc[\"neuron_1\", \"neuron_0\"] = True\n        &gt;&gt;&gt; connectivity_df.loc[\"neuron_0\", \"output\"] = True\n        &gt;&gt;&gt; config = SpatiallyEmbeddedAreaConfig(\n        ...     in_size=(32, 32),\n        ...     in_channels=3,\n        ...     out_channels=16,\n        ...     inter_neuron_type_connectivity=connectivity_df.to_numpy(),\n        ... )\n    \"\"\"\n\n    # Input, output, and feedback parameters\n    in_size: tuple[int, int]\n    in_channels: int\n    out_channels: int\n    feedback_channels: Optional[int] = None\n    in_class: str = \"hybrid\"\n    feedback_class: str = \"hybrid\"\n\n    # Neuron type parameters\n    num_neuron_types: int = 1\n    num_neuron_subtypes: ScalarOrListLike[int] = 16\n    neuron_type_class: ScalarOrListLike[str] = \"hybrid\"\n    neuron_type_density: ScalarOrListLike[str] = \"same\"\n    neuron_type_nonlinearity: ScalarOrListLike[\n        Optional[Union[str, nn.Module]]\n    ] = \"Sigmoid\"\n    tau_mode: ScalarOrListLike[str] = \"subtype\"\n    tau_init_fn: ScalarOrListLike[Union[str, TensorInitFnType]] = \"ones\"\n\n    # Circuit motif connectivity parameters\n    inter_neuron_type_connectivity: Array2dType[Union[int, bool]] = field(\n        default_factory=lambda: [[1, 0], [1, 1]]\n    )\n    inter_neuron_type_spatial_extents: ScalarOrArray2dType[tuple[int, int]] = (\n        3,\n        3,\n    )\n    inter_neuron_type_num_subtype_groups: ScalarOrArray2dType[int] = 1\n    inter_neuron_type_nonlinearity: ScalarOrArray2dType[\n        Optional[Union[str, nn.Module]]\n    ] = None\n    inter_neuron_type_bias: ScalarOrArray2dType[bool] = True\n    out_nonlinearity: Optional[Union[str, nn.Module]] = None\n    default_neuron_state_init_fn: Union[str, TensorInitFnType] = \"zeros\"\n    default_feedback_state_init_fn: Union[str, TensorInitFnType] = \"zeros\"\n    default_output_state_init_fn: Union[str, TensorInitFnType] = \"zeros\"\n\n    def asdict(self) -&gt; dict[str, Any]:\n        \"\"\"Converts the configuration object to a dictionary.\n\n        Returns:\n            dict[str, Any]: Dictionary representation of the configuration.\n        \"\"\"\n        return asdict(self)\n\n    @staticmethod\n    def inter_neuron_type_connectivity_template_df(\n        use_feedback: bool, num_neuron_types: int\n    ) -&gt; pd.DataFrame:\n        \"\"\"Samples the inter-neuron type connectivity matrix.\n\n        Returns:\n            pd.DataFrame: DataFrame representation of the connectivity matrix.\n        \"\"\"\n        row_labels = (\n            [\"input\"]\n            + ([\"feedback\"] if use_feedback else [])\n            + [f\"neuron_{i}\" for i in range(num_neuron_types)]\n        )\n        column_labels = [f\"neuron_{i}\" for i in range(num_neuron_types)] + [\n            \"output\"\n        ]\n\n        return pd.DataFrame(\n            np.zeros((len(row_labels), len(column_labels)), dtype=np.bool),\n            index=row_labels,\n            columns=column_labels,\n        )\n</code></pre>"},{"location":"reference/eirnn/#src.bioplnn.models.spatially_embedded.SpatiallyEmbeddedAreaConfig.asdict","title":"<code>asdict()</code>","text":"<p>Converts the configuration object to a dictionary.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: Dictionary representation of the configuration.</p> Source code in <code>src/bioplnn/models/spatially_embedded.py</code> <pre><code>def asdict(self) -&gt; dict[str, Any]:\n    \"\"\"Converts the configuration object to a dictionary.\n\n    Returns:\n        dict[str, Any]: Dictionary representation of the configuration.\n    \"\"\"\n    return asdict(self)\n</code></pre>"},{"location":"reference/eirnn/#src.bioplnn.models.spatially_embedded.SpatiallyEmbeddedAreaConfig.inter_neuron_type_connectivity_template_df","title":"<code>inter_neuron_type_connectivity_template_df(use_feedback, num_neuron_types)</code>  <code>staticmethod</code>","text":"<p>Samples the inter-neuron type connectivity matrix.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame representation of the connectivity matrix.</p> Source code in <code>src/bioplnn/models/spatially_embedded.py</code> <pre><code>@staticmethod\ndef inter_neuron_type_connectivity_template_df(\n    use_feedback: bool, num_neuron_types: int\n) -&gt; pd.DataFrame:\n    \"\"\"Samples the inter-neuron type connectivity matrix.\n\n    Returns:\n        pd.DataFrame: DataFrame representation of the connectivity matrix.\n    \"\"\"\n    row_labels = (\n        [\"input\"]\n        + ([\"feedback\"] if use_feedback else [])\n        + [f\"neuron_{i}\" for i in range(num_neuron_types)]\n    )\n    column_labels = [f\"neuron_{i}\" for i in range(num_neuron_types)] + [\n        \"output\"\n    ]\n\n    return pd.DataFrame(\n        np.zeros((len(row_labels), len(column_labels)), dtype=np.bool),\n        index=row_labels,\n        columns=column_labels,\n    )\n</code></pre>"},{"location":"reference/eirnn/#src.bioplnn.models.spatially_embedded.SpatiallyEmbeddedRNN","title":"<code>SpatiallyEmbeddedRNN</code>","text":"<p>               Bases: <code>Module</code></p> <p>Spatially embedded RNN.</p> <p>This module stacks multiple SpatiallyEmbeddedArea instances with optional feedback connections between them. It handles spatial dimension matching between areas and provides a flexible interface for configuring the network.</p> <p>Attributes:</p> Name Type Description <code>num_areas</code> <p>Number of SpatiallyEmbeddedArea instances in the network.</p> <code>areas</code> <p>ModuleList containing the SpatiallyEmbeddedArea instances.</p> <code>feedback_convs</code> <p>ModuleDict of feedback convolution layers between areas.</p> <code>area_time_delay</code> <p>Whether to introduce a time delay between areas.</p> <code>pool_mode</code> <p>Pooling mode for area outputs ('max' or 'avg').</p> <code>batch_first</code> <p>Whether input has batch dimension as the first dimension.</p> <code>inter_area_feedback_connectivity</code> <p>Connectivity matrix for feedback connections between areas.</p> <code>inter_area_feedback_nonlinearity</code> <p>Nonlinearities for feedback connections between areas.</p> <code>inter_area_feedback_spatial_extents</code> <p>Kernel sizes for feedback convolutions between areas.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Create a SpatiallyEmbeddedRNN with 2 areas\n&gt;&gt;&gt; area_configs = [\n...     SpatiallyEmbeddedAreaConfig(\n...         in_channels=1,\n...         out_channels=16,\n...         in_size=(64, 64),\n...         feedback_channels=16,\n...     ),\n...     SpatiallyEmbeddedAreaConfig(\n...         in_channels=16,\n...         out_channels=32,\n...         in_size=(32, 32),\n...         feedback_channels=32,\n...     ),\n... ]\n&gt;&gt;&gt; connectivity = [\n...     [0, 0],\n...     [1, 0]\n... ]\n&gt;&gt;&gt; rnn = SpatiallyEmbeddedRNN(\n...     num_areas=2,\n...     area_configs=area_configs,\n...     inter_area_feedback_connectivity=connectivity,\n... )\n&gt;&gt;&gt; # Create a SpatiallyEmbeddedRNN with 2 areas\n&gt;&gt;&gt; area_configs = [\n&gt;&gt;&gt;     SpatiallyEmbeddedAreaConfig(in_channels=1, out_channels=16, in_size=(64, 64)),\n&gt;&gt;&gt;     SpatiallyEmbeddedAreaConfig(in_channels=16, out_channels=32, in_size=(32, 32)),\n&gt;&gt;&gt; ]\n&gt;&gt;&gt; rnn = SpatiallyEmbeddedRNN(num_areas=2, area_configs=area_configs)\n</code></pre> Source code in <code>src/bioplnn/models/spatially_embedded.py</code> <pre><code>class SpatiallyEmbeddedRNN(nn.Module):\n    \"\"\"Spatially embedded RNN.\n\n    This module stacks multiple SpatiallyEmbeddedArea instances with optional\n    feedback connections between them.\n    It handles spatial dimension matching between areas and provides a\n    flexible interface for configuring the network.\n\n    Attributes:\n        num_areas: Number of SpatiallyEmbeddedArea instances in the network.\n        areas: ModuleList containing the SpatiallyEmbeddedArea instances.\n        feedback_convs: ModuleDict of feedback convolution layers between areas.\n        area_time_delay: Whether to introduce a time delay between areas.\n        pool_mode: Pooling mode for area outputs ('max' or 'avg').\n        batch_first: Whether input has batch dimension as the first dimension.\n        inter_area_feedback_connectivity: Connectivity matrix for feedback\n            connections between areas.\n        inter_area_feedback_nonlinearity: Nonlinearities for feedback\n            connections between areas.\n        inter_area_feedback_spatial_extents: Kernel sizes for feedback\n            convolutions between areas.\n\n    Examples:\n        &gt;&gt;&gt; # Create a SpatiallyEmbeddedRNN with 2 areas\n        &gt;&gt;&gt; area_configs = [\n        ...     SpatiallyEmbeddedAreaConfig(\n        ...         in_channels=1,\n        ...         out_channels=16,\n        ...         in_size=(64, 64),\n        ...         feedback_channels=16,\n        ...     ),\n        ...     SpatiallyEmbeddedAreaConfig(\n        ...         in_channels=16,\n        ...         out_channels=32,\n        ...         in_size=(32, 32),\n        ...         feedback_channels=32,\n        ...     ),\n        ... ]\n        &gt;&gt;&gt; connectivity = [\n        ...     [0, 0],\n        ...     [1, 0]\n        ... ]\n        &gt;&gt;&gt; rnn = SpatiallyEmbeddedRNN(\n        ...     num_areas=2,\n        ...     area_configs=area_configs,\n        ...     inter_area_feedback_connectivity=connectivity,\n        ... )\n        &gt;&gt;&gt; # Create a SpatiallyEmbeddedRNN with 2 areas\n        &gt;&gt;&gt; area_configs = [\n        &gt;&gt;&gt;     SpatiallyEmbeddedAreaConfig(in_channels=1, out_channels=16, in_size=(64, 64)),\n        &gt;&gt;&gt;     SpatiallyEmbeddedAreaConfig(in_channels=16, out_channels=32, in_size=(32, 32)),\n        &gt;&gt;&gt; ]\n        &gt;&gt;&gt; rnn = SpatiallyEmbeddedRNN(num_areas=2, area_configs=area_configs)\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        num_areas: int = 1,\n        area_configs: Optional[Sequence[SpatiallyEmbeddedAreaConfig]] = None,\n        area_kwargs: Optional[Sequence[Mapping[str, Any]]] = None,\n        common_area_kwargs: Optional[Mapping[str, Any]] = None,\n        inter_area_feedback_connectivity: Optional[\n            InterAreaParam[Union[int, bool]]\n        ] = None,\n        inter_area_feedback_nonlinearity: Optional[\n            InterAreaParam[Union[str, nn.Module, None]]\n        ] = None,\n        inter_area_feedback_spatial_extents: InterAreaParam[\n            tuple[int, int]\n        ] = (3, 3),\n        area_time_delay: bool = False,\n        pool_mode: Optional[str] = \"max\",\n        batch_first: bool = True,\n    ):\n        \"\"\"Initialize the SpatiallyEmbeddedRNN.\n\n        Args:\n            num_areas: Number of `SpatiallyEmbeddedArea` instances in the network.\n            area_configs: Configuration object(s) for the `SpatiallyEmbeddedArea` instances.\n                If provided as a list, must match the number of areas. If a single config\n                is provided, it will be used for all areas with appropriate adjustments.\n            area_kwargs: Additional keyword arguments for each area. If provided as a list,\n                must match the number of areas.\n            common_area_kwargs: Keyword arguments to apply to all areas.\n            inter_area_feedback_connectivity: Connectivity matrix for feedback connections\n                between areas of shape (num_areas, num_areas). Must be lower triangular and\n                zero/False on the diagonal.\n            inter_area_feedback_nonlinearity: Nonlinearities for feedback connections of\n                shape (num_areas, num_areas).\n            inter_area_feedback_spatial_extents: Kernel sizes for feedback convolutions\n                of shape (num_areas, num_areas).\n            pool_mode: Pooling mode for area outputs.\n            area_time_delay: Whether to introduce a time delay between areas.\n            batch_first: Whether the input tensor has batch dimension as the first dimension.\n\n        Raises:\n            ValueError: If any of the provided parameters are invalid.\n        \"\"\"\n        super().__init__()\n\n        ############################################################\n        # Area configs\n        ############################################################\n\n        self.num_areas = num_areas\n\n        if area_configs is not None:\n            if area_kwargs is not None or common_area_kwargs is not None:\n                raise ValueError(\n                    \"area_configs cannot be provided if area_configs_kwargs \"\n                    \"or common_area_config_kwargs is provided.\"\n                )\n            if len(area_configs) != self.num_areas:\n                raise ValueError(\"area_configs must be of length num_areas.\")\n        else:\n            if area_kwargs is None:\n                if common_area_kwargs is None:\n                    raise ValueError(\n                        \"area_kwargs or common_area_kwargs must be provided if \"\n                        \"area_configs is not provided.\"\n                    )\n                area_kwargs = [{}] * self.num_areas  # type: ignore\n            elif len(area_kwargs) != self.num_areas:\n                raise ValueError(\"area_kwargs must be of length num_areas.\")\n\n            if common_area_kwargs is None:\n                common_area_kwargs = {}\n\n            area_configs = [\n                SpatiallyEmbeddedAreaConfig(\n                    **common_area_kwargs,\n                    **area_kwargs[i],  # type: ignore\n                )\n                for i in range(self.num_areas)\n            ]\n\n        # Validate area configurations\n        for i in range(self.num_areas - 1):\n            if area_configs[i].out_channels != area_configs[i + 1].in_channels:\n                raise ValueError(\n                    f\"The output channels of area {i} must match the input \"\n                    f\"channels of area {i + 1}.\"\n                )\n\n        ############################################################\n        # RNN parameters\n        ############################################################\n\n        self.area_time_delay = area_time_delay\n        self.pool_mode = pool_mode\n        self.batch_first = batch_first\n\n        ############################################################\n        # Initialize areas\n        ############################################################\n\n        # Create areas\n        self.areas = nn.ModuleList(\n            [\n                SpatiallyEmbeddedArea(area_config)\n                for area_config in area_configs\n            ]\n        )\n\n        ############################################################\n        # Initialize feedback connections\n        ############################################################\n\n        self.feedback_convs = nn.ModuleDict()\n        if inter_area_feedback_connectivity is None:\n            if any(\n                area_config.feedback_channels for area_config in area_configs\n            ):\n                raise ValueError(\n                    \"inter_area_feedback_connectivity must be provided if and only if \"\n                    \"feedback_channels is provided for at least one area.\"\n                )\n        else:\n            self.inter_area_feedback_connectivity = np.array(\n                inter_area_feedback_connectivity, dtype=bool\n            )\n            if self.inter_area_feedback_connectivity.shape != (\n                self.num_areas,\n                self.num_areas,\n            ):\n                raise ValueError(\n                    \"The shape of inter_area_feedback_connectivity must be (num_areas, num_areas).\"\n                )\n            self.inter_area_feedback_nonlinearity = expand_array_2d(\n                inter_area_feedback_nonlinearity,\n                self.num_areas,\n                self.num_areas,\n            )\n            self.inter_area_feedback_spatial_extents = expand_array_2d(\n                inter_area_feedback_spatial_extents,\n                self.num_areas,\n                self.num_areas,\n                depth=1,\n            )\n\n            # Validate inter_area_feedback_connectivity tensor\n            if (\n                self.inter_area_feedback_connectivity.ndim != 2\n                or self.inter_area_feedback_connectivity.shape[0]\n                != self.num_areas\n                or self.inter_area_feedback_connectivity.shape[1]\n                != self.num_areas\n            ):\n                raise ValueError(\n                    \"The dimensions of inter_area_feedback_connectivity must match the number of areas.\"\n                )\n            if self.inter_area_feedback_connectivity.sum() == 0:\n                raise ValueError(\n                    \"inter_area_feedback_connectivity must be a non-zero tensor if provided.\"\n                )\n\n            # Create feedback convolutions\n            for i, row in enumerate(self.inter_area_feedback_connectivity):\n                nonzero_indices = np.nonzero(row)[0]\n                for j in nonzero_indices:\n                    if i &lt;= j:\n                        raise ValueError(\n                            f\"the feedback connection from area {i} to area {j} \"\n                            f\"is not valid because feedback connections must \"\n                            f\"pass information from later areas to earlier \"\n                            f\"areas (i.e. inter_area_feedback_connectivity must \"\n                            f\"be lower triangular and zero on the diagonal).\"\n                        )\n                    if not self.areas[j].use_feedback:\n                        raise ValueError(\n                            f\"the connection from area {i} to area {j} is \"\n                            f\"not valid because area {j} does not receive \"\n                            f\"feedback (hint: feedback_channels may not be provided)\"\n                        )\n                    self.feedback_convs[f\"{i}-&gt;{j}\"] = nn.Sequential(\n                        nn.Upsample(\n                            size=area_configs[j].in_size,\n                            mode=\"bilinear\",\n                        ),\n                        nn.Conv2d(\n                            in_channels=area_configs[i].out_channels,\n                            out_channels=area_configs[j].feedback_channels,\n                            kernel_size=self.inter_area_feedback_spatial_extents[\n                                i, j\n                            ],\n                            padding=(\n                                self.inter_area_feedback_spatial_extents[i, j][\n                                    0\n                                ]\n                                // 2,\n                                self.inter_area_feedback_spatial_extents[i, j][\n                                    1\n                                ]\n                                // 2,\n                            ),\n                            bias=area_configs[\n                                j\n                            ].default_feedback_state_init_fn,\n                        ),\n                        get_activation(\n                            self.inter_area_feedback_nonlinearity[i, j]\n                        ),\n                    )\n\n    def init_neuron_states(\n        self,\n        batch_size: int,\n        init_fn: Optional[Union[str, TensorInitFnType]] = None,\n        device: Optional[Union[torch.device, str]] = None,\n    ) -&gt; list[list[torch.Tensor]]:\n        \"\"\"Initializes the hidden states for all areas.\n\n        Args:\n            batch_size: Batch size.\n            init_fn: Initialization function.\n            device: Device to allocate tensors.\n\n        Returns:\n            A list containing the initialized neuron hidden states for each area.\n        \"\"\"\n\n        return [\n            area.init_neuron_state(batch_size, init_fn, device)  # type: ignore\n            for area in self.areas\n        ]\n\n    def init_feedback_states(\n        self,\n        batch_size: int,\n        init_fn: Optional[Union[str, TensorInitFnType]] = None,\n        device: Optional[Union[torch.device, str]] = None,\n    ) -&gt; list[Optional[torch.Tensor]]:\n        \"\"\"Initializes the feedback inputs for all areas.\n\n        Args:\n            batch_size: Batch size.\n            init_fn: Initialization function.\n            device: Device to allocate tensors.\n\n        Returns:\n            A list of initialized feedback inputs for each area.\n        \"\"\"\n        return [\n            area.init_feedback_state(batch_size, init_fn, device)  # type: ignore\n            for area in self.areas\n        ]\n\n    def init_output_states(\n        self,\n        batch_size: int,\n        init_fn: Optional[Union[str, TensorInitFnType]] = None,\n        device: Optional[Union[torch.device, str]] = None,\n    ) -&gt; list[torch.Tensor]:\n        \"\"\"Initializes the outputs for all areas.\n\n        Args:\n            batch_size: Batch size.\n            init_fn: Initialization function.\n            device: Device to allocate tensors.\n\n        Returns:\n            A list of initialized outputs for each area.\n        \"\"\"\n\n        return [\n            area.init_output_state(batch_size, init_fn, device)  # type: ignore\n            for area in self.areas\n        ]\n\n    def init_states(\n        self,\n        out0: Optional[Sequence[Union[torch.Tensor, None]]],\n        h_neuron0: Optional[Sequence[Sequence[Union[torch.Tensor, None]]]],\n        fb0: Optional[Sequence[Union[torch.Tensor, None]]],\n        num_steps: int,\n        batch_size: int,\n        out_init_fn: Optional[Union[str, TensorInitFnType]] = None,\n        hidden_init_fn: Optional[Union[str, TensorInitFnType]] = None,\n        fb_init_fn: Optional[Union[str, TensorInitFnType]] = None,\n        device: Optional[Union[torch.device, str]] = None,\n    ) -&gt; tuple[\n        list[list[Union[torch.Tensor, None]]],\n        list[list[list[Union[torch.Tensor, None]]]],\n        list[list[Union[torch.Tensor, int, None]]],\n    ]:\n        \"\"\"Initializes the state of the network.\n\n        Args:\n            out0: Initial outputs for each area. If None, default initialization is used.\n            h_neuron0: Initial neuron hidden states for each area. If None, default\n                initialization is used.\n            fb0: Initial feedback inputs for each area. If None, default initialization is used.\n            num_steps: Number of time steps.\n            batch_size: Batch size.\n            out_init_fn: Initialization function for outputs if out0 is None. Defaults to None.\n            hidden_init_fn: Initialization function for hidden states if h_neuron0 is None.\n                Defaults to None.\n            fb_init_fn: Initialization function for feedback inputs if fb0 is None.\n                Defaults to None.\n            device: Device to allocate tensors on. Defaults to None.\n\n        Returns:\n            A tuple containing:\n            - Initialized outputs for each area and time step\n            - Initialized neuron hidden states for each area, time step, and neuron type\n            - Initialized feedback inputs for each area and time step\n\n        Raises:\n            ValueError: If the length of out0, h_neuron0, or fb0 doesn't match\n                the number of areas.\n        \"\"\"\n\n        # Validate input shapes of out0, h_neuron0, fb0\n        if out0 is None:\n            out0 = [None] * self.num_areas\n        elif len(out0) != self.num_areas:\n            raise ValueError(\n                \"The length of out0 must be equal to the number of areas.\"\n            )\n        if h_neuron0 is None:\n            h_neuron0 = [\n                [None] * self.areas[i].num_neuron_types  # type: ignore\n                for i in range(self.num_areas)\n            ]\n        elif len(h_neuron0) != self.num_areas:\n            raise ValueError(\n                \"The length of h_neuron0 must be equal to the number of areas.\"\n            )\n        if fb0 is None:\n            fb0 = [None] * self.num_areas\n        elif len(fb0) != self.num_areas:\n            raise ValueError(\n                \"The length of fb0 must be equal to the number of areas.\"\n            )\n\n        # Initialize default values\n        h_neuron0_default = self.init_neuron_states(\n            batch_size, hidden_init_fn, device\n        )\n        fb0_default = self.init_feedback_states(batch_size, fb_init_fn, device)\n        out0_default = self.init_output_states(batch_size, out_init_fn, device)\n\n        # Initialize output, hidden state, and feedback lists\n        outs: list[list[Union[torch.Tensor, None]]] = [\n            [None] * num_steps for _ in range(self.num_areas)\n        ]\n        h_neurons: list[list[list[Union[torch.Tensor, None]]]] = [\n            [[None] * self.areas[i].num_neuron_types for _ in range(num_steps)]  # type: ignore\n            for i in range(self.num_areas)\n        ]\n        fbs: list[list[Union[torch.Tensor, int, None]]] = [\n            [0 if self.areas[i].use_feedback else None] * num_steps\n            for i in range(self.num_areas)\n        ]\n\n        # Fill time step -1 with the initial values\n        for i in range(self.num_areas):\n            outs[i][-1] = out0[i] if out0[i] is not None else out0_default[i]\n            fbs[i][-1] = fb0[i] if fb0[i] is not None else fb0_default[i]\n            for k in range(self.areas[i].num_neuron_types):  # type: ignore\n                h_neurons[i][-1][k] = (\n                    h_neuron0[i][k]\n                    if h_neuron0[i][k] is not None\n                    else h_neuron0_default[i][k]\n                )\n\n        return outs, h_neurons, fbs\n\n    def _format_x(\n        self,\n        x: torch.Tensor,\n        num_steps: Optional[int] = None,\n    ) -&gt; tuple[torch.Tensor, int]:\n        \"\"\"Formats the input tensor to match the expected shape.\n\n        This method handles both single-step (4D) and multi-step (5D) input tensors,\n        converting them to a consistent format with seq_len as the first dimension.\n        For 4D inputs, it replicates the input across all time steps.\n\n        Args:\n            x: Input tensor, can be:\n                - 4D tensor of shape (batch_size, channels, height, width) for a\n                  single time step. Will be expanded to all time steps.\n                - 5D tensor of shape (seq_len, batch_size, channels, height, width) or\n                  (batch_size, seq_len, channels, height, width) if batch_first=True.\n            num_steps: Number of time steps. Required if x is 4D.\n                If x is 5D, it will be inferred from the sequence dimension unless\n                explicitly provided.\n\n        Returns:\n            A tuple containing:\n            - The formatted input tensor with shape (seq_len, batch_size, channels, height, width)\n            - The number of time steps\n\n        Raises:\n            ValueError: If x has invalid dimensions (not 4D or 5D), if num_steps is\n                not provided for 4D inputs, or if num_steps is inconsistent with\n                the sequence length of 5D inputs.\n        \"\"\"\n        if x.dim() == 4:\n            if num_steps is None or num_steps &lt; 1:\n                raise ValueError(\n                    \"If x is 4D, num_steps must be provided and greater than 0\"\n                )\n            x = x.unsqueeze(0).expand((num_steps, -1, -1, -1, -1))\n        elif x.dim() == 5:\n            if self.batch_first:\n                x = x.transpose(0, 1)\n            if num_steps is not None and num_steps != x.shape[0]:\n                raise ValueError(\n                    \"If x is 5D and num_steps is provided, it must match the sequence length.\"\n                )\n            num_steps = x.shape[0]\n        else:\n            raise ValueError(\n                \"The input must be a 4D tensor or a 5D tensor with sequence length.\"\n            )\n\n        return x, num_steps\n\n    def _format_result(\n        self,\n        outs: list[list[torch.Tensor]],\n        h_neurons: list[list[list[torch.Tensor]]],\n        fbs: list[list[Optional[torch.Tensor]]],\n    ) -&gt; tuple[\n        list[torch.Tensor],\n        list[list[torch.Tensor]],\n        list[Optional[torch.Tensor]],\n    ]:\n        \"\"\"Formats the outputs, hidden states, and feedback inputs for return.\n\n        This method stacks tensors across time steps and applies batch_first\n        transposition if needed. Each tensor has shape (batch_size, channels,\n        height, width).\n\n        Args:\n            outs: Outputs for each area and time step. Shape: [num_areas][num_steps],\n            h_neurons: Neuron hidden states for each area, time step, and neuron type.\n                Shape: [num_areas][num_steps][num_neuron_types].\n            fbs: Feedback inputs for each area and time step. Shape: [num_areas][num_steps].\n\n        Returns:\n            A tuple containing:\n            - List of stacked outputs per area. Each tensor has shape:\n              (seq_len, batch_size, channels, height, width) or\n              (batch_size, seq_len, channels, height, width) if batch_first=True.\n            - List of lists of stacked hidden states per area and neuron type.\n              Same shape pattern as outputs.\n            - List of stacked feedback inputs per area (or None if not used).\n              Same shape pattern as outputs.\n        \"\"\"\n        outs_stack: list[torch.Tensor] = []\n        h_neurons_stack: list[list[torch.Tensor]] = []\n        fbs_stack: Optional[list[Optional[torch.Tensor]]] = []\n\n        for i in range(self.num_areas):\n            outs_stack.append(torch.stack(outs[i]))\n            h_neurons_stack.append(\n                [\n                    torch.stack(\n                        [h_neurons[i][t][j] for t in range(len(h_neurons[i]))]\n                    )\n                    for j in range(self.areas[i].num_neuron_types)  # type: ignore\n                ]\n            )\n            if self.areas[i].use_feedback:\n                fbs_stack.append(torch.stack(fbs[i]))  # type: ignore\n            else:\n                assert all(feedback_state is None for feedback_state in fbs[i])\n                fbs_stack.append(None)\n            if self.batch_first:\n                outs_stack[i] = outs_stack[i].transpose(0, 1)\n                for j in range(self.areas[i].num_neuron_types):  # type: ignore\n                    h_neurons_stack[i][j] = h_neurons_stack[i][j].transpose(\n                        0, 1\n                    )\n                if self.areas[i].use_feedback:\n                    fbs_stack[i] = fbs_stack[i].transpose(0, 1)  # type: ignore\n\n        return outs_stack, h_neurons_stack, fbs_stack\n\n    def _match_spatial_size(\n        self,\n        x: torch.Tensor,\n        size: tuple[int, int],\n    ) -&gt; torch.Tensor:\n        \"\"\"Adjusts the spatial dimensions of the input tensor.\n\n        This method ensures that tensors have compatible spatial dimensions when\n        passing between areas. It uses either pooling (when downsampling) or\n        interpolation (when upsampling).\n\n        Args:\n            x: Input tensor of shape (batch_size, channels, height, width).\n            size: Target spatial size (height, width).\n\n        Returns:\n            Resized tensor matching the target spatial size.\n\n        Raises:\n            ValueError: If self.pool_mode is not 'avg' or 'max'.\n        \"\"\"\n        if x.shape[-2] &gt; size[0] and x.shape[-1] &gt; size[1]:\n            if self.pool_mode == \"avg\":\n                return F.adaptive_avg_pool2d(x, size)\n            elif self.pool_mode == \"max\":\n                return F.adaptive_max_pool2d(x, size)\n            else:\n                raise ValueError(f\"Invalid pool_mode: {self.pool_mode}\")\n        elif x.shape[-2] &lt; size[0] and x.shape[-1] &lt; size[1]:\n            return F.interpolate(x, size, mode=\"bilinear\", align_corners=False)\n        else:\n            assert x.shape[-2] == size[0] and x.shape[-1] == size[1]\n            return x\n\n    def query_neuron_states(\n        self,\n        neuron_states: list[list[torch.Tensor]],\n        area: int,\n        neuron_type: int,\n        time_step: Optional[Union[int, slice]] = None,\n        batch: Optional[Union[int, slice]] = None,\n        neuron_subtype: Optional[Union[int, slice]] = None,\n        spatial_location_i: Optional[Union[int, slice]] = None,\n        spatial_location_j: Optional[Union[int, slice]] = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"Query the model states for a given area, time step, neuron type, neuron subtype, and spatial location.\n\n        Args:\n            neuron_states: List of lists of tensors containing the model states.\n            area: The area index.\n            neuron_type: The neuron type index.\n            time_step: The time step index. If not provided, all time steps are returned.\n            batch: The batch index. If not provided, all batches are returned.\n            neuron_subtype: The neuron subtype index. If not provided, all neuron subtypes are returned.\n            spatial_location_i: The spatial location (height). If not provided, all spatial locations are returned.\n            spatial_location_j: The spatial location (width). If not provided, all spatial locations are returned.\n\n        Returns:\n            The queried state of shape (batch_size_slice, channels, height, width)\n        \"\"\"\n        if time_step is None:\n            time_idx = slice(None)\n        else:\n            time_idx = time_step\n        if batch is None:\n            batch_idx = slice(None)\n        else:\n            batch_idx = batch\n        if neuron_subtype is None:\n            neuron_subtype_idx = slice(None)\n        else:\n            neuron_subtype_idx = neuron_subtype\n        if spatial_location_i is None:\n            spatial_location_idx_i = slice(None)\n        else:\n            spatial_location_idx_i = spatial_location_i\n        if spatial_location_j is None:\n            spatial_location_idx_j = slice(None)\n        else:\n            spatial_location_idx_j = spatial_location_j\n\n        if self.batch_first:\n            return neuron_states[area][neuron_type][\n                batch_idx,\n                time_idx,\n                neuron_subtype_idx,\n                spatial_location_idx_i,\n                spatial_location_idx_j,\n            ]\n        else:\n            return neuron_states[area][neuron_type][\n                time_idx,\n                batch_idx,\n                neuron_subtype_idx,\n                spatial_location_idx_i,\n                spatial_location_idx_j,\n            ]\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        num_steps: Optional[int] = None,\n        output_state0: Optional[Sequence[Optional[torch.Tensor]]] = None,\n        neuron_state0: Optional[\n            Sequence[Sequence[Optional[torch.Tensor]]]\n        ] = None,\n        feedback_state0: Optional[Sequence[Optional[torch.Tensor]]] = None,\n    ) -&gt; tuple[\n        list[torch.Tensor],\n        list[list[torch.Tensor]],\n        list[Union[torch.Tensor, None]],\n    ]:\n        \"\"\"Performs forward pass of the SpatiallyEmbeddedRNN.\n\n        Args:\n            x: Input tensor. Can be either:\n                - 4D tensor of shape (batch_size, in_channels, height, width)\n                  representing a single time step. In this case, num_steps must be\n                  provided.\n                - 5D tensor of shape (seq_len, batch_size, in_channels, height, width)\n                  or (batch_size, seq_len, in_channels, height, width) if\n                  batch_first=True.\n            num_steps: Number of time steps. Required if x is 4D.\n                If x is 5D, this must match the sequence length dimension in x.\n            output_state0: Initial outputs for each area. Length should match the\n                number of areas. Each element can be None to use default initialization.\n            neuron_state0: Initial neuron hidden states for each area and neuron type.\n                Length should match the number of areas, and each inner sequence length\n                should match the number of neuron types in that area.\n            feedback_state0: Initial feedback inputs for each area. Length should match\n                the number of areas.\n\n        Returns:\n            A tuple containing:\n            - Outputs for each area. Each tensor has shape:\n              (seq_len, batch_size, out_channels, height, width) or\n              (batch_size, seq_len, out_channels, height, width) if batch_first=True.\n            - Hidden states for each area and neuron type.\n              Same shape pattern as outputs but with neuron_channels.\n            - Feedback inputs for each area (None if the area doesn't use feedback).\n\n        Raises:\n            ValueError: If input shape is invalid or num_steps is inconsistent with\n                the input shape.\n        \"\"\"\n\n        device = x.device\n\n        x, num_steps = self._format_x(x, num_steps)\n\n        batch_size = x.shape[1]\n\n        output_states, neuron_states, feedback_states = self.init_states(\n            output_state0,\n            neuron_state0,\n            feedback_state0,\n            num_steps,\n            batch_size,\n            device=device,\n        )\n\n        for t in range(num_steps):\n            for i, area in enumerate(self.areas):\n                # Compute area update and output\n                if i == 0:\n                    area_in = x[t]\n                else:\n                    if self.area_time_delay:\n                        area_in = output_states[i - 1][t - 1]\n                    else:\n                        area_in = output_states[i - 1][t]\n                    assert isinstance(area_in, torch.Tensor)\n                    area_in = self._match_spatial_size(\n                        area_in,\n                        self.areas[i].in_size,  # type: ignore\n                    )\n\n                output_states[i][t], neuron_states[i][t] = area(\n                    input=area_in,\n                    neuron_state=neuron_states[i][t - 1],\n                    feedback_state=feedback_states[i][t - 1],\n                )\n\n            # Apply feedback\n            for key, conv in self.feedback_convs.items():\n                area_i, area_j = key.split(\"-&gt;\")\n                area_i, area_j = int(area_i), int(area_j)\n                feedback_states[area_j][t] = feedback_states[area_j][t] + conv(\n                    output_states[area_i][t]\n                )\n\n        output_states, neuron_states, feedback_states = self._format_result(\n            output_states,  # type: ignore\n            neuron_states,  # type: ignore\n            feedback_states,  # type: ignore\n        )\n\n        return output_states, neuron_states, feedback_states\n</code></pre>"},{"location":"reference/eirnn/#src.bioplnn.models.spatially_embedded.SpatiallyEmbeddedRNN.__init__","title":"<code>__init__(*, num_areas=1, area_configs=None, area_kwargs=None, common_area_kwargs=None, inter_area_feedback_connectivity=None, inter_area_feedback_nonlinearity=None, inter_area_feedback_spatial_extents=(3, 3), area_time_delay=False, pool_mode='max', batch_first=True)</code>","text":"<p>Initialize the SpatiallyEmbeddedRNN.</p> <p>Parameters:</p> Name Type Description Default <code>num_areas</code> <code>int</code> <p>Number of <code>SpatiallyEmbeddedArea</code> instances in the network.</p> <code>1</code> <code>area_configs</code> <code>Optional[Sequence[SpatiallyEmbeddedAreaConfig]]</code> <p>Configuration object(s) for the <code>SpatiallyEmbeddedArea</code> instances. If provided as a list, must match the number of areas. If a single config is provided, it will be used for all areas with appropriate adjustments.</p> <code>None</code> <code>area_kwargs</code> <code>Optional[Sequence[Mapping[str, Any]]]</code> <p>Additional keyword arguments for each area. If provided as a list, must match the number of areas.</p> <code>None</code> <code>common_area_kwargs</code> <code>Optional[Mapping[str, Any]]</code> <p>Keyword arguments to apply to all areas.</p> <code>None</code> <code>inter_area_feedback_connectivity</code> <code>Optional[InterAreaParam[Union[int, bool]]]</code> <p>Connectivity matrix for feedback connections between areas of shape (num_areas, num_areas). Must be lower triangular and zero/False on the diagonal.</p> <code>None</code> <code>inter_area_feedback_nonlinearity</code> <code>Optional[InterAreaParam[Union[str, Module, None]]]</code> <p>Nonlinearities for feedback connections of shape (num_areas, num_areas).</p> <code>None</code> <code>inter_area_feedback_spatial_extents</code> <code>InterAreaParam[tuple[int, int]]</code> <p>Kernel sizes for feedback convolutions of shape (num_areas, num_areas).</p> <code>(3, 3)</code> <code>pool_mode</code> <code>Optional[str]</code> <p>Pooling mode for area outputs.</p> <code>'max'</code> <code>area_time_delay</code> <code>bool</code> <p>Whether to introduce a time delay between areas.</p> <code>False</code> <code>batch_first</code> <code>bool</code> <p>Whether the input tensor has batch dimension as the first dimension.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any of the provided parameters are invalid.</p> Source code in <code>src/bioplnn/models/spatially_embedded.py</code> <pre><code>def __init__(\n    self,\n    *,\n    num_areas: int = 1,\n    area_configs: Optional[Sequence[SpatiallyEmbeddedAreaConfig]] = None,\n    area_kwargs: Optional[Sequence[Mapping[str, Any]]] = None,\n    common_area_kwargs: Optional[Mapping[str, Any]] = None,\n    inter_area_feedback_connectivity: Optional[\n        InterAreaParam[Union[int, bool]]\n    ] = None,\n    inter_area_feedback_nonlinearity: Optional[\n        InterAreaParam[Union[str, nn.Module, None]]\n    ] = None,\n    inter_area_feedback_spatial_extents: InterAreaParam[\n        tuple[int, int]\n    ] = (3, 3),\n    area_time_delay: bool = False,\n    pool_mode: Optional[str] = \"max\",\n    batch_first: bool = True,\n):\n    \"\"\"Initialize the SpatiallyEmbeddedRNN.\n\n    Args:\n        num_areas: Number of `SpatiallyEmbeddedArea` instances in the network.\n        area_configs: Configuration object(s) for the `SpatiallyEmbeddedArea` instances.\n            If provided as a list, must match the number of areas. If a single config\n            is provided, it will be used for all areas with appropriate adjustments.\n        area_kwargs: Additional keyword arguments for each area. If provided as a list,\n            must match the number of areas.\n        common_area_kwargs: Keyword arguments to apply to all areas.\n        inter_area_feedback_connectivity: Connectivity matrix for feedback connections\n            between areas of shape (num_areas, num_areas). Must be lower triangular and\n            zero/False on the diagonal.\n        inter_area_feedback_nonlinearity: Nonlinearities for feedback connections of\n            shape (num_areas, num_areas).\n        inter_area_feedback_spatial_extents: Kernel sizes for feedback convolutions\n            of shape (num_areas, num_areas).\n        pool_mode: Pooling mode for area outputs.\n        area_time_delay: Whether to introduce a time delay between areas.\n        batch_first: Whether the input tensor has batch dimension as the first dimension.\n\n    Raises:\n        ValueError: If any of the provided parameters are invalid.\n    \"\"\"\n    super().__init__()\n\n    ############################################################\n    # Area configs\n    ############################################################\n\n    self.num_areas = num_areas\n\n    if area_configs is not None:\n        if area_kwargs is not None or common_area_kwargs is not None:\n            raise ValueError(\n                \"area_configs cannot be provided if area_configs_kwargs \"\n                \"or common_area_config_kwargs is provided.\"\n            )\n        if len(area_configs) != self.num_areas:\n            raise ValueError(\"area_configs must be of length num_areas.\")\n    else:\n        if area_kwargs is None:\n            if common_area_kwargs is None:\n                raise ValueError(\n                    \"area_kwargs or common_area_kwargs must be provided if \"\n                    \"area_configs is not provided.\"\n                )\n            area_kwargs = [{}] * self.num_areas  # type: ignore\n        elif len(area_kwargs) != self.num_areas:\n            raise ValueError(\"area_kwargs must be of length num_areas.\")\n\n        if common_area_kwargs is None:\n            common_area_kwargs = {}\n\n        area_configs = [\n            SpatiallyEmbeddedAreaConfig(\n                **common_area_kwargs,\n                **area_kwargs[i],  # type: ignore\n            )\n            for i in range(self.num_areas)\n        ]\n\n    # Validate area configurations\n    for i in range(self.num_areas - 1):\n        if area_configs[i].out_channels != area_configs[i + 1].in_channels:\n            raise ValueError(\n                f\"The output channels of area {i} must match the input \"\n                f\"channels of area {i + 1}.\"\n            )\n\n    ############################################################\n    # RNN parameters\n    ############################################################\n\n    self.area_time_delay = area_time_delay\n    self.pool_mode = pool_mode\n    self.batch_first = batch_first\n\n    ############################################################\n    # Initialize areas\n    ############################################################\n\n    # Create areas\n    self.areas = nn.ModuleList(\n        [\n            SpatiallyEmbeddedArea(area_config)\n            for area_config in area_configs\n        ]\n    )\n\n    ############################################################\n    # Initialize feedback connections\n    ############################################################\n\n    self.feedback_convs = nn.ModuleDict()\n    if inter_area_feedback_connectivity is None:\n        if any(\n            area_config.feedback_channels for area_config in area_configs\n        ):\n            raise ValueError(\n                \"inter_area_feedback_connectivity must be provided if and only if \"\n                \"feedback_channels is provided for at least one area.\"\n            )\n    else:\n        self.inter_area_feedback_connectivity = np.array(\n            inter_area_feedback_connectivity, dtype=bool\n        )\n        if self.inter_area_feedback_connectivity.shape != (\n            self.num_areas,\n            self.num_areas,\n        ):\n            raise ValueError(\n                \"The shape of inter_area_feedback_connectivity must be (num_areas, num_areas).\"\n            )\n        self.inter_area_feedback_nonlinearity = expand_array_2d(\n            inter_area_feedback_nonlinearity,\n            self.num_areas,\n            self.num_areas,\n        )\n        self.inter_area_feedback_spatial_extents = expand_array_2d(\n            inter_area_feedback_spatial_extents,\n            self.num_areas,\n            self.num_areas,\n            depth=1,\n        )\n\n        # Validate inter_area_feedback_connectivity tensor\n        if (\n            self.inter_area_feedback_connectivity.ndim != 2\n            or self.inter_area_feedback_connectivity.shape[0]\n            != self.num_areas\n            or self.inter_area_feedback_connectivity.shape[1]\n            != self.num_areas\n        ):\n            raise ValueError(\n                \"The dimensions of inter_area_feedback_connectivity must match the number of areas.\"\n            )\n        if self.inter_area_feedback_connectivity.sum() == 0:\n            raise ValueError(\n                \"inter_area_feedback_connectivity must be a non-zero tensor if provided.\"\n            )\n\n        # Create feedback convolutions\n        for i, row in enumerate(self.inter_area_feedback_connectivity):\n            nonzero_indices = np.nonzero(row)[0]\n            for j in nonzero_indices:\n                if i &lt;= j:\n                    raise ValueError(\n                        f\"the feedback connection from area {i} to area {j} \"\n                        f\"is not valid because feedback connections must \"\n                        f\"pass information from later areas to earlier \"\n                        f\"areas (i.e. inter_area_feedback_connectivity must \"\n                        f\"be lower triangular and zero on the diagonal).\"\n                    )\n                if not self.areas[j].use_feedback:\n                    raise ValueError(\n                        f\"the connection from area {i} to area {j} is \"\n                        f\"not valid because area {j} does not receive \"\n                        f\"feedback (hint: feedback_channels may not be provided)\"\n                    )\n                self.feedback_convs[f\"{i}-&gt;{j}\"] = nn.Sequential(\n                    nn.Upsample(\n                        size=area_configs[j].in_size,\n                        mode=\"bilinear\",\n                    ),\n                    nn.Conv2d(\n                        in_channels=area_configs[i].out_channels,\n                        out_channels=area_configs[j].feedback_channels,\n                        kernel_size=self.inter_area_feedback_spatial_extents[\n                            i, j\n                        ],\n                        padding=(\n                            self.inter_area_feedback_spatial_extents[i, j][\n                                0\n                            ]\n                            // 2,\n                            self.inter_area_feedback_spatial_extents[i, j][\n                                1\n                            ]\n                            // 2,\n                        ),\n                        bias=area_configs[\n                            j\n                        ].default_feedback_state_init_fn,\n                    ),\n                    get_activation(\n                        self.inter_area_feedback_nonlinearity[i, j]\n                    ),\n                )\n</code></pre>"},{"location":"reference/eirnn/#src.bioplnn.models.spatially_embedded.SpatiallyEmbeddedRNN.forward","title":"<code>forward(x, num_steps=None, output_state0=None, neuron_state0=None, feedback_state0=None)</code>","text":"<p>Performs forward pass of the SpatiallyEmbeddedRNN.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor. Can be either: - 4D tensor of shape (batch_size, in_channels, height, width)   representing a single time step. In this case, num_steps must be   provided. - 5D tensor of shape (seq_len, batch_size, in_channels, height, width)   or (batch_size, seq_len, in_channels, height, width) if   batch_first=True.</p> required <code>num_steps</code> <code>Optional[int]</code> <p>Number of time steps. Required if x is 4D. If x is 5D, this must match the sequence length dimension in x.</p> <code>None</code> <code>output_state0</code> <code>Optional[Sequence[Optional[Tensor]]]</code> <p>Initial outputs for each area. Length should match the number of areas. Each element can be None to use default initialization.</p> <code>None</code> <code>neuron_state0</code> <code>Optional[Sequence[Sequence[Optional[Tensor]]]]</code> <p>Initial neuron hidden states for each area and neuron type. Length should match the number of areas, and each inner sequence length should match the number of neuron types in that area.</p> <code>None</code> <code>feedback_state0</code> <code>Optional[Sequence[Optional[Tensor]]]</code> <p>Initial feedback inputs for each area. Length should match the number of areas.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Tensor]</code> <p>A tuple containing:</p> <code>list[list[Tensor]]</code> <ul> <li>Outputs for each area. Each tensor has shape: (seq_len, batch_size, out_channels, height, width) or (batch_size, seq_len, out_channels, height, width) if batch_first=True.</li> </ul> <code>list[Union[Tensor, None]]</code> <ul> <li>Hidden states for each area and neuron type. Same shape pattern as outputs but with neuron_channels.</li> </ul> <code>tuple[list[Tensor], list[list[Tensor]], list[Union[Tensor, None]]]</code> <ul> <li>Feedback inputs for each area (None if the area doesn't use feedback).</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input shape is invalid or num_steps is inconsistent with the input shape.</p> Source code in <code>src/bioplnn/models/spatially_embedded.py</code> <pre><code>def forward(\n    self,\n    x: torch.Tensor,\n    num_steps: Optional[int] = None,\n    output_state0: Optional[Sequence[Optional[torch.Tensor]]] = None,\n    neuron_state0: Optional[\n        Sequence[Sequence[Optional[torch.Tensor]]]\n    ] = None,\n    feedback_state0: Optional[Sequence[Optional[torch.Tensor]]] = None,\n) -&gt; tuple[\n    list[torch.Tensor],\n    list[list[torch.Tensor]],\n    list[Union[torch.Tensor, None]],\n]:\n    \"\"\"Performs forward pass of the SpatiallyEmbeddedRNN.\n\n    Args:\n        x: Input tensor. Can be either:\n            - 4D tensor of shape (batch_size, in_channels, height, width)\n              representing a single time step. In this case, num_steps must be\n              provided.\n            - 5D tensor of shape (seq_len, batch_size, in_channels, height, width)\n              or (batch_size, seq_len, in_channels, height, width) if\n              batch_first=True.\n        num_steps: Number of time steps. Required if x is 4D.\n            If x is 5D, this must match the sequence length dimension in x.\n        output_state0: Initial outputs for each area. Length should match the\n            number of areas. Each element can be None to use default initialization.\n        neuron_state0: Initial neuron hidden states for each area and neuron type.\n            Length should match the number of areas, and each inner sequence length\n            should match the number of neuron types in that area.\n        feedback_state0: Initial feedback inputs for each area. Length should match\n            the number of areas.\n\n    Returns:\n        A tuple containing:\n        - Outputs for each area. Each tensor has shape:\n          (seq_len, batch_size, out_channels, height, width) or\n          (batch_size, seq_len, out_channels, height, width) if batch_first=True.\n        - Hidden states for each area and neuron type.\n          Same shape pattern as outputs but with neuron_channels.\n        - Feedback inputs for each area (None if the area doesn't use feedback).\n\n    Raises:\n        ValueError: If input shape is invalid or num_steps is inconsistent with\n            the input shape.\n    \"\"\"\n\n    device = x.device\n\n    x, num_steps = self._format_x(x, num_steps)\n\n    batch_size = x.shape[1]\n\n    output_states, neuron_states, feedback_states = self.init_states(\n        output_state0,\n        neuron_state0,\n        feedback_state0,\n        num_steps,\n        batch_size,\n        device=device,\n    )\n\n    for t in range(num_steps):\n        for i, area in enumerate(self.areas):\n            # Compute area update and output\n            if i == 0:\n                area_in = x[t]\n            else:\n                if self.area_time_delay:\n                    area_in = output_states[i - 1][t - 1]\n                else:\n                    area_in = output_states[i - 1][t]\n                assert isinstance(area_in, torch.Tensor)\n                area_in = self._match_spatial_size(\n                    area_in,\n                    self.areas[i].in_size,  # type: ignore\n                )\n\n            output_states[i][t], neuron_states[i][t] = area(\n                input=area_in,\n                neuron_state=neuron_states[i][t - 1],\n                feedback_state=feedback_states[i][t - 1],\n            )\n\n        # Apply feedback\n        for key, conv in self.feedback_convs.items():\n            area_i, area_j = key.split(\"-&gt;\")\n            area_i, area_j = int(area_i), int(area_j)\n            feedback_states[area_j][t] = feedback_states[area_j][t] + conv(\n                output_states[area_i][t]\n            )\n\n    output_states, neuron_states, feedback_states = self._format_result(\n        output_states,  # type: ignore\n        neuron_states,  # type: ignore\n        feedback_states,  # type: ignore\n    )\n\n    return output_states, neuron_states, feedback_states\n</code></pre>"},{"location":"reference/eirnn/#src.bioplnn.models.spatially_embedded.SpatiallyEmbeddedRNN.init_feedback_states","title":"<code>init_feedback_states(batch_size, init_fn=None, device=None)</code>","text":"<p>Initializes the feedback inputs for all areas.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size.</p> required <code>init_fn</code> <code>Optional[Union[str, TensorInitFnType]]</code> <p>Initialization function.</p> <code>None</code> <code>device</code> <code>Optional[Union[device, str]]</code> <p>Device to allocate tensors.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Optional[Tensor]]</code> <p>A list of initialized feedback inputs for each area.</p> Source code in <code>src/bioplnn/models/spatially_embedded.py</code> <pre><code>def init_feedback_states(\n    self,\n    batch_size: int,\n    init_fn: Optional[Union[str, TensorInitFnType]] = None,\n    device: Optional[Union[torch.device, str]] = None,\n) -&gt; list[Optional[torch.Tensor]]:\n    \"\"\"Initializes the feedback inputs for all areas.\n\n    Args:\n        batch_size: Batch size.\n        init_fn: Initialization function.\n        device: Device to allocate tensors.\n\n    Returns:\n        A list of initialized feedback inputs for each area.\n    \"\"\"\n    return [\n        area.init_feedback_state(batch_size, init_fn, device)  # type: ignore\n        for area in self.areas\n    ]\n</code></pre>"},{"location":"reference/eirnn/#src.bioplnn.models.spatially_embedded.SpatiallyEmbeddedRNN.init_neuron_states","title":"<code>init_neuron_states(batch_size, init_fn=None, device=None)</code>","text":"<p>Initializes the hidden states for all areas.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size.</p> required <code>init_fn</code> <code>Optional[Union[str, TensorInitFnType]]</code> <p>Initialization function.</p> <code>None</code> <code>device</code> <code>Optional[Union[device, str]]</code> <p>Device to allocate tensors.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[list[Tensor]]</code> <p>A list containing the initialized neuron hidden states for each area.</p> Source code in <code>src/bioplnn/models/spatially_embedded.py</code> <pre><code>def init_neuron_states(\n    self,\n    batch_size: int,\n    init_fn: Optional[Union[str, TensorInitFnType]] = None,\n    device: Optional[Union[torch.device, str]] = None,\n) -&gt; list[list[torch.Tensor]]:\n    \"\"\"Initializes the hidden states for all areas.\n\n    Args:\n        batch_size: Batch size.\n        init_fn: Initialization function.\n        device: Device to allocate tensors.\n\n    Returns:\n        A list containing the initialized neuron hidden states for each area.\n    \"\"\"\n\n    return [\n        area.init_neuron_state(batch_size, init_fn, device)  # type: ignore\n        for area in self.areas\n    ]\n</code></pre>"},{"location":"reference/eirnn/#src.bioplnn.models.spatially_embedded.SpatiallyEmbeddedRNN.init_output_states","title":"<code>init_output_states(batch_size, init_fn=None, device=None)</code>","text":"<p>Initializes the outputs for all areas.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size.</p> required <code>init_fn</code> <code>Optional[Union[str, TensorInitFnType]]</code> <p>Initialization function.</p> <code>None</code> <code>device</code> <code>Optional[Union[device, str]]</code> <p>Device to allocate tensors.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Tensor]</code> <p>A list of initialized outputs for each area.</p> Source code in <code>src/bioplnn/models/spatially_embedded.py</code> <pre><code>def init_output_states(\n    self,\n    batch_size: int,\n    init_fn: Optional[Union[str, TensorInitFnType]] = None,\n    device: Optional[Union[torch.device, str]] = None,\n) -&gt; list[torch.Tensor]:\n    \"\"\"Initializes the outputs for all areas.\n\n    Args:\n        batch_size: Batch size.\n        init_fn: Initialization function.\n        device: Device to allocate tensors.\n\n    Returns:\n        A list of initialized outputs for each area.\n    \"\"\"\n\n    return [\n        area.init_output_state(batch_size, init_fn, device)  # type: ignore\n        for area in self.areas\n    ]\n</code></pre>"},{"location":"reference/eirnn/#src.bioplnn.models.spatially_embedded.SpatiallyEmbeddedRNN.init_states","title":"<code>init_states(out0, h_neuron0, fb0, num_steps, batch_size, out_init_fn=None, hidden_init_fn=None, fb_init_fn=None, device=None)</code>","text":"<p>Initializes the state of the network.</p> <p>Parameters:</p> Name Type Description Default <code>out0</code> <code>Optional[Sequence[Union[Tensor, None]]]</code> <p>Initial outputs for each area. If None, default initialization is used.</p> required <code>h_neuron0</code> <code>Optional[Sequence[Sequence[Union[Tensor, None]]]]</code> <p>Initial neuron hidden states for each area. If None, default initialization is used.</p> required <code>fb0</code> <code>Optional[Sequence[Union[Tensor, None]]]</code> <p>Initial feedback inputs for each area. If None, default initialization is used.</p> required <code>num_steps</code> <code>int</code> <p>Number of time steps.</p> required <code>batch_size</code> <code>int</code> <p>Batch size.</p> required <code>out_init_fn</code> <code>Optional[Union[str, TensorInitFnType]]</code> <p>Initialization function for outputs if out0 is None. Defaults to None.</p> <code>None</code> <code>hidden_init_fn</code> <code>Optional[Union[str, TensorInitFnType]]</code> <p>Initialization function for hidden states if h_neuron0 is None. Defaults to None.</p> <code>None</code> <code>fb_init_fn</code> <code>Optional[Union[str, TensorInitFnType]]</code> <p>Initialization function for feedback inputs if fb0 is None. Defaults to None.</p> <code>None</code> <code>device</code> <code>Optional[Union[device, str]]</code> <p>Device to allocate tensors on. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[list[Union[Tensor, None]]]</code> <p>A tuple containing:</p> <code>list[list[list[Union[Tensor, None]]]]</code> <ul> <li>Initialized outputs for each area and time step</li> </ul> <code>list[list[Union[Tensor, int, None]]]</code> <ul> <li>Initialized neuron hidden states for each area, time step, and neuron type</li> </ul> <code>tuple[list[list[Union[Tensor, None]]], list[list[list[Union[Tensor, None]]]], list[list[Union[Tensor, int, None]]]]</code> <ul> <li>Initialized feedback inputs for each area and time step</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the length of out0, h_neuron0, or fb0 doesn't match the number of areas.</p> Source code in <code>src/bioplnn/models/spatially_embedded.py</code> <pre><code>def init_states(\n    self,\n    out0: Optional[Sequence[Union[torch.Tensor, None]]],\n    h_neuron0: Optional[Sequence[Sequence[Union[torch.Tensor, None]]]],\n    fb0: Optional[Sequence[Union[torch.Tensor, None]]],\n    num_steps: int,\n    batch_size: int,\n    out_init_fn: Optional[Union[str, TensorInitFnType]] = None,\n    hidden_init_fn: Optional[Union[str, TensorInitFnType]] = None,\n    fb_init_fn: Optional[Union[str, TensorInitFnType]] = None,\n    device: Optional[Union[torch.device, str]] = None,\n) -&gt; tuple[\n    list[list[Union[torch.Tensor, None]]],\n    list[list[list[Union[torch.Tensor, None]]]],\n    list[list[Union[torch.Tensor, int, None]]],\n]:\n    \"\"\"Initializes the state of the network.\n\n    Args:\n        out0: Initial outputs for each area. If None, default initialization is used.\n        h_neuron0: Initial neuron hidden states for each area. If None, default\n            initialization is used.\n        fb0: Initial feedback inputs for each area. If None, default initialization is used.\n        num_steps: Number of time steps.\n        batch_size: Batch size.\n        out_init_fn: Initialization function for outputs if out0 is None. Defaults to None.\n        hidden_init_fn: Initialization function for hidden states if h_neuron0 is None.\n            Defaults to None.\n        fb_init_fn: Initialization function for feedback inputs if fb0 is None.\n            Defaults to None.\n        device: Device to allocate tensors on. Defaults to None.\n\n    Returns:\n        A tuple containing:\n        - Initialized outputs for each area and time step\n        - Initialized neuron hidden states for each area, time step, and neuron type\n        - Initialized feedback inputs for each area and time step\n\n    Raises:\n        ValueError: If the length of out0, h_neuron0, or fb0 doesn't match\n            the number of areas.\n    \"\"\"\n\n    # Validate input shapes of out0, h_neuron0, fb0\n    if out0 is None:\n        out0 = [None] * self.num_areas\n    elif len(out0) != self.num_areas:\n        raise ValueError(\n            \"The length of out0 must be equal to the number of areas.\"\n        )\n    if h_neuron0 is None:\n        h_neuron0 = [\n            [None] * self.areas[i].num_neuron_types  # type: ignore\n            for i in range(self.num_areas)\n        ]\n    elif len(h_neuron0) != self.num_areas:\n        raise ValueError(\n            \"The length of h_neuron0 must be equal to the number of areas.\"\n        )\n    if fb0 is None:\n        fb0 = [None] * self.num_areas\n    elif len(fb0) != self.num_areas:\n        raise ValueError(\n            \"The length of fb0 must be equal to the number of areas.\"\n        )\n\n    # Initialize default values\n    h_neuron0_default = self.init_neuron_states(\n        batch_size, hidden_init_fn, device\n    )\n    fb0_default = self.init_feedback_states(batch_size, fb_init_fn, device)\n    out0_default = self.init_output_states(batch_size, out_init_fn, device)\n\n    # Initialize output, hidden state, and feedback lists\n    outs: list[list[Union[torch.Tensor, None]]] = [\n        [None] * num_steps for _ in range(self.num_areas)\n    ]\n    h_neurons: list[list[list[Union[torch.Tensor, None]]]] = [\n        [[None] * self.areas[i].num_neuron_types for _ in range(num_steps)]  # type: ignore\n        for i in range(self.num_areas)\n    ]\n    fbs: list[list[Union[torch.Tensor, int, None]]] = [\n        [0 if self.areas[i].use_feedback else None] * num_steps\n        for i in range(self.num_areas)\n    ]\n\n    # Fill time step -1 with the initial values\n    for i in range(self.num_areas):\n        outs[i][-1] = out0[i] if out0[i] is not None else out0_default[i]\n        fbs[i][-1] = fb0[i] if fb0[i] is not None else fb0_default[i]\n        for k in range(self.areas[i].num_neuron_types):  # type: ignore\n            h_neurons[i][-1][k] = (\n                h_neuron0[i][k]\n                if h_neuron0[i][k] is not None\n                else h_neuron0_default[i][k]\n            )\n\n    return outs, h_neurons, fbs\n</code></pre>"},{"location":"reference/eirnn/#src.bioplnn.models.spatially_embedded.SpatiallyEmbeddedRNN.query_neuron_states","title":"<code>query_neuron_states(neuron_states, area, neuron_type, time_step=None, batch=None, neuron_subtype=None, spatial_location_i=None, spatial_location_j=None)</code>","text":"<p>Query the model states for a given area, time step, neuron type, neuron subtype, and spatial location.</p> <p>Parameters:</p> Name Type Description Default <code>neuron_states</code> <code>list[list[Tensor]]</code> <p>List of lists of tensors containing the model states.</p> required <code>area</code> <code>int</code> <p>The area index.</p> required <code>neuron_type</code> <code>int</code> <p>The neuron type index.</p> required <code>time_step</code> <code>Optional[Union[int, slice]]</code> <p>The time step index. If not provided, all time steps are returned.</p> <code>None</code> <code>batch</code> <code>Optional[Union[int, slice]]</code> <p>The batch index. If not provided, all batches are returned.</p> <code>None</code> <code>neuron_subtype</code> <code>Optional[Union[int, slice]]</code> <p>The neuron subtype index. If not provided, all neuron subtypes are returned.</p> <code>None</code> <code>spatial_location_i</code> <code>Optional[Union[int, slice]]</code> <p>The spatial location (height). If not provided, all spatial locations are returned.</p> <code>None</code> <code>spatial_location_j</code> <code>Optional[Union[int, slice]]</code> <p>The spatial location (width). If not provided, all spatial locations are returned.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The queried state of shape (batch_size_slice, channels, height, width)</p> Source code in <code>src/bioplnn/models/spatially_embedded.py</code> <pre><code>def query_neuron_states(\n    self,\n    neuron_states: list[list[torch.Tensor]],\n    area: int,\n    neuron_type: int,\n    time_step: Optional[Union[int, slice]] = None,\n    batch: Optional[Union[int, slice]] = None,\n    neuron_subtype: Optional[Union[int, slice]] = None,\n    spatial_location_i: Optional[Union[int, slice]] = None,\n    spatial_location_j: Optional[Union[int, slice]] = None,\n) -&gt; torch.Tensor:\n    \"\"\"Query the model states for a given area, time step, neuron type, neuron subtype, and spatial location.\n\n    Args:\n        neuron_states: List of lists of tensors containing the model states.\n        area: The area index.\n        neuron_type: The neuron type index.\n        time_step: The time step index. If not provided, all time steps are returned.\n        batch: The batch index. If not provided, all batches are returned.\n        neuron_subtype: The neuron subtype index. If not provided, all neuron subtypes are returned.\n        spatial_location_i: The spatial location (height). If not provided, all spatial locations are returned.\n        spatial_location_j: The spatial location (width). If not provided, all spatial locations are returned.\n\n    Returns:\n        The queried state of shape (batch_size_slice, channels, height, width)\n    \"\"\"\n    if time_step is None:\n        time_idx = slice(None)\n    else:\n        time_idx = time_step\n    if batch is None:\n        batch_idx = slice(None)\n    else:\n        batch_idx = batch\n    if neuron_subtype is None:\n        neuron_subtype_idx = slice(None)\n    else:\n        neuron_subtype_idx = neuron_subtype\n    if spatial_location_i is None:\n        spatial_location_idx_i = slice(None)\n    else:\n        spatial_location_idx_i = spatial_location_i\n    if spatial_location_j is None:\n        spatial_location_idx_j = slice(None)\n    else:\n        spatial_location_idx_j = spatial_location_j\n\n    if self.batch_first:\n        return neuron_states[area][neuron_type][\n            batch_idx,\n            time_idx,\n            neuron_subtype_idx,\n            spatial_location_idx_i,\n            spatial_location_idx_j,\n        ]\n    else:\n        return neuron_states[area][neuron_type][\n            time_idx,\n            batch_idx,\n            neuron_subtype_idx,\n            spatial_location_idx_i,\n            spatial_location_idx_j,\n        ]\n</code></pre>"},{"location":"reference/misc/","title":"Miscellaneous utilities","text":""},{"location":"reference/misc/#src.bioplnn.utils","title":"<code>utils</code>","text":""},{"location":"reference/misc/#src.bioplnn.utils.check_possible_values","title":"<code>check_possible_values(param_name, params, valid_values)</code>","text":"<p>Check if the provided parameters are all valid values.</p> <p>Parameters:</p> Name Type Description Default <code>param_name</code> <code>str</code> <p>The name of the parameter (for error message).</p> required <code>params</code> <code>Iterable</code> <p>The parameters to check.</p> required <code>valid_values</code> <code>Iterable</code> <p>The valid values to check against.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If any of the parameters are not one of the valid values.</p> Source code in <code>src/bioplnn/utils/common.py</code> <pre><code>def check_possible_values(\n    param_name: str,\n    params: Union[Iterable, NDArray[Any], torch.Tensor],\n    valid_values: Union[Iterable, NDArray[Any], torch.Tensor],\n) -&gt; None:\n    \"\"\"Check if the provided parameters are all valid values.\n\n    Args:\n        param_name (str): The name of the parameter (for error message).\n        params (Iterable): The parameters to check.\n        valid_values (Iterable): The valid values to check against.\n\n    Raises:\n        ValueError: If any of the parameters are not one of the valid values.\n    \"\"\"\n    if isinstance(params, torch.Tensor):\n        params = set(torch.unique(params).tolist())\n    elif isinstance(params, np.ndarray):\n        params = set(np.unique(params).tolist())\n    else:\n        params = set(params)\n\n    if isinstance(valid_values, torch.Tensor):\n        valid_values = set(torch.unique(valid_values).tolist())\n    elif isinstance(valid_values, np.ndarray):\n        valid_values = set(np.unique(valid_values).tolist())\n    else:\n        valid_values = set(valid_values)\n\n    if not params &lt;= valid_values:\n        raise ValueError(f\"{param_name} must be one of {valid_values}.\")\n</code></pre>"},{"location":"reference/misc/#src.bioplnn.utils.count_parameters","title":"<code>count_parameters(model)</code>","text":"<p>Count the number of trainable parameters in a model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>PyTorch model.</p> required <p>Returns:</p> Name Type Description <code>int</code> <p>Number of trainable parameters.</p> Source code in <code>src/bioplnn/utils/torch.py</code> <pre><code>def count_parameters(model):\n    \"\"\"Count the number of trainable parameters in a model.\n\n    Args:\n        model: PyTorch model.\n\n    Returns:\n        int: Number of trainable parameters.\n    \"\"\"\n    total_params = 0\n    for param in model.parameters():\n        num_params = (\n            param._nnz()\n            if param.layout\n            in (torch.sparse_coo, torch.sparse_csr, torch.sparse_csc)\n            else param.numel()\n        )\n        total_params += num_params\n    return total_params\n</code></pre>"},{"location":"reference/misc/#src.bioplnn.utils.create_sparse_projection","title":"<code>create_sparse_projection(size, num_neurons, indices=None, mode='ih')</code>","text":"<p>Create identity connectivity for input-to-hidden or hidden-to-output connections.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>Size of the input or output.</p> required <code>num_neurons</code> <code>int</code> <p>Number of neurons in the hidden layer.</p> required <code>indices</code> <code>Union[Tensor, PathLike]</code> <p>Indices of neurons that receive input. If None, all neurons receive input from corresponding input indices. Defaults to None.</p> <code>None</code> <code>mode</code> <code>Literal['ih', 'ho']</code> <p>Whether to create input-to-hidden or hidden-to-output connectivity (only changes the orientation of the connectivity matrix). Defaults to \"ih\".</p> <code>'ih'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Sparse connectivity matrix in COO format.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input_indices are invalid.</p> Source code in <code>src/bioplnn/utils/torch.py</code> <pre><code>def create_sparse_projection(\n    size: int,\n    num_neurons: int,\n    indices: Optional[Union[torch.Tensor, PathLikeType]] = None,\n    mode: Literal[\"ih\", \"ho\"] = \"ih\",\n) -&gt; torch.Tensor:\n    \"\"\"Create identity connectivity for input-to-hidden or hidden-to-output\n    connections.\n\n    Args:\n        size (int): Size of the input or output.\n        num_neurons (int): Number of neurons in the hidden layer.\n        indices (Union[torch.Tensor, PathLike], optional): Indices of\n            neurons that receive input. If None, all neurons receive input from\n            corresponding input indices. Defaults to None.\n        mode (Literal[\"ih\", \"ho\"], optional): Whether to create input-to-hidden\n            or hidden-to-output connectivity (only changes the orientation of\n            the connectivity matrix). Defaults to \"ih\".\n\n    Returns:\n        torch.Tensor: Sparse connectivity matrix in COO format.\n\n    Raises:\n        ValueError: If input_indices are invalid.\n    \"\"\"\n\n    # Generate identity connectivity for input-to-hidden connections\n    if indices is None:\n        indices = torch.arange(size)\n\n    if mode == \"ih\":\n        indices = torch.stack((indices, torch.arange(size)))  # type: ignore\n        shape = (num_neurons, size)\n    else:\n        indices = torch.stack((torch.arange(size), indices))  # type: ignore\n        shape = (size, num_neurons)\n\n    values = torch.ones(indices.shape[1])\n\n    connectivity = torch.sparse_coo_tensor(\n        indices,\n        values,\n        shape,\n        check_invariants=True,\n    ).coalesce()\n\n    return connectivity\n</code></pre>"},{"location":"reference/misc/#src.bioplnn.utils.create_sparse_topographic_connectome","title":"<code>create_sparse_topographic_connectome(sheet_size, synapse_std, synapses_per_neuron, self_recurrence)</code>","text":"<p>Create random topographic hidden-to-hidden connectivity.</p> <p>Parameters:</p> Name Type Description Default <code>sheet_size</code> <code>tuple[int, int]</code> <p>Size of the sheet-like neural layer (rows, columns).</p> required <code>synapse_std</code> <code>float</code> <p>Standard deviation of the Gaussian distribution for sampling synapse connections.</p> required <code>synapses_per_neuron</code> <code>int</code> <p>Number of incoming synapses per neuron.</p> required <code>self_recurrence</code> <code>bool</code> <p>Whether neurons can connect to themselves.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Sparse connectivity matrix in COO format.</p> Source code in <code>src/bioplnn/utils/torch.py</code> <pre><code>def create_sparse_topographic_connectome(\n    sheet_size: tuple[int, int],\n    synapse_std: float,\n    synapses_per_neuron: int,\n    self_recurrence: bool,\n) -&gt; torch.Tensor:\n    \"\"\"Create random topographic hidden-to-hidden connectivity.\n\n    Args:\n        sheet_size (tuple[int, int]): Size of the sheet-like neural layer (rows,\n            columns).\n        synapse_std (float): Standard deviation of the Gaussian distribution for\n            sampling synapse connections.\n        synapses_per_neuron (int): Number of incoming synapses per neuron.\n        self_recurrence (bool): Whether neurons can connect to themselves.\n\n    Returns:\n        torch.Tensor: Sparse connectivity matrix in COO format.\n    \"\"\"\n    # Generate random connectivity for hidden-to-hidden connections\n    num_neurons = sheet_size[0] * sheet_size[1]\n\n    idx_1d = torch.arange(num_neurons)\n    idx = idx_1D_to_2D_tensor(idx_1d, sheet_size[0], sheet_size[1]).t()\n\n    synapses = (\n        torch.randn(num_neurons, 2, synapses_per_neuron) * synapse_std\n        + idx.unsqueeze(-1)\n    ).long()\n\n    if self_recurrence:\n        synapses = torch.cat([synapses, idx.unsqueeze(-1)], dim=2)\n\n    synapses = synapses.clamp(\n        torch.zeros(2).view(1, 2, 1),\n        torch.tensor((sheet_size[0] - 1, sheet_size[1] - 1)).view(1, 2, 1),\n    )\n    synapses = idx_2D_to_1D_tensor(\n        synapses.transpose(0, 1).flatten(1), sheet_size[0], sheet_size[1]\n    ).view(num_neurons, -1)\n\n    synapse_root = idx_1d.unsqueeze(-1).expand(-1, synapses.shape[1])\n\n    indices_hh = torch.stack((synapses, synapse_root)).flatten(1)\n\n    ## He initialization of values (synapses_per_neuron is the fan_in)\n    values_hh = torch.randn(indices_hh.shape[1]) * math.sqrt(\n        2 / synapses_per_neuron\n    )\n\n    connectivity_hh = torch.sparse_coo_tensor(\n        indices_hh,\n        values_hh,\n        (num_neurons, num_neurons),\n        check_invariants=True,\n    ).coalesce()\n\n    return connectivity_hh\n</code></pre>"},{"location":"reference/misc/#src.bioplnn.utils.dict_flatten","title":"<code>dict_flatten(d, delimiter='.', key=None)</code>","text":"<p>Flattens a nested dictionary into a single-level dictionary.</p> <p>Keys of the flattened dictionary will be the path to the value, with path components joined by delimiter.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict</code> <p>Dictionary to flatten.</p> required <code>delimiter</code> <code>str</code> <p>String to join key path components. Defaults to \".\".</p> <code>'.'</code> <code>key</code> <code>str</code> <p>Current key prefix. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>Flattened dictionary.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If flattening would result in duplicate keys.</p> Source code in <code>src/bioplnn/utils/common.py</code> <pre><code>def dict_flatten(d, delimiter=\".\", key=None):\n    \"\"\"Flattens a nested dictionary into a single-level dictionary.\n\n    Keys of the flattened dictionary will be the path to the value, with path\n    components joined by delimiter.\n\n    Args:\n        d (dict): Dictionary to flatten.\n        delimiter (str, optional): String to join key path components.\n            Defaults to \".\".\n        key (str, optional): Current key prefix. Defaults to None.\n\n    Returns:\n        dict: Flattened dictionary.\n\n    Raises:\n        ValueError: If flattening would result in duplicate keys.\n    \"\"\"\n    key = f\"{key}{delimiter}\" if key is not None else \"\"\n    non_dicts = {\n        f\"{key}{k}\": v for k, v in d.items() if not isinstance(v, dict)\n    }\n    dicts = {\n        f\"{key}{k}\": v\n        for _k, _v in d.items()\n        if isinstance(_v, dict)\n        for k, v in dict_flatten(_v, delimiter=delimiter, key=_k).items()\n    }\n\n    if in_both := dicts.keys() &amp; non_dicts.keys():\n        if len(in_both) &gt; 1:\n            raise ValueError(\n                f\"flattened keys {list(in_both)} used more than once in dict\"\n            )\n        else:\n            raise ValueError(\n                f\"flattened key {list(in_both)[0]} used more than once in dict\"\n            )\n\n    return {**non_dicts, **dicts}\n</code></pre>"},{"location":"reference/misc/#src.bioplnn.utils.expand_array_2d","title":"<code>expand_array_2d(x, m, n, depth=0)</code>","text":"<p>Expands a value to a 2D numpy array of shape (m, n).</p> <p>Use depth &gt; 0 if the intended type T can be indexed recursively, where depth is the maximum number of times x can be recursively indexed if of type T. For example, if x is a shallow list, then depth = 1. If x is a list of lists or an array or tensor, then depth = 2.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Any</code> <p>The variable to expand.</p> required <code>m</code> <code>int</code> <p>The number of rows in the expanded array.</p> required <code>n</code> <code>int</code> <p>The number of columns in the expanded array.</p> required <code>depth</code> <code>int</code> <p>The depth x can be recursively indexed. A depth of -1 will assume x is of type list[T] and check if x is already of the correct shape.</p> <code>0</code> <p>Returns:</p> Type Description <code>NDArray[Any]</code> <p>np.ndarray: Expanded 2D numpy array.</p> Source code in <code>src/bioplnn/utils/common.py</code> <pre><code>def expand_array_2d(\n    x: Optional[ScalarOrArray2dType[T]], m: int, n: int, depth: int = 0\n) -&gt; NDArray[Any]:\n    \"\"\"Expands a value to a 2D numpy array of shape (m, n).\n\n    Use depth &gt; 0 if the intended type T can be indexed recursively, where\n    depth is the maximum number of times x can be recursively indexed if of\n    type T. For example, if x is a shallow list, then depth = 1. If x is a\n    list of lists or an array or tensor, then depth = 2.\n\n    Args:\n        x (Any): The variable to expand.\n        m (int): The number of rows in the expanded array.\n        n (int): The number of columns in the expanded array.\n        depth (int, optional): The depth x can be recursively indexed. A depth\n            of -1 will assume x is of type list[T] and check if x is already of\n            the correct shape.\n\n    Returns:\n        np.ndarray: Expanded 2D numpy array.\n    \"\"\"\n\n    if m &lt; 1 or n &lt; 1:\n        raise ValueError(\"m and n must be at least 1.\")\n\n    inner = x\n    try:\n        for _ in range(depth + 2):\n            inner = inner[0]  # type: ignore\n    except TypeError:\n        array = np.empty((m, n), dtype=object)\n        for i in range(m):\n            for j in range(n):\n                array[i, j] = x\n        return array\n\n    if x is None:\n        assert depth == -1\n        raise ValueError(\"x cannot be None if depth is -1.\")\n\n    x = np.array(x, dtype=object)\n\n    if x.shape != (m, n):\n        raise ValueError(f\"x must have shape ({m}, {n}).\")\n\n    return x\n</code></pre>"},{"location":"reference/misc/#src.bioplnn.utils.expand_list","title":"<code>expand_list(x, n, depth=0)</code>","text":"<p>Expands a value to a list of length n.</p> <p>If x is already a list, then the list is returned unchanged.</p> <p>If x is not a list, then x is expanded to a list of length n.</p> <p>Use depth &gt; 0 if the intended type T can be indexed recursively, where depth is the maximum number of times x can be recursively indexed if of type T. For example, if x is a shallow list, then depth = 1. If T is a list of lists or an array or tensor, then depth = 2.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Any</code> <p>The variable to expand.</p> required <code>n</code> <code>int</code> <p>The number of lists or tuples to expand to.</p> required <code>depth</code> <code>int</code> <p>The depth x can be recursively indexed. A depth of -1 will assume x is of type list[T] and check if x is already of the correct length. Defaults to 0.</p> <code>0</code> <p>Returns:     list[Any]: Expanded list.</p> Source code in <code>src/bioplnn/utils/common.py</code> <pre><code>def expand_list(\n    x: Optional[ScalarOrListLike[T]], n: int, depth: int = 0\n) -&gt; Union[list[T], NDArray[Any]]:\n    \"\"\"Expands a value to a list of length n.\n\n    If x is already a list, then the list is returned unchanged.\n\n    If x is not a list, then x is expanded to a list of length n.\n\n    Use depth &gt; 0 if the intended type T can be indexed recursively, where\n    depth is the maximum number of times x can be recursively indexed if of\n    type T. For example, if x is a shallow list, then depth = 1. If T is a\n    list of lists or an array or tensor, then depth = 2.\n\n    Args:\n        x (Any): The variable to expand.\n        n (int): The number of lists or tuples to expand to.\n        depth (int, optional): The depth x can be recursively indexed. A depth\n            of -1 will assume x is of type list[T] and check if x is already of\n            the correct length. Defaults to 0.\n    Returns:\n        list[Any]: Expanded list.\n    \"\"\"\n\n    if n &lt; 1:\n        raise ValueError(\"n must be at least 1.\")\n\n    inner = x\n    try:\n        for _ in range(depth + 1):\n            if isinstance(inner, str):\n                raise TypeError\n            inner = inner[0]  # type: ignore\n    except (IndexError, TypeError):\n        return [x] * n  # type: ignore\n\n    if x is None:\n        assert depth == -1\n        raise ValueError(\"x cannot be None if depth is -1.\")\n\n    if len(x) != n:  # type: ignore\n        raise ValueError(f\"x must have length {n}.\")\n\n    return x  # type: ignore\n</code></pre>"},{"location":"reference/misc/#src.bioplnn.utils.get_activation","title":"<code>get_activation(activation)</code>","text":"<p>Get an initialized activation function module.</p> <p>Parameters:</p> Name Type Description Default <code>activation</code> <code>Union[str, Module]</code> <p>The name(s) of the activation function(s) or an already initialized nn.Module. If the latter, the moduleis returned as is. If None, returns nn.Identity().</p> required <p>Returns:</p> Type Description <code>Module</code> <p>nn.Module: The initialized activation function.</p> Source code in <code>src/bioplnn/utils/torch.py</code> <pre><code>def get_activation(\n    activation: Union[str, None, nn.Module],\n) -&gt; nn.Module:\n    \"\"\"Get an initialized activation function module.\n\n    Args:\n        activation (Union[str, nn.Module], optional): The name(s) of the\n            activation function(s) or an already initialized nn.Module. If the\n            latter, the moduleis returned as is. If None, returns nn.Identity().\n\n    Returns:\n        nn.Module: The initialized activation function.\n    \"\"\"\n    if isinstance(activation, nn.Module):\n        return activation\n    activation_classes = get_activation_class(activation)\n    if isinstance(activation_classes, list):\n        return nn.Sequential(*[act() for act in activation_classes])\n    else:\n        return activation_classes()\n</code></pre>"},{"location":"reference/misc/#src.bioplnn.utils.get_activation_class","title":"<code>get_activation_class(activation)</code>","text":"<p>Get one or more activation function classes.</p> <p>If activation is a string with commas, split and get each activation.</p> <p>Parameters:</p> Name Type Description Default <code>activation</code> <code>str</code> <p>The name(s) of the activation function(s). If None, returns nn.Identity. Defaults to None.</p> required <p>Returns:</p> Type Description <code>Union[Type[Module], list[Type[Module]]]</code> <p>Union[Type[nn.Module], List[Type[nn.Module]]]: A single activation class or a list of activation classes if comma-separated.</p> Source code in <code>src/bioplnn/utils/torch.py</code> <pre><code>def get_activation_class(\n    activation: Union[str, None],\n) -&gt; Union[Type[nn.Module], list[Type[nn.Module]]]:\n    \"\"\"Get one or more activation function classes.\n\n    If activation is a string with commas, split and get each activation.\n\n    Args:\n        activation (str, optional): The name(s) of the activation function(s).\n            If None, returns nn.Identity. Defaults to None.\n\n    Returns:\n        Union[Type[nn.Module], List[Type[nn.Module]]]: A single activation class\n            or a list of activation classes if comma-separated.\n    \"\"\"\n    if activation is None:\n        return nn.Identity\n    activations = [act.strip() for act in activation.split(\",\")]\n    if len(activations) == 1:\n        return _get_single_activation_class(activations[0])\n    else:\n        return [_get_single_activation_class(act) for act in activations]\n</code></pre>"},{"location":"reference/misc/#src.bioplnn.utils.idx_1D_to_2D_tensor","title":"<code>idx_1D_to_2D_tensor(x, m, n)</code>","text":"<p>Convert 1D indices to 2D coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>1D indices tensor.</p> required <code>m</code> <code>int</code> <p>Number of rows in the 2D grid.</p> required <code>n</code> <code>int</code> <p>Number of columns in the 2D grid.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: 2D coordinates tensor of shape (len(x), 2).</p> Source code in <code>src/bioplnn/utils/torch.py</code> <pre><code>def idx_1D_to_2D_tensor(x: torch.Tensor, m: int, n: int) -&gt; torch.Tensor:\n    \"\"\"Convert 1D indices to 2D coordinates.\n\n    Args:\n        x (torch.Tensor): 1D indices tensor.\n        m (int): Number of rows in the 2D grid.\n        n (int): Number of columns in the 2D grid.\n\n    Returns:\n        torch.Tensor: 2D coordinates tensor of shape (len(x), 2).\n    \"\"\"\n    return torch.stack((x // m, x % n))\n</code></pre>"},{"location":"reference/misc/#src.bioplnn.utils.idx_2D_to_1D_tensor","title":"<code>idx_2D_to_1D_tensor(x, m, n)</code>","text":"<p>Convert 2D coordinates to 1D indices.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>2D coordinates tensor of shape (N, 2).</p> required <code>m</code> <code>int</code> <p>Number of rows in the 2D grid.</p> required <code>n</code> <code>int</code> <p>Number of columns in the 2D grid.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: 1D indices tensor.</p> Source code in <code>src/bioplnn/utils/torch.py</code> <pre><code>def idx_2D_to_1D_tensor(x: torch.Tensor, m: int, n: int) -&gt; torch.Tensor:\n    \"\"\"Convert 2D coordinates to 1D indices.\n\n    Args:\n        x (torch.Tensor): 2D coordinates tensor of shape (N, 2).\n        m (int): Number of rows in the 2D grid.\n        n (int): Number of columns in the 2D grid.\n\n    Returns:\n        torch.Tensor: 1D indices tensor.\n    \"\"\"\n    return x[0] * n + x[1]\n</code></pre>"},{"location":"reference/misc/#src.bioplnn.utils.init_tensor","title":"<code>init_tensor(init_fn, *args, **kwargs)</code>","text":"<p>Initialize a tensor with a specified initialization function.</p> <p>Parameters:</p> Name Type Description Default <code>init_fn</code> <code>Union[str, TensorInitFnType]</code> <p>The initialization function name or callable.</p> required <code>*args</code> <p>Arguments to pass to the initialization function (usually shape).</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments to pass to the initialization function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The initialized tensor.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the initialization function is not supported.</p> Source code in <code>src/bioplnn/utils/torch.py</code> <pre><code>def init_tensor(\n    init_fn: Union[str, TensorInitFnType], *args, **kwargs\n) -&gt; torch.Tensor:\n    \"\"\"Initialize a tensor with a specified initialization function.\n\n    Args:\n        init_fn (Union[str, TensorInitFnType]): The initialization function name\n            or callable.\n        *args: Arguments to pass to the initialization function (usually shape).\n        **kwargs: Keyword arguments to pass to the initialization function.\n\n    Returns:\n        torch.Tensor: The initialized tensor.\n\n    Raises:\n        ValueError: If the initialization function is not supported.\n    \"\"\"\n\n    if isinstance(init_fn, str):\n        if init_fn == \"zeros\":\n            return torch.zeros(*args, **kwargs)\n        elif init_fn == \"ones\":\n            return torch.ones(*args, **kwargs)\n        elif init_fn == \"randn\":\n            return torch.randn(*args, **kwargs)\n        elif init_fn == \"rand\":\n            return torch.rand(*args, **kwargs)\n        else:\n            raise ValueError(\n                \"Invalid initialization function string. Must be 'zeros', \"\n                \"'ones', 'randn', or 'rand'.\"\n            )\n\n    try:\n        return init_fn(*args, **kwargs)\n    except TypeError as e:\n        if \"device\" in kwargs:\n            return init_fn(*args, **kwargs).to(kwargs[\"device\"])\n        else:\n            raise e\n</code></pre>"},{"location":"reference/misc/#src.bioplnn.utils.initialize_connectome","title":"<code>initialize_connectome(num_neurons, cell_type_probs, celltype_connectivity, deterministic_type_assignment=False)</code>","text":"<p>Initialize a synthetic connectome as a sparse adjacency matrix.</p> <p>Parameters:</p> Name Type Description Default <code>num_neurons</code> <code>int</code> <p>Total number of neurons.</p> required <code>cell_type_probs</code> <code>ndarray</code> <p>Proportion of each cell type, summing to 1.</p> required <code>celltype_connectivity</code> <code>ndarray</code> <p>Cell-type to cell-type connectivity probabilities.</p> required <code>deterministic_type_assignment</code> <code>bool</code> <p>If True, assigns neurons deterministically based on cell_type_probs interpreted as exact counts rather than probabilities. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[Tensor, ndarray]</code> <p>tuple[torch.sparse.FloatTensor, np.ndarray]: Sparse adjacency matrix of neuron-neuron connections and the assigned neuron types.</p> Source code in <code>src/bioplnn/utils/torch.py</code> <pre><code>def initialize_connectome(\n    num_neurons: int,\n    cell_type_probs: np.ndarray,\n    celltype_connectivity: np.ndarray,\n    deterministic_type_assignment: bool = False,\n) -&gt; tuple[torch.Tensor, np.ndarray]:\n    \"\"\"Initialize a synthetic connectome as a sparse adjacency matrix.\n\n    Args:\n        num_neurons (int): Total number of neurons.\n        cell_type_probs (np.ndarray): Proportion of each cell type, summing to 1.\n        celltype_connectivity (np.ndarray): Cell-type to cell-type connectivity probabilities.\n        deterministic_type_assignment (bool, optional): If True, assigns neurons deterministically based on cell_type_probs\n            interpreted as exact counts rather than probabilities. Defaults to False.\n\n    Returns:\n        tuple[torch.sparse.FloatTensor, np.ndarray]: Sparse adjacency matrix of neuron-neuron connections\n            and the assigned neuron types.\n    \"\"\"\n\n    # Determine number of cell types\n    num_cell_types = len(cell_type_probs)\n\n    # Assign cell types to neurons\n    if deterministic_type_assignment:\n        neuron_counts = (np.array(cell_type_probs) * num_neurons).astype(int)\n        neuron_types = np.concatenate(\n            [\n                np.full(count, i, dtype=int)\n                for i, count in enumerate(neuron_counts)\n            ]\n        )\n        np.random.shuffle(neuron_types)  # Shuffle to avoid ordering bias\n    else:\n        neuron_types = np.random.choice(\n            num_cell_types, size=num_neurons, p=cell_type_probs\n        )\n\n    # Generate all possible neuron-neuron pairs\n    row_indices, col_indices = np.meshgrid(\n        np.arange(num_neurons), np.arange(num_neurons), indexing=\"ij\"\n    )\n    row_indices = row_indices.flatten()\n    col_indices = col_indices.flatten()\n\n    # Get corresponding cell-type pairs\n    src_types = neuron_types[row_indices]\n    tgt_types = neuron_types[col_indices]\n\n    # Get connection probabilities from the connectivity matrix\n    probs = celltype_connectivity[src_types, tgt_types]\n\n    # Sample connections\n    mask = np.random.rand(len(probs)) &lt; probs\n    row_indices = row_indices[mask]\n    col_indices = col_indices[mask]\n\n    # Create sparse adjacency matrix\n    indices = torch.tensor([row_indices, col_indices], dtype=torch.long)\n    values = torch.ones(len(row_indices), dtype=torch.float)\n\n    sparse_adj = torch.sparse_coo_tensor(\n        indices,\n        values,\n        (num_neurons, num_neurons),\n        check_invariants=True,\n    ).coalesce()\n    return sparse_adj, neuron_types\n</code></pre>"},{"location":"reference/misc/#src.bioplnn.utils.initialize_criterion","title":"<code>initialize_criterion(*, class_name, **kwargs)</code>","text":"<p>Initialize a loss criterion.</p> <p>Parameters:</p> Name Type Description Default <code>class_name</code> <code>str</code> <p>The name of the criterion class to use.</p> required <code>**kwargs</code> <p>Additional keyword arguments to pass to the criterion.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Module</code> <p>torch.nn.Module: The initialized criterion.</p> Source code in <code>src/bioplnn/utils/initializers.py</code> <pre><code>def initialize_criterion(*, class_name: str, **kwargs) -&gt; torch.nn.Module:\n    \"\"\"Initialize a loss criterion.\n\n    Args:\n        class_name (str): The name of the criterion class to use.\n        **kwargs: Additional keyword arguments to pass to the criterion.\n\n    Returns:\n        torch.nn.Module: The initialized criterion.\n    \"\"\"\n    return getattr(torch.nn, class_name)(**kwargs)\n</code></pre>"},{"location":"reference/misc/#src.bioplnn.utils.initialize_dataloader","title":"<code>initialize_dataloader(*, dataset, seed=None, **kwargs)</code>","text":"<p>Initialize a dataloader for a given dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>str</code> <p>The dataset to use.</p> required <code>seed</code> <code>int</code> <p>The seed to use for the dataloader. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the dataloader.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[DataLoader, DataLoader]</code> <p>tuple[torch.utils.data.DataLoader, torch.utils.data.DataLoader]: The train and validation dataloaders.</p> Source code in <code>src/bioplnn/utils/initializers.py</code> <pre><code>def initialize_dataloader(\n    *, dataset: str, seed: Optional[int] = None, **kwargs\n) -&gt; tuple[torch.utils.data.DataLoader, torch.utils.data.DataLoader]:\n    \"\"\"Initialize a dataloader for a given dataset.\n\n    Args:\n        dataset (str): The dataset to use.\n        seed (int, optional): The seed to use for the dataloader. Defaults to None.\n        **kwargs: Additional keyword arguments to pass to the dataloader.\n\n    Returns:\n        tuple[torch.utils.data.DataLoader, torch.utils.data.DataLoader]:\n            The train and validation dataloaders.\n    \"\"\"\n\n    return getattr(dataloaders, f\"get_{dataset}_dataloaders\")(\n        **kwargs, seed=seed\n    )\n</code></pre>"},{"location":"reference/misc/#src.bioplnn.utils.initialize_model","title":"<code>initialize_model(*, class_name, **kwargs)</code>","text":"<p>Initialize a model based on the class name.</p> <p>Parameters:</p> Name Type Description Default <code>class_name</code> <code>str</code> <p>The name of the model class to use.</p> required <code>**kwargs</code> <p>Additional keyword arguments to pass to the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Module</code> <p>nn.Module: The initialized model.</p> Source code in <code>src/bioplnn/utils/initializers.py</code> <pre><code>def initialize_model(*, class_name: str, **kwargs) -&gt; nn.Module:\n    \"\"\"Initialize a model based on the class name.\n\n    Args:\n        class_name (str): The name of the model class to use.\n        **kwargs: Additional keyword arguments to pass to the model.\n\n    Returns:\n        nn.Module: The initialized model.\n    \"\"\"\n    import bioplnn.models\n\n    return getattr(bioplnn.models, class_name)(**kwargs)\n</code></pre>"},{"location":"reference/misc/#src.bioplnn.utils.initialize_optimizer","title":"<code>initialize_optimizer(*, class_name, model_parameters, **kwargs)</code>","text":"<p>Initialize an optimizer for model training.</p> <p>Parameters:</p> Name Type Description Default <code>class_name</code> <code>str</code> <p>The name of the optimizer class to use.</p> required <code>model_parameters</code> <code>ParameterList</code> <p>The model parameters to optimize.</p> required <code>**kwargs</code> <p>Additional keyword arguments to pass to the optimizer.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optimizer</code> <p>torch.optim.Optimizer: The initialized optimizer.</p> Source code in <code>src/bioplnn/utils/initializers.py</code> <pre><code>def initialize_optimizer(\n    *, class_name: str, model_parameters: Iterator[nn.Parameter], **kwargs\n) -&gt; torch.optim.Optimizer:\n    \"\"\"Initialize an optimizer for model training.\n\n    Args:\n        class_name (str): The name of the optimizer class to use.\n        model_parameters (nn.ParameterList): The model parameters to optimize.\n        **kwargs: Additional keyword arguments to pass to the optimizer.\n\n    Returns:\n        torch.optim.Optimizer: The initialized optimizer.\n    \"\"\"\n    return getattr(torch.optim, class_name)(model_parameters, **kwargs)\n</code></pre>"},{"location":"reference/misc/#src.bioplnn.utils.initialize_scheduler","title":"<code>initialize_scheduler(*, class_name, optimizer, **kwargs)</code>","text":"<p>Initialize a learning rate scheduler.</p> <p>Parameters:</p> Name Type Description Default <code>class_name</code> <code>str</code> <p>The name of the scheduler class to use.</p> required <code>optimizer</code> <code>Optimizer</code> <p>The optimizer to schedule.</p> required <code>**kwargs</code> <p>Additional keyword arguments to pass to the scheduler.</p> <code>{}</code> <p>Returns:</p> Type Description <code>LRScheduler</code> <p>torch.optim.lr_scheduler.LRScheduler: The initialized scheduler.</p> Source code in <code>src/bioplnn/utils/initializers.py</code> <pre><code>def initialize_scheduler(\n    *, class_name: str, optimizer: torch.optim.Optimizer, **kwargs\n) -&gt; torch.optim.lr_scheduler.LRScheduler:\n    \"\"\"Initialize a learning rate scheduler.\n\n    Args:\n        class_name (str): The name of the scheduler class to use.\n        optimizer (torch.optim.Optimizer): The optimizer to schedule.\n        **kwargs: Additional keyword arguments to pass to the scheduler.\n\n    Returns:\n        torch.optim.lr_scheduler.LRScheduler: The initialized scheduler.\n    \"\"\"\n    return getattr(torch.optim.lr_scheduler, class_name)(optimizer, **kwargs)\n</code></pre>"},{"location":"reference/misc/#src.bioplnn.utils.is_list_like","title":"<code>is_list_like(x)</code>","text":"<p>Determines if an object is list-like (iterable but not a string or mapping).</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Any</code> <p>Object to check.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the object is list-like, False otherwise.</p> Source code in <code>src/bioplnn/utils/common.py</code> <pre><code>def is_list_like(x: Any) -&gt; bool:\n    \"\"\"Determines if an object is list-like (iterable but not a string or mapping).\n\n    Args:\n        x (Any): Object to check.\n\n    Returns:\n        bool: True if the object is list-like, False otherwise.\n    \"\"\"\n    if isinstance(x, (str, Mapping)):\n        return False\n    try:\n        iter(x)\n        if len(x) &gt; 0:\n            x[0]\n    except Exception:\n        return False\n    return True\n</code></pre>"},{"location":"reference/misc/#src.bioplnn.utils.load_array","title":"<code>load_array(array)</code>","text":"<p>Load a numpy array from an array, iterable, or file.</p> <p>Supported file formats: - npz - npy - csv - pt</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>Union[ndarray, Tensor, DataFrame, Iterable[Any], PathLikeType]</code> <p>numpy array, iterable, or path to file containing numpy array.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The original or loaded numpy array.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the array cannot be loaded from the given file or iterable.</p> Source code in <code>src/bioplnn/utils/torch.py</code> <pre><code>def load_array(\n    array: Union[\n        np.ndarray, torch.Tensor, pd.DataFrame, Iterable[Any], PathLikeType\n    ],\n) -&gt; np.ndarray:\n    \"\"\"Load a numpy array from an array, iterable, or file.\n\n    Supported file formats:\n    - npz\n    - npy\n    - csv\n    - pt\n\n    Args:\n        array: numpy array, iterable, or path to file containing numpy array.\n\n    Returns:\n        The original or loaded numpy array.\n\n    Raises:\n        ValueError: If the array cannot be loaded from the given file or iterable.\n    \"\"\"\n\n    if isinstance(array, PathLikeType):\n        return _load_array_from_file(array)\n    return _load_array_from_iterable(array)\n</code></pre>"},{"location":"reference/misc/#src.bioplnn.utils.load_sparse_tensor","title":"<code>load_sparse_tensor(x)</code>","text":"<p>Load a torch tensor from an array, iterable, or file.</p> Source code in <code>src/bioplnn/utils/torch.py</code> <pre><code>def load_sparse_tensor(\n    x: Union[\n        torch.Tensor, pd.DataFrame, np.ndarray, Iterable[Any], PathLikeType\n    ],\n) -&gt; torch.Tensor:\n    \"\"\"Load a torch tensor from an array, iterable, or file.\"\"\"\n\n    if isinstance(x, PathLikeType):\n        x = _load_tensor_from_file(x)\n    else:\n        x = _load_tensor_from_iterable(x)\n\n    x = x.to_sparse()\n\n    if x._nnz() &gt; SPARSE_TENSOR_WARNING_THRESHOLD * x.numel():\n        warnings.warn(\n            f\"loaded a sparse tensor with more than \"\n            f\"{SPARSE_TENSOR_WARNING_THRESHOLD:.0%}% non-zero elements. \"\n            \"This is likely undesirable. Ensure your input is sufficiently \"\n            \"sparse (whether explicitly or implicitly) to leverage the \"\n            \"benefits of sparse tensors.\"\n        )\n\n    return x\n</code></pre>"},{"location":"reference/misc/#src.bioplnn.utils.load_tensor","title":"<code>load_tensor(tensor)</code>","text":"<p>Load a torch tensor from an array, iterable, or file.</p> Source code in <code>src/bioplnn/utils/torch.py</code> <pre><code>def load_tensor(\n    tensor: Union[\n        torch.Tensor, pd.DataFrame, np.ndarray, Iterable[Any], PathLikeType\n    ],\n) -&gt; torch.Tensor:\n    \"\"\"Load a torch tensor from an array, iterable, or file.\"\"\"\n\n    if isinstance(tensor, PathLikeType):\n        return _load_tensor_from_file(tensor)\n    return _load_tensor_from_iterable(tensor)\n</code></pre>"},{"location":"reference/misc/#src.bioplnn.utils.manual_seed","title":"<code>manual_seed(seed)</code>","text":"<p>Set random seeds for reproducibility.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>The random seed to use.</p> required Source code in <code>src/bioplnn/utils/torch.py</code> <pre><code>def manual_seed(seed: int):\n    \"\"\"Set random seeds for reproducibility.\n\n    Args:\n        seed (int): The random seed to use.\n    \"\"\"\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n</code></pre>"},{"location":"reference/misc/#src.bioplnn.utils.manual_seed_deterministic","title":"<code>manual_seed_deterministic(seed)</code>","text":"<p>Set random seeds and configure PyTorch for deterministic execution.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>The random seed to use.</p> required Source code in <code>src/bioplnn/utils/torch.py</code> <pre><code>def manual_seed_deterministic(seed: int):\n    \"\"\"Set random seeds and configure PyTorch for deterministic execution.\n\n    Args:\n        seed (int): The random seed to use.\n    \"\"\"\n    manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    torch.use_deterministic_algorithms(True)\n    os.environ.setdefault(\"CUBLAS_WORKSPACE_CONFIG\", \":4096:8\")\n</code></pre>"},{"location":"reference/misc/#src.bioplnn.utils.pass_fn","title":"<code>pass_fn(*args, **kwargs)</code>","text":"<p>A no-op function that accepts any arguments and does nothing.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Any positional arguments.</p> <code>()</code> <code>**kwargs</code> <p>Any keyword arguments.</p> <code>{}</code> Source code in <code>src/bioplnn/utils/common.py</code> <pre><code>def pass_fn(*args, **kwargs):\n    \"\"\"A no-op function that accepts any arguments and does nothing.\n\n    Args:\n        *args: Any positional arguments.\n        **kwargs: Any keyword arguments.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/misc/#src.bioplnn.utils.print_cuda_mem_stats","title":"<code>print_cuda_mem_stats(device=None)</code>","text":"<p>Print CUDA memory statistics for debugging.</p> Source code in <code>src/bioplnn/utils/torch.py</code> <pre><code>def print_cuda_mem_stats(device: Optional[torch.device] = None):\n    \"\"\"Print CUDA memory statistics for debugging.\"\"\"\n    f, t = torch.cuda.mem_get_info(device)\n    print(f\"Free/Total: {f / (1024**3):.2f}GB/{t / (1024**3):.2f}GB\")\n</code></pre>"},{"location":"reference/misc/#src.bioplnn.utils.profile_fn","title":"<code>profile_fn(fn, sort_by='cuda_time_total', row_limit=50, profile_kwargs={}, fn_kwargs={})</code>","text":"<p>Profile a function with PyTorch's profiler.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <p>Function to profile.</p> required <code>sort_by</code> <code>str</code> <p>Column to sort results by. Defaults to \"cuda_time_total\".</p> <code>'cuda_time_total'</code> <code>row_limit</code> <code>int</code> <p>Maximum number of rows to display. Defaults to 50.</p> <code>50</code> <code>**fn_kwargs</code> <p>Keyword arguments to pass to the function.</p> <code>{}</code> Source code in <code>src/bioplnn/utils/torch.py</code> <pre><code>def profile_fn(\n    fn,\n    sort_by=\"cuda_time_total\",\n    row_limit=50,\n    profile_kwargs={},\n    fn_kwargs={},\n):\n    \"\"\"Profile a function with PyTorch's profiler.\n\n    Args:\n        fn: Function to profile.\n        sort_by (str, optional): Column to sort results by. Defaults to\n            \"cuda_time_total\".\n        row_limit (int, optional): Maximum number of rows to display.\n            Defaults to 50.\n        **fn_kwargs: Keyword arguments to pass to the function.\n    \"\"\"\n    with profile(\n        activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n        **profile_kwargs,\n    ) as prof:\n        fn(**fn_kwargs)\n    print(prof.key_averages().table(sort_by=sort_by, row_limit=row_limit))\n</code></pre>"},{"location":"reference/misc/#src.bioplnn.utils.visualize_v1_activations","title":"<code>visualize_v1_activations(activations_path='activations/ei/mazes/daily-armadillo-3361/activations.pt', save_dir='visualizations', fps=5, sheet_size=(150, 300))</code>","text":"<p>Visualizes the activations of the TopographicalRNN as an animation.</p> <p>Parameters:</p> Name Type Description Default <code>activations_path</code> <code>str</code> <p>Path to the activations file.</p> <code>'activations/ei/mazes/daily-armadillo-3361/activations.pt'</code> <code>save_dir</code> <code>str</code> <p>Directory to save the visualizations. Defaults to \"visualizations\".</p> <code>'visualizations'</code> <code>fps</code> <code>int</code> <p>Frames per second for the animation. Defaults to 5.</p> <code>5</code> <code>sheet_size</code> <code>tuple[int, int]</code> <p>Size of each sheet. Defaults to (150, 300).</p> <code>(150, 300)</code> Source code in <code>src/bioplnn/utils/visualization.py</code> <pre><code>def visualize_v1_activations(\n    activations_path: str = \"activations/ei/mazes/daily-armadillo-3361/activations.pt\",\n    save_dir: str = \"visualizations\",\n    fps: int = 5,\n    sheet_size: tuple[int, int] = (150, 300),\n):\n    \"\"\"\n    Visualizes the activations of the TopographicalRNN as an animation.\n\n    Args:\n        activations_path (str): Path to the activations file.\n        save_dir (str, optional): Directory to save the visualizations. Defaults to \"visualizations\".\n        fps (int, optional): Frames per second for the animation. Defaults to 5.\n        sheet_size (tuple[int, int], optional): Size of each sheet. Defaults to (150, 300).\n    \"\"\"\n    os.makedirs(save_dir, exist_ok=True)\n\n    activations = torch.load(activations_path)\n\n    for i in range(len(activations)):\n        activations[i] = activations[i][0].reshape(*sheet_size)\n\n    # Convert activations (which is a list of dicts, where the inner dicts are column names) to a pandas df\n    df = pd.DataFrame(activations)\n\n    for i, sample in df.iterrows():\n        hs = sample[\"hs\"][-1].squeeze()\n\n        # Create a figure and axis\n        fig, ax = plt.subplots(figsize=(8, 8))\n\n        # Initialize the plot with the first frame\n        im = ax.imshow(hs[0].sum(dim=0).cpu().numpy(), cmap=\"viridis\")\n\n        # Function to update the frame\n        def update(frame):\n            im.set_array(hs[frame].sum(dim=0).cpu().numpy())\n            return [im]\n\n        # Create the animation\n        anim = animation.FuncAnimation(\n            fig, update, frames=len(hs), interval=1000 / fps, blit=True\n        )\n\n        # Save the animation as an MP4 file\n        anim.save(\n            f\"{save_dir}/v1_{i}.mp4\",\n            writer=\"ffmpeg\",\n            fps=fps,\n        )\n\n        plt.close(fig)\n</code></pre>"},{"location":"reference/misc/#src.bioplnn.utils.without_keys","title":"<code>without_keys(d, keys)</code>","text":"<p>Creates a new dictionary without specified keys.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>Mapping</code> <p>Input dictionary.</p> required <code>keys</code> <code>list[str]</code> <p>List of keys to exclude.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A new dictionary without the specified keys.</p> Source code in <code>src/bioplnn/utils/common.py</code> <pre><code>def without_keys(d: Mapping, keys: list[str]) -&gt; dict:\n    \"\"\"Creates a new dictionary without specified keys.\n\n    Args:\n        d (Mapping): Input dictionary.\n        keys (list[str]): List of keys to exclude.\n\n    Returns:\n        dict: A new dictionary without the specified keys.\n    \"\"\"\n    return {x: d[x] for x in d if x not in keys}\n</code></pre>"},{"location":"reference/sparse/","title":"Utilities for efficient sparse matrix operations","text":""},{"location":"reference/sparse/#src.bioplnn.models.sparse","title":"<code>sparse</code>","text":""},{"location":"reference/sparse/#src.bioplnn.models.sparse.SparseLinear","title":"<code>SparseLinear</code>","text":"<p>               Bases: <code>Module</code></p> <p>Sparse linear layer for efficient operations with sparse matrices.</p> <p>This layer implements a sparse linear transformation, similar to nn.Linear, but operates on sparse matrices for memory efficiency.</p> <p>Parameters:</p> Name Type Description Default <code>in_features</code> <code>int</code> <p>Size of the input feature dimension.</p> required <code>out_features</code> <code>int</code> <p>Size of the output feature dimension.</p> required <code>connectivity</code> <code>Tensor</code> <p>Sparse connectivity matrix in COO format.</p> required <code>feature_dim</code> <code>int</code> <p>Dimension on which features reside (0 for rows, 1 for columns).</p> <code>-1</code> <code>bias</code> <code>bool</code> <p>If set to False, no bias term is added.</p> <code>True</code> <code>requires_grad</code> <code>bool</code> <p>Whether the weight and bias parameters require gradient updates.</p> <code>True</code> Source code in <code>src/bioplnn/models/sparse.py</code> <pre><code>class SparseLinear(nn.Module):\n    \"\"\"Sparse linear layer for efficient operations with sparse matrices.\n\n    This layer implements a sparse linear transformation, similar to nn.Linear,\n    but operates on sparse matrices for memory efficiency.\n\n    Args:\n        in_features: Size of the input feature dimension.\n        out_features: Size of the output feature dimension.\n        connectivity: Sparse connectivity matrix in COO format.\n        feature_dim: Dimension on which features reside (0 for rows, 1 for columns).\n        bias: If set to False, no bias term is added.\n        requires_grad: Whether the weight and bias parameters require gradient updates.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        connectivity: torch.Tensor,\n        feature_dim: int = -1,\n        bias: bool = True,\n        requires_grad: bool = True,\n    ):\n        super().__init__()\n\n        self.in_features = in_features\n        self.out_features = out_features\n        self.bias = bias\n        self.feature_dim = feature_dim\n\n        # Validate connectivity format\n        if connectivity.layout != torch.sparse_coo:\n            raise ValueError(\"connectivity must be in COO format.\")\n\n        # Validate input and output sizes against connectivity\n        if in_features != connectivity.shape[1]:\n            raise ValueError(\n                f\"Input size ({in_features}) must be equal to the number of columns in connectivity ({connectivity.shape[1]}).\"\n            )\n        if out_features != connectivity.shape[0]:\n            raise ValueError(\n                f\"Output size ({out_features}) must be equal to the number of rows in connectivity ({connectivity.shape[0]}).\"\n            )\n\n        # Create sparse matrix\n        indices: torch.Tensor\n        values: torch.Tensor\n        indices, values = torch_sparse.coalesce(\n            connectivity.indices().clone(),\n            connectivity.values().clone(),\n            self.out_features,\n            self.in_features,\n        )  # type: ignore\n\n        self.indices = nn.Parameter(indices, requires_grad=False)\n        self.values = nn.Parameter(values.float(), requires_grad=requires_grad)\n\n        self.bias = (\n            nn.Parameter(\n                torch.zeros(self.out_features, 1), requires_grad=requires_grad\n            )\n            if bias\n            else None\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Performs sparse linear transformation on the input tensor.\n\n        Args:\n            x: Input tensor of shape (H, *) if feature_dim is 0, otherwise (*, H).\n\n        Returns:\n            Output tensor after sparse linear transformation.\n        \"\"\"\n\n        shape = list(x.shape)\n\n        if self.feature_dim != 0:\n            permutation = torch.arange(x.dim())\n            permutation[self.feature_dim] = 0\n            permutation[0] = self.feature_dim\n            x = x.permute(*permutation)  # type: ignore\n\n        x = x.flatten(start_dim=1)\n\n        x = torch_sparse.spmm(\n            self.indices,\n            self.values,\n            self.out_features,\n            self.in_features,\n            x,\n        )\n\n        if self.bias is not None:\n            x = x + self.bias\n\n        if self.feature_dim != 0:\n            x = x.permute(*permutation)  # type: ignore\n\n        shape[self.feature_dim] = self.out_features\n        x = x.view(*shape)\n\n        return x\n</code></pre>"},{"location":"reference/sparse/#src.bioplnn.models.sparse.SparseLinear.forward","title":"<code>forward(x)</code>","text":"<p>Performs sparse linear transformation on the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (H, ) if feature_dim is 0, otherwise (, H).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor after sparse linear transformation.</p> Source code in <code>src/bioplnn/models/sparse.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Performs sparse linear transformation on the input tensor.\n\n    Args:\n        x: Input tensor of shape (H, *) if feature_dim is 0, otherwise (*, H).\n\n    Returns:\n        Output tensor after sparse linear transformation.\n    \"\"\"\n\n    shape = list(x.shape)\n\n    if self.feature_dim != 0:\n        permutation = torch.arange(x.dim())\n        permutation[self.feature_dim] = 0\n        permutation[0] = self.feature_dim\n        x = x.permute(*permutation)  # type: ignore\n\n    x = x.flatten(start_dim=1)\n\n    x = torch_sparse.spmm(\n        self.indices,\n        self.values,\n        self.out_features,\n        self.in_features,\n        x,\n    )\n\n    if self.bias is not None:\n        x = x + self.bias\n\n    if self.feature_dim != 0:\n        x = x.permute(*permutation)  # type: ignore\n\n    shape[self.feature_dim] = self.out_features\n    x = x.view(*shape)\n\n    return x\n</code></pre>"},{"location":"reference/sparse/#src.bioplnn.models.sparse.SparseODERNN","title":"<code>SparseODERNN</code>","text":"<p>               Bases: <code>SparseRNN</code></p> <p>Sparse Ordinary Differential Equation Recurrent Neural Network.</p> <p>A continuous-time version of SparseRNN that uses an ODE solver to simulate the dynamics of the network and simultaneously compute the parameter gradients (see <code>torchode.AutoDiffAdjoint</code>).</p> <p>Parameters:</p> Name Type Description Default <code>compile_solver_kwargs</code> <code>Optional[Mapping[str, Any]]</code> <p>Keyword arguments for torch.compile.</p> <code>None</code> Source code in <code>src/bioplnn/models/sparse.py</code> <pre><code>class SparseODERNN(SparseRNN):\n    \"\"\"Sparse Ordinary Differential Equation Recurrent Neural Network.\n\n    A continuous-time version of SparseRNN that uses an ODE solver to\n    simulate the dynamics of the network and simultaneously compute the\n    parameter gradients (see `torchode.AutoDiffAdjoint`).\n\n    Args:\n        compile_solver_kwargs: Keyword arguments for torch.compile.\n    \"\"\"\n\n    def __init__(\n        self,\n        *args,\n        compile_solver_kwargs: Optional[Mapping[str, Any]] = None,\n        compile_update_fn_kwargs: Optional[Mapping[str, Any]] = None,\n        **kwargs,\n    ):\n        super().__init__(*args, **kwargs)\n\n        # Compile update_fn\n        if compile_update_fn_kwargs is not None:\n            self.update_fn = torch.compile(\n                self.update_fn, **compile_update_fn_kwargs\n            )\n\n        # Define ODE solver\n        term = to.ODETerm(self.update_fn, with_args=True)  # type: ignore\n        step_method = to.Dopri5(term=term)\n        step_size_controller = to.IntegralController(\n            atol=1e-6, rtol=1e-3, term=term\n        )\n        self.solver = to.AutoDiffAdjoint(step_method, step_size_controller)  # type: ignore\n\n        # Compile solver\n        if compile_solver_kwargs is not None:\n            self.solver = torch.compile(self.solver, **compile_solver_kwargs)\n\n    def _format_x(self, x: torch.Tensor):\n        \"\"\"Format the input tensor to match the expected shape.\n\n        Args:\n            x: Input tensor. If 2-dimensional, it is assumed to be of shape\n                (batch_size, input_size). If 3-dimensional, it is assumed to be\n                of shape (batch_size, sequence_length, input_size) if\n                batch_first, else (sequence_length, batch_size, input_size).\n\n        Returns:\n            Formatted input tensor of shape (sequence_length, batch_size,\n            input_size)\n\n        Raises:\n            ValueError: For invalid input dimensions.\n        \"\"\"\n\n        if x.dim() == 2:\n            x = x.t()\n            x = x.unsqueeze(0)\n        elif x.dim() == 3:\n            if self.batch_first:\n                x = x.permute(1, 2, 0)\n            else:\n                x = x.permute(0, 2, 1)\n        else:\n            raise ValueError(\n                f\"Input tensor must be 2D or 3D, but got {x.dim()} dimensions.\"\n            )\n\n        return x\n\n    def _format_ts(self, ts: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Format the time points based on batch_first setting.\n\n        Args:\n            ts: Time points tensor.\n\n        Returns:\n            Formatted time points tensor.\n        \"\"\"\n        if self.batch_first:\n            return ts\n        else:\n            return ts.transpose(0, 1)\n\n    def _index_from_time(\n        self,\n        t: torch.Tensor,\n        x: torch.Tensor,\n        start_time: float,\n        end_time: float,\n    ) -&gt; torch.Tensor:\n        \"\"\"Calculate the index of the input tensor corresponding to the given time.\n\n        Args:\n            t: Current time point.\n            x: Input tensor.\n            start_time: Start time for simulation.\n            end_time: End time for simulation.\n\n        Returns:\n            Index tensor for selecting the correct input.\n        \"\"\"\n        idx = (t - start_time) / (end_time - start_time) * x.shape[0]\n        idx[idx == x.shape[0]] = x.shape[0] - 1\n\n        return idx.long()\n\n    def update_fn(\n        self, t: torch.Tensor, h: torch.Tensor, args: Mapping[str, Any]\n    ) -&gt; torch.Tensor:\n        \"\"\"ODE function for the SparseODERNN.\n\n        Args:\n            t: Current time point.\n            h: Current hidden state.\n            args: Additional arguments including input data.\n\n        Returns:\n            Rate of change of the hidden state.\n        \"\"\"\n        h = h.t()\n        x = args[\"x\"]\n        start_time = args[\"start_time\"]\n        end_time = args[\"end_time\"]\n\n        idx = self._index_from_time(t, x, start_time, end_time)\n\n        h_new = self.nonlinearity(self.ih(x[idx]) + self.hh(h))\n\n        dhdt = h_new - h\n\n        return dhdt.t()\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        num_evals: int = 2,\n        start_time: float = 0.0,\n        end_time: float = 1.0,\n        h0: Optional[torch.Tensor] = None,\n        hidden_init_fn: Optional[Union[str, TensorInitFnType]] = None,\n    ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Forward pass of the SparseODERNN layer.\n\n        Solves the initial value problem for the ODE defined by update_fn.\n        The gradients of the parameters are computed using the adjoint method\n        (see `torchode.AutoDiffAdjoint`).\n\n        Args:\n            x: Input tensor.\n            num_evals: Number of evaluations to return. The default of 2 means\n                that the ODE will be evaluated at the start and end of the\n                simulation and those values will be returned. Note that this\n                does not mean the `update_fn` will be called `num_evals` times.\n                It only affects the number of values returned, the step size\n                controller determines the number of times the solver will call\n                the `update_fn`.\n            start_time: Start time for simulation.\n            end_time: End time for simulation.\n            h0: Initial hidden state.\n            hidden_init_fn: Initialization function.\n\n        Returns:\n            Hidden states, outputs, and time points.\n        \"\"\"\n\n        # Ensure connectivity matrix is nonnegative\n        self._clamp_connectivity()\n\n        # Format input and initialize variables\n        x = self._format_x(x)\n        batch_size = x.shape[-1]\n        device = x.device\n\n        # Define evaluation time points\n        if num_evals &lt; 2:\n            raise ValueError(\"num_evals must be greater than 1\")\n        t_eval = (\n            torch.linspace(start_time, end_time, num_evals, device=device)\n            .unsqueeze(0)\n            .expand(batch_size, -1)\n        )\n\n        # Initialize hidden state\n        if h0 is None:\n            h0 = self.init_hidden(\n                batch_size,\n                init_fn=hidden_init_fn,\n                device=device,\n            )\n\n        # Solve ODE\n        problem = to.InitialValueProblem(y0=h0, t_eval=t_eval)  # type: ignore\n        sol = self.solver.solve(\n            problem,\n            args={\n                \"x\": x,\n                \"start_time\": start_time,\n                \"end_time\": end_time,\n            },\n        )\n        hs = sol.ys.permute(1, 2, 0)\n\n        # Project to output space\n        outs = self.ho(hs.transpose(0, 1).flatten(1))\n        outs = outs.view(self.output_size, num_evals, batch_size).transpose(\n            0, 1\n        )\n\n        # Format outputs\n        ts = self._format_ts(sol.ts)\n        outs, hs = self._format_result(outs, hs)\n\n        return outs, hs, ts\n</code></pre>"},{"location":"reference/sparse/#src.bioplnn.models.sparse.SparseODERNN.forward","title":"<code>forward(x, num_evals=2, start_time=0.0, end_time=1.0, h0=None, hidden_init_fn=None)</code>","text":"<p>Forward pass of the SparseODERNN layer.</p> <p>Solves the initial value problem for the ODE defined by update_fn. The gradients of the parameters are computed using the adjoint method (see <code>torchode.AutoDiffAdjoint</code>).</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <code>num_evals</code> <code>int</code> <p>Number of evaluations to return. The default of 2 means that the ODE will be evaluated at the start and end of the simulation and those values will be returned. Note that this does not mean the <code>update_fn</code> will be called <code>num_evals</code> times. It only affects the number of values returned, the step size controller determines the number of times the solver will call the <code>update_fn</code>.</p> <code>2</code> <code>start_time</code> <code>float</code> <p>Start time for simulation.</p> <code>0.0</code> <code>end_time</code> <code>float</code> <p>End time for simulation.</p> <code>1.0</code> <code>h0</code> <code>Optional[Tensor]</code> <p>Initial hidden state.</p> <code>None</code> <code>hidden_init_fn</code> <code>Optional[Union[str, TensorInitFnType]]</code> <p>Initialization function.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor, Tensor]</code> <p>Hidden states, outputs, and time points.</p> Source code in <code>src/bioplnn/models/sparse.py</code> <pre><code>def forward(\n    self,\n    x: torch.Tensor,\n    num_evals: int = 2,\n    start_time: float = 0.0,\n    end_time: float = 1.0,\n    h0: Optional[torch.Tensor] = None,\n    hidden_init_fn: Optional[Union[str, TensorInitFnType]] = None,\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Forward pass of the SparseODERNN layer.\n\n    Solves the initial value problem for the ODE defined by update_fn.\n    The gradients of the parameters are computed using the adjoint method\n    (see `torchode.AutoDiffAdjoint`).\n\n    Args:\n        x: Input tensor.\n        num_evals: Number of evaluations to return. The default of 2 means\n            that the ODE will be evaluated at the start and end of the\n            simulation and those values will be returned. Note that this\n            does not mean the `update_fn` will be called `num_evals` times.\n            It only affects the number of values returned, the step size\n            controller determines the number of times the solver will call\n            the `update_fn`.\n        start_time: Start time for simulation.\n        end_time: End time for simulation.\n        h0: Initial hidden state.\n        hidden_init_fn: Initialization function.\n\n    Returns:\n        Hidden states, outputs, and time points.\n    \"\"\"\n\n    # Ensure connectivity matrix is nonnegative\n    self._clamp_connectivity()\n\n    # Format input and initialize variables\n    x = self._format_x(x)\n    batch_size = x.shape[-1]\n    device = x.device\n\n    # Define evaluation time points\n    if num_evals &lt; 2:\n        raise ValueError(\"num_evals must be greater than 1\")\n    t_eval = (\n        torch.linspace(start_time, end_time, num_evals, device=device)\n        .unsqueeze(0)\n        .expand(batch_size, -1)\n    )\n\n    # Initialize hidden state\n    if h0 is None:\n        h0 = self.init_hidden(\n            batch_size,\n            init_fn=hidden_init_fn,\n            device=device,\n        )\n\n    # Solve ODE\n    problem = to.InitialValueProblem(y0=h0, t_eval=t_eval)  # type: ignore\n    sol = self.solver.solve(\n        problem,\n        args={\n            \"x\": x,\n            \"start_time\": start_time,\n            \"end_time\": end_time,\n        },\n    )\n    hs = sol.ys.permute(1, 2, 0)\n\n    # Project to output space\n    outs = self.ho(hs.transpose(0, 1).flatten(1))\n    outs = outs.view(self.output_size, num_evals, batch_size).transpose(\n        0, 1\n    )\n\n    # Format outputs\n    ts = self._format_ts(sol.ts)\n    outs, hs = self._format_result(outs, hs)\n\n    return outs, hs, ts\n</code></pre>"},{"location":"reference/sparse/#src.bioplnn.models.sparse.SparseODERNN.update_fn","title":"<code>update_fn(t, h, args)</code>","text":"<p>ODE function for the SparseODERNN.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Tensor</code> <p>Current time point.</p> required <code>h</code> <code>Tensor</code> <p>Current hidden state.</p> required <code>args</code> <code>Mapping[str, Any]</code> <p>Additional arguments including input data.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Rate of change of the hidden state.</p> Source code in <code>src/bioplnn/models/sparse.py</code> <pre><code>def update_fn(\n    self, t: torch.Tensor, h: torch.Tensor, args: Mapping[str, Any]\n) -&gt; torch.Tensor:\n    \"\"\"ODE function for the SparseODERNN.\n\n    Args:\n        t: Current time point.\n        h: Current hidden state.\n        args: Additional arguments including input data.\n\n    Returns:\n        Rate of change of the hidden state.\n    \"\"\"\n    h = h.t()\n    x = args[\"x\"]\n    start_time = args[\"start_time\"]\n    end_time = args[\"end_time\"]\n\n    idx = self._index_from_time(t, x, start_time, end_time)\n\n    h_new = self.nonlinearity(self.ih(x[idx]) + self.hh(h))\n\n    dhdt = h_new - h\n\n    return dhdt.t()\n</code></pre>"},{"location":"reference/sparse/#src.bioplnn.models.sparse.SparseRNN","title":"<code>SparseRNN</code>","text":"<p>               Bases: <code>Module</code></p> <p>Sparse Recurrent Neural Network (RNN) layer.</p> <p>A sparse variant of the standard RNN that uses truly sparse linear transformations to compute the input-to-hidden and hidden-to-hidden transformations (and optionally the hidden-to-output transformations).</p> <p>These sparse transformations are computed using the <code>torch_sparse</code> package and allow for efficient memory usage for large networks.</p> <p>This allows for the network weights to directly be trained, a departure from GANs, which typically use fixed sparse weights.</p> Source code in <code>src/bioplnn/models/sparse.py</code> <pre><code>class SparseRNN(nn.Module):\n    \"\"\"Sparse Recurrent Neural Network (RNN) layer.\n\n    A sparse variant of the standard RNN that uses truly sparse linear\n    transformations to compute the input-to-hidden and hidden-to-hidden\n    transformations (and optionally the hidden-to-output transformations).\n\n    These sparse transformations are computed using the `torch_sparse` package\n    and allow for efficient memory usage for large networks.\n\n    This allows for the network weights to directly be trained, a departure\n    from GANs, which typically use fixed sparse weights.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_size: int,\n        hidden_size: int,\n        connectivity_hh: Union[torch.Tensor, PathLike, str],\n        output_size: Optional[int] = None,\n        connectivity_ih: Optional[Union[torch.Tensor, PathLike, str]] = None,\n        connectivity_ho: Optional[Union[torch.Tensor, PathLike, str]] = None,\n        bias_hh: bool = True,\n        bias_ih: bool = False,\n        bias_ho: bool = True,\n        use_dense_ih: bool = False,\n        use_dense_ho: bool = False,\n        train_hh: bool = True,\n        train_ih: bool = True,\n        train_ho: bool = True,\n        default_hidden_init_fn: str = \"zeros\",\n        nonlinearity: str = \"Sigmoid\",\n        batch_first: bool = True,\n    ):\n        \"\"\"Initialize the SparseRNN layer.\n\n        Args:\n            input_size: Size of the input features.\n            hidden_size: Size of the hidden state.\n            connectivity_hh: Connectivity matrix for hidden-to-hidden connections.\n            output_size: Size of the output features.\n            connectivity_ih: Connectivity matrix for input-to-hidden connections.\n            connectivity_ho: Connectivity matrix for hidden-to-output connections.\n            bias_hh: Whether to use bias in the hidden-to-hidden connections.\n            bias_ih: Whether to use bias in the input-to-hidden connections.\n            bias_ho: Whether to use bias in the hidden-to-output connections.\n            use_dense_ih: Whether to use a dense linear layer for input-to-hidden connections.\n            use_dense_ho: Whether to use a dense linear layer for hidden-to-output connections.\n            train_hh: Whether to train the hidden-to-hidden connections.\n            train_ih: Whether to train the input-to-hidden connections.\n            train_ho: Whether to train the hidden-to-output connections.\n            default_hidden_init_fn: Initialization mode for the hidden state.\n            nonlinearity: Nonlinearity function.\n            batch_first: Whether the input is in (batch_size, seq_len, input_size) format.\n        \"\"\"\n\n        super().__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = (\n            output_size if output_size is not None else hidden_size\n        )\n        self.default_hidden_init_fn = default_hidden_init_fn\n        self.nonlinearity = get_activation(nonlinearity)\n        self.batch_first = batch_first\n\n        connectivity_hh, connectivity_ih, connectivity_ho = (\n            self._init_connectivity(\n                connectivity_hh, connectivity_ih, connectivity_ho\n            )\n        )\n\n        self.hh = SparseLinear(\n            in_features=hidden_size,\n            out_features=hidden_size,\n            connectivity=connectivity_hh,\n            feature_dim=0,\n            bias=bias_hh,\n            requires_grad=train_hh,\n        )\n\n        if connectivity_ih is not None:\n            if use_dense_ih:\n                raise ValueError(\n                    \"use_dense_ih must be False if connectivity_ih is provided\"\n                )\n            self.ih = SparseLinear(\n                in_features=input_size,\n                out_features=hidden_size,\n                connectivity=connectivity_ih,\n                feature_dim=0,\n                bias=bias_ih,\n                requires_grad=train_ih,\n            )\n        elif use_dense_ih:\n            warnings.warn(\n                \"connectivity_ih is not provided and use_dense_ih is True, \"\n                \"using a dense linear layer for input-to-hidden connections. \"\n                \"This may result in memory issues. If you are running out of \"\n                \"memory, consider providing connectivity_ih or decreasing \"\n                \"input_size.\"\n            )\n            if not train_ih:\n                raise ValueError(\n                    \"train_ih must be True if connectivity_ih is not provided\"\n                )\n            self.ih = nn.Linear(\n                in_features=input_size,\n                out_features=hidden_size,\n                bias=bias_ih,\n            )\n        else:\n            if input_size != hidden_size:\n                raise ValueError(\n                    \"input_size must be equal to hidden_size if \"\n                    \"connectivity_ih is not provided and use_dense_ih is False\"\n                )\n            self.ih = nn.Identity()\n\n        if connectivity_ho is not None:\n            if use_dense_ho:\n                raise ValueError(\n                    \"use_dense_ho must be False if connectivity_ho is provided\"\n                )\n            if output_size is None:\n                raise ValueError(\n                    \"output_size must be provided if and only if connectivity_ho is provided\"\n                )\n            self.ho = SparseLinear(\n                in_features=hidden_size,\n                out_features=output_size,\n                connectivity=connectivity_ho,\n                feature_dim=0,\n                bias=bias_ho,\n                requires_grad=train_ho,\n            )\n        elif use_dense_ho:\n            warnings.warn(\n                \"connectivity_ho is not provided and use_dense_ho is True, \"\n                \"using a dense linear layer for hidden-to-output connections. \"\n                \"This may result in memory issues. If you are running out of \"\n                \"memory, consider providing connectivity_ho or decreasing \"\n                \"hidden_size.\"\n            )\n            if output_size is None:\n                raise ValueError(\n                    \"output_size must be provided if and only if use_dense_ho is True\"\n                )\n            if not train_ho:\n                raise ValueError(\n                    \"train_ho must be True if connectivity_ho is not provided\"\n                )\n            self.ho = nn.Linear(\n                in_features=hidden_size,\n                out_features=output_size,\n                bias=bias_ho,\n            )\n        else:\n            if output_size is not None and output_size != hidden_size:\n                raise ValueError(\n                    \"output_size should not be provided or should be equal to \"\n                    \"hidden_size if connectivity_ho is not provided and \"\n                    \"use_dense_ho is False\"\n                )\n            self.ho = nn.Identity()\n\n    def _init_connectivity(\n        self,\n        connectivity_hh: Union[torch.Tensor, PathLike, str],\n        connectivity_ih: Optional[Union[torch.Tensor, PathLike, str]] = None,\n        connectivity_ho: Optional[Union[torch.Tensor, PathLike, str]] = None,\n    ) -&gt; tuple[\n        torch.Tensor, Union[torch.Tensor, None], Union[torch.Tensor, None]\n    ]:\n        \"\"\"Initialize connectivity matrices.\n\n        Args:\n            connectivity_hh: Connectivity matrix for hidden-to-hidden connections or path to load it from.\n            connectivity_ih: Connectivity matrix for input-to-hidden connections or path to load it from.\n            connectivity_ho: Connectivity matrix for hidden-to-output connections or path to load it from.\n\n        Returns:\n            Tuple containing the hidden-to-hidden connectivity tensor and input-to-hidden connectivity tensor (or None).\n\n        Raises:\n            ValueError: If connectivity matrices are not in COO format or have\n                invalid dimensions.\n        \"\"\"\n\n        connectivity_hh_tensor = load_sparse_tensor(connectivity_hh)\n\n        if connectivity_ih is not None:\n            connectivity_ih_tensor = load_sparse_tensor(connectivity_ih)\n        else:\n            connectivity_ih_tensor = None\n\n        if connectivity_ho is not None:\n            connectivity_ho_tensor = load_sparse_tensor(connectivity_ho)\n        else:\n            connectivity_ho_tensor = None\n\n        # Validate connectivity matrix dimensions\n        if not (\n            self.hidden_size\n            == connectivity_hh_tensor.shape[0]\n            == connectivity_hh_tensor.shape[1]\n        ):\n            raise ValueError(\n                \"connectivity_ih.shape[0], connectivity_hh.shape[0], and connectivity_hh.shape[1] must be equal\"\n            )\n\n        if connectivity_ih_tensor is not None and (\n            self.input_size != connectivity_ih_tensor.shape[1]\n            or self.hidden_size != connectivity_ih_tensor.shape[0]\n        ):\n            raise ValueError(\n                \"connectivity_ih.shape[1] and input_size must be equal\"\n            )\n\n        if connectivity_ho_tensor is not None and (\n            self.hidden_size != connectivity_ho_tensor.shape[1]\n            or self.output_size != connectivity_ho_tensor.shape[0]\n        ):\n            raise ValueError(\n                \"connectivity_ho.shape[0] and output_size must be equal\"\n            )\n\n        return (\n            connectivity_hh_tensor,\n            connectivity_ih_tensor,\n            connectivity_ho_tensor,\n        )\n\n    def init_hidden(\n        self,\n        batch_size: int,\n        init_fn: Optional[Union[str, TensorInitFnType]] = None,\n        device: Optional[Union[torch.device, str]] = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"Initialize the hidden state.\n\n        Args:\n            batch_size: Batch size.\n            init_fn: Initialization function.\n            device: Device to allocate the hidden state on.\n\n        Returns:\n            The initialized hidden state of shape (batch_size, hidden_size).\n        \"\"\"\n\n        if init_fn is None:\n            init_fn = self.default_hidden_init_fn\n\n        return init_tensor(\n            init_fn, batch_size, self.hidden_size, device=device\n        )\n\n    def init_state(\n        self,\n        num_steps: int,\n        batch_size: int,\n        h0: Optional[torch.Tensor] = None,\n        hidden_init_fn: Optional[Union[str, TensorInitFnType]] = None,\n        device: Optional[Union[torch.device, str]] = None,\n    ) -&gt; list[Optional[torch.Tensor]]:\n        \"\"\"Initialize the internal state of the network.\n\n        Args:\n            num_steps: Number of time steps.\n            batch_size: Batch size.\n            h0: Initial hidden states.\n            hidden_init_fn: Initialization function.\n            device: Device to allocate tensors on.\n\n        Returns:\n            The initialized hidden states for each time step.\n        \"\"\"\n\n        hs: list[Optional[torch.Tensor]] = [None] * num_steps\n        if h0 is None:\n            h0 = self.init_hidden(\n                batch_size,\n                init_fn=hidden_init_fn,\n                device=device,\n            )\n        hs[-1] = h0.t()\n        return hs\n\n    def _format_x(self, x: torch.Tensor, num_steps: Optional[int] = None):\n        \"\"\"Format the input tensor to match the expected shape.\n\n        Args:\n            x: Input tensor.\n            num_steps: Number of time steps.\n\n        Returns:\n            The formatted input tensor and corrected number of time steps.\n\n        Raises:\n            ValueError: For invalid input dimensions or step counts.\n        \"\"\"\n        if x.dim() == 2:\n            if num_steps is None or num_steps &lt; 1:\n                raise ValueError(\n                    \"If x is 2D, num_steps must be provided and greater than 0\"\n                )\n            x = x.t()\n            x = x.unsqueeze(0).expand((num_steps, -1, -1))\n        elif x.dim() == 3:\n            if self.batch_first:\n                x = x.permute(1, 2, 0)\n            else:\n                x = x.permute(0, 2, 1)\n            if num_steps is not None and num_steps != x.shape[0]:\n                raise ValueError(\n                    \"If x is 3D and num_steps is provided, it must match the \"\n                    \"sequence length.\"\n                )\n            num_steps = x.shape[0]\n        else:\n            raise ValueError(\n                f\"Input tensor must be 2D or 3D, but got {x.dim()} dimensions.\"\n            )\n        return x, num_steps\n\n    def _format_result(\n        self,\n        outs: torch.Tensor,\n        hs: torch.Tensor,\n    ) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Format the hidden states and outputs for output.\n\n        Args:\n            hs: Hidden states for each time step.\n            outs: Outputs for each time step.\n\n        Returns:\n            Formatted outputs and hidden states.\n        \"\"\"\n\n        if self.batch_first:\n            return outs.permute(2, 0, 1), hs.permute(2, 0, 1)\n        else:\n            return outs.permute(0, 2, 1), hs.permute(0, 2, 1)\n\n    def _clamp_connectivity(self) -&gt; None:\n        \"\"\"Ensure the connectivity matrix is nonnegative.\"\"\"\n\n        self.hh.values.data.clamp_(min=0.0)\n\n    def update_fn(self, x: torch.Tensor, h: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Update function for the SparseRNN.\n\n        Args:\n            x: Input tensor at current timestep.\n            h: Hidden state from previous timestep.\n\n        Returns:\n            Updated hidden state.\n        \"\"\"\n        return self.nonlinearity(self.ih(x) + self.hh(h))\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        num_steps: Optional[int] = None,\n        h0: Optional[torch.Tensor] = None,\n        hidden_init_fn: Optional[Union[str, TensorInitFnType]] = None,\n    ) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Forward pass of the SparseRNN layer.\n\n        Args:\n            x: Input tensor.\n            num_steps: Number of time steps.\n            h0: Initial hidden state.\n            hidden_init_fn: Initialization function.\n\n        Returns:\n            Hidden states and outputs.\n        \"\"\"\n\n        # Ensure connectivity matrix is nonnegative\n        self._clamp_connectivity()\n\n        # Format input and initialize variables\n        x, num_steps = self._format_x(x, num_steps)\n        batch_size = x.shape[-1]\n        device = x.device\n\n        hs = self.init_state(\n            num_steps,\n            batch_size,\n            h0=h0,\n            hidden_init_fn=hidden_init_fn,\n            device=device,\n        )\n\n        for t in range(num_steps):\n            hs[t] = self.update_fn(x[t], hs[t - 1])  # type: ignore\n\n        assert all(h is not None for h in hs)\n        hs = torch.stack(hs)  # type: ignore\n\n        assert hs.shape == (num_steps, self.hidden_size, batch_size)\n        outs = self.ho(hs.transpose(0, 1).flatten(1))\n        outs = outs.view(self.output_size, num_steps, batch_size).transpose(\n            0, 1\n        )\n\n        return self._format_result(outs, hs)\n</code></pre>"},{"location":"reference/sparse/#src.bioplnn.models.sparse.SparseRNN.__init__","title":"<code>__init__(input_size, hidden_size, connectivity_hh, output_size=None, connectivity_ih=None, connectivity_ho=None, bias_hh=True, bias_ih=False, bias_ho=True, use_dense_ih=False, use_dense_ho=False, train_hh=True, train_ih=True, train_ho=True, default_hidden_init_fn='zeros', nonlinearity='Sigmoid', batch_first=True)</code>","text":"<p>Initialize the SparseRNN layer.</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>int</code> <p>Size of the input features.</p> required <code>hidden_size</code> <code>int</code> <p>Size of the hidden state.</p> required <code>connectivity_hh</code> <code>Union[Tensor, PathLike, str]</code> <p>Connectivity matrix for hidden-to-hidden connections.</p> required <code>output_size</code> <code>Optional[int]</code> <p>Size of the output features.</p> <code>None</code> <code>connectivity_ih</code> <code>Optional[Union[Tensor, PathLike, str]]</code> <p>Connectivity matrix for input-to-hidden connections.</p> <code>None</code> <code>connectivity_ho</code> <code>Optional[Union[Tensor, PathLike, str]]</code> <p>Connectivity matrix for hidden-to-output connections.</p> <code>None</code> <code>bias_hh</code> <code>bool</code> <p>Whether to use bias in the hidden-to-hidden connections.</p> <code>True</code> <code>bias_ih</code> <code>bool</code> <p>Whether to use bias in the input-to-hidden connections.</p> <code>False</code> <code>bias_ho</code> <code>bool</code> <p>Whether to use bias in the hidden-to-output connections.</p> <code>True</code> <code>use_dense_ih</code> <code>bool</code> <p>Whether to use a dense linear layer for input-to-hidden connections.</p> <code>False</code> <code>use_dense_ho</code> <code>bool</code> <p>Whether to use a dense linear layer for hidden-to-output connections.</p> <code>False</code> <code>train_hh</code> <code>bool</code> <p>Whether to train the hidden-to-hidden connections.</p> <code>True</code> <code>train_ih</code> <code>bool</code> <p>Whether to train the input-to-hidden connections.</p> <code>True</code> <code>train_ho</code> <code>bool</code> <p>Whether to train the hidden-to-output connections.</p> <code>True</code> <code>default_hidden_init_fn</code> <code>str</code> <p>Initialization mode for the hidden state.</p> <code>'zeros'</code> <code>nonlinearity</code> <code>str</code> <p>Nonlinearity function.</p> <code>'Sigmoid'</code> <code>batch_first</code> <code>bool</code> <p>Whether the input is in (batch_size, seq_len, input_size) format.</p> <code>True</code> Source code in <code>src/bioplnn/models/sparse.py</code> <pre><code>def __init__(\n    self,\n    input_size: int,\n    hidden_size: int,\n    connectivity_hh: Union[torch.Tensor, PathLike, str],\n    output_size: Optional[int] = None,\n    connectivity_ih: Optional[Union[torch.Tensor, PathLike, str]] = None,\n    connectivity_ho: Optional[Union[torch.Tensor, PathLike, str]] = None,\n    bias_hh: bool = True,\n    bias_ih: bool = False,\n    bias_ho: bool = True,\n    use_dense_ih: bool = False,\n    use_dense_ho: bool = False,\n    train_hh: bool = True,\n    train_ih: bool = True,\n    train_ho: bool = True,\n    default_hidden_init_fn: str = \"zeros\",\n    nonlinearity: str = \"Sigmoid\",\n    batch_first: bool = True,\n):\n    \"\"\"Initialize the SparseRNN layer.\n\n    Args:\n        input_size: Size of the input features.\n        hidden_size: Size of the hidden state.\n        connectivity_hh: Connectivity matrix for hidden-to-hidden connections.\n        output_size: Size of the output features.\n        connectivity_ih: Connectivity matrix for input-to-hidden connections.\n        connectivity_ho: Connectivity matrix for hidden-to-output connections.\n        bias_hh: Whether to use bias in the hidden-to-hidden connections.\n        bias_ih: Whether to use bias in the input-to-hidden connections.\n        bias_ho: Whether to use bias in the hidden-to-output connections.\n        use_dense_ih: Whether to use a dense linear layer for input-to-hidden connections.\n        use_dense_ho: Whether to use a dense linear layer for hidden-to-output connections.\n        train_hh: Whether to train the hidden-to-hidden connections.\n        train_ih: Whether to train the input-to-hidden connections.\n        train_ho: Whether to train the hidden-to-output connections.\n        default_hidden_init_fn: Initialization mode for the hidden state.\n        nonlinearity: Nonlinearity function.\n        batch_first: Whether the input is in (batch_size, seq_len, input_size) format.\n    \"\"\"\n\n    super().__init__()\n    self.input_size = input_size\n    self.hidden_size = hidden_size\n    self.output_size = (\n        output_size if output_size is not None else hidden_size\n    )\n    self.default_hidden_init_fn = default_hidden_init_fn\n    self.nonlinearity = get_activation(nonlinearity)\n    self.batch_first = batch_first\n\n    connectivity_hh, connectivity_ih, connectivity_ho = (\n        self._init_connectivity(\n            connectivity_hh, connectivity_ih, connectivity_ho\n        )\n    )\n\n    self.hh = SparseLinear(\n        in_features=hidden_size,\n        out_features=hidden_size,\n        connectivity=connectivity_hh,\n        feature_dim=0,\n        bias=bias_hh,\n        requires_grad=train_hh,\n    )\n\n    if connectivity_ih is not None:\n        if use_dense_ih:\n            raise ValueError(\n                \"use_dense_ih must be False if connectivity_ih is provided\"\n            )\n        self.ih = SparseLinear(\n            in_features=input_size,\n            out_features=hidden_size,\n            connectivity=connectivity_ih,\n            feature_dim=0,\n            bias=bias_ih,\n            requires_grad=train_ih,\n        )\n    elif use_dense_ih:\n        warnings.warn(\n            \"connectivity_ih is not provided and use_dense_ih is True, \"\n            \"using a dense linear layer for input-to-hidden connections. \"\n            \"This may result in memory issues. If you are running out of \"\n            \"memory, consider providing connectivity_ih or decreasing \"\n            \"input_size.\"\n        )\n        if not train_ih:\n            raise ValueError(\n                \"train_ih must be True if connectivity_ih is not provided\"\n            )\n        self.ih = nn.Linear(\n            in_features=input_size,\n            out_features=hidden_size,\n            bias=bias_ih,\n        )\n    else:\n        if input_size != hidden_size:\n            raise ValueError(\n                \"input_size must be equal to hidden_size if \"\n                \"connectivity_ih is not provided and use_dense_ih is False\"\n            )\n        self.ih = nn.Identity()\n\n    if connectivity_ho is not None:\n        if use_dense_ho:\n            raise ValueError(\n                \"use_dense_ho must be False if connectivity_ho is provided\"\n            )\n        if output_size is None:\n            raise ValueError(\n                \"output_size must be provided if and only if connectivity_ho is provided\"\n            )\n        self.ho = SparseLinear(\n            in_features=hidden_size,\n            out_features=output_size,\n            connectivity=connectivity_ho,\n            feature_dim=0,\n            bias=bias_ho,\n            requires_grad=train_ho,\n        )\n    elif use_dense_ho:\n        warnings.warn(\n            \"connectivity_ho is not provided and use_dense_ho is True, \"\n            \"using a dense linear layer for hidden-to-output connections. \"\n            \"This may result in memory issues. If you are running out of \"\n            \"memory, consider providing connectivity_ho or decreasing \"\n            \"hidden_size.\"\n        )\n        if output_size is None:\n            raise ValueError(\n                \"output_size must be provided if and only if use_dense_ho is True\"\n            )\n        if not train_ho:\n            raise ValueError(\n                \"train_ho must be True if connectivity_ho is not provided\"\n            )\n        self.ho = nn.Linear(\n            in_features=hidden_size,\n            out_features=output_size,\n            bias=bias_ho,\n        )\n    else:\n        if output_size is not None and output_size != hidden_size:\n            raise ValueError(\n                \"output_size should not be provided or should be equal to \"\n                \"hidden_size if connectivity_ho is not provided and \"\n                \"use_dense_ho is False\"\n            )\n        self.ho = nn.Identity()\n</code></pre>"},{"location":"reference/sparse/#src.bioplnn.models.sparse.SparseRNN.forward","title":"<code>forward(x, num_steps=None, h0=None, hidden_init_fn=None)</code>","text":"<p>Forward pass of the SparseRNN layer.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <code>num_steps</code> <code>Optional[int]</code> <p>Number of time steps.</p> <code>None</code> <code>h0</code> <code>Optional[Tensor]</code> <p>Initial hidden state.</p> <code>None</code> <code>hidden_init_fn</code> <code>Optional[Union[str, TensorInitFnType]]</code> <p>Initialization function.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>Hidden states and outputs.</p> Source code in <code>src/bioplnn/models/sparse.py</code> <pre><code>def forward(\n    self,\n    x: torch.Tensor,\n    num_steps: Optional[int] = None,\n    h0: Optional[torch.Tensor] = None,\n    hidden_init_fn: Optional[Union[str, TensorInitFnType]] = None,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Forward pass of the SparseRNN layer.\n\n    Args:\n        x: Input tensor.\n        num_steps: Number of time steps.\n        h0: Initial hidden state.\n        hidden_init_fn: Initialization function.\n\n    Returns:\n        Hidden states and outputs.\n    \"\"\"\n\n    # Ensure connectivity matrix is nonnegative\n    self._clamp_connectivity()\n\n    # Format input and initialize variables\n    x, num_steps = self._format_x(x, num_steps)\n    batch_size = x.shape[-1]\n    device = x.device\n\n    hs = self.init_state(\n        num_steps,\n        batch_size,\n        h0=h0,\n        hidden_init_fn=hidden_init_fn,\n        device=device,\n    )\n\n    for t in range(num_steps):\n        hs[t] = self.update_fn(x[t], hs[t - 1])  # type: ignore\n\n    assert all(h is not None for h in hs)\n    hs = torch.stack(hs)  # type: ignore\n\n    assert hs.shape == (num_steps, self.hidden_size, batch_size)\n    outs = self.ho(hs.transpose(0, 1).flatten(1))\n    outs = outs.view(self.output_size, num_steps, batch_size).transpose(\n        0, 1\n    )\n\n    return self._format_result(outs, hs)\n</code></pre>"},{"location":"reference/sparse/#src.bioplnn.models.sparse.SparseRNN.init_hidden","title":"<code>init_hidden(batch_size, init_fn=None, device=None)</code>","text":"<p>Initialize the hidden state.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size.</p> required <code>init_fn</code> <code>Optional[Union[str, TensorInitFnType]]</code> <p>Initialization function.</p> <code>None</code> <code>device</code> <code>Optional[Union[device, str]]</code> <p>Device to allocate the hidden state on.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The initialized hidden state of shape (batch_size, hidden_size).</p> Source code in <code>src/bioplnn/models/sparse.py</code> <pre><code>def init_hidden(\n    self,\n    batch_size: int,\n    init_fn: Optional[Union[str, TensorInitFnType]] = None,\n    device: Optional[Union[torch.device, str]] = None,\n) -&gt; torch.Tensor:\n    \"\"\"Initialize the hidden state.\n\n    Args:\n        batch_size: Batch size.\n        init_fn: Initialization function.\n        device: Device to allocate the hidden state on.\n\n    Returns:\n        The initialized hidden state of shape (batch_size, hidden_size).\n    \"\"\"\n\n    if init_fn is None:\n        init_fn = self.default_hidden_init_fn\n\n    return init_tensor(\n        init_fn, batch_size, self.hidden_size, device=device\n    )\n</code></pre>"},{"location":"reference/sparse/#src.bioplnn.models.sparse.SparseRNN.init_state","title":"<code>init_state(num_steps, batch_size, h0=None, hidden_init_fn=None, device=None)</code>","text":"<p>Initialize the internal state of the network.</p> <p>Parameters:</p> Name Type Description Default <code>num_steps</code> <code>int</code> <p>Number of time steps.</p> required <code>batch_size</code> <code>int</code> <p>Batch size.</p> required <code>h0</code> <code>Optional[Tensor]</code> <p>Initial hidden states.</p> <code>None</code> <code>hidden_init_fn</code> <code>Optional[Union[str, TensorInitFnType]]</code> <p>Initialization function.</p> <code>None</code> <code>device</code> <code>Optional[Union[device, str]]</code> <p>Device to allocate tensors on.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Optional[Tensor]]</code> <p>The initialized hidden states for each time step.</p> Source code in <code>src/bioplnn/models/sparse.py</code> <pre><code>def init_state(\n    self,\n    num_steps: int,\n    batch_size: int,\n    h0: Optional[torch.Tensor] = None,\n    hidden_init_fn: Optional[Union[str, TensorInitFnType]] = None,\n    device: Optional[Union[torch.device, str]] = None,\n) -&gt; list[Optional[torch.Tensor]]:\n    \"\"\"Initialize the internal state of the network.\n\n    Args:\n        num_steps: Number of time steps.\n        batch_size: Batch size.\n        h0: Initial hidden states.\n        hidden_init_fn: Initialization function.\n        device: Device to allocate tensors on.\n\n    Returns:\n        The initialized hidden states for each time step.\n    \"\"\"\n\n    hs: list[Optional[torch.Tensor]] = [None] * num_steps\n    if h0 is None:\n        h0 = self.init_hidden(\n            batch_size,\n            init_fn=hidden_init_fn,\n            device=device,\n        )\n    hs[-1] = h0.t()\n    return hs\n</code></pre>"},{"location":"reference/sparse/#src.bioplnn.models.sparse.SparseRNN.update_fn","title":"<code>update_fn(x, h)</code>","text":"<p>Update function for the SparseRNN.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor at current timestep.</p> required <code>h</code> <code>Tensor</code> <p>Hidden state from previous timestep.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Updated hidden state.</p> Source code in <code>src/bioplnn/models/sparse.py</code> <pre><code>def update_fn(self, x: torch.Tensor, h: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Update function for the SparseRNN.\n\n    Args:\n        x: Input tensor at current timestep.\n        h: Hidden state from previous timestep.\n\n    Returns:\n        Updated hidden state.\n    \"\"\"\n    return self.nonlinearity(self.ih(x) + self.hh(h))\n</code></pre>"}]}